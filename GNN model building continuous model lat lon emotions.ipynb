{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data from 1 Person "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta, timezone  # Import timezone here\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Constants\n",
    "PAD_VALUE = 0\n",
    "SOS = 1\n",
    "EOS = 2\n",
    "TOKEN_OFFSET = 3  # Offset for unique activity and mode indices to include special tokens\n",
    "LAT_LON_PAD = 0\n",
    "\n",
    "def preprocess_data(csv_file, hr_file, start_time, end_time, dest_dir):\n",
    "    # Convert start and end times to datetime objects, explicitly defining them as UTC\n",
    "    start_dt = datetime.strptime(start_time, '%A, %B %d, %Y %I:%M:%S %p').replace(tzinfo=timezone.utc)\n",
    "    end_dt = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S UTC').replace(tzinfo=timezone.utc)\n",
    "\n",
    "    # Load CSV files\n",
    "    df_activities = pd.read_csv(csv_file)\n",
    "    df_hr = pd.read_csv(hr_file, skiprows=2, header=None, names=['HR'])\n",
    "\n",
    "    # Convert 'Start Time' and 'End Time' to timezone-aware datetime objects\n",
    "    df_activities['Start Time'] = pd.to_datetime(df_activities['Start Time'])\n",
    "    df_activities['End Time'] = pd.to_datetime(df_activities['End Time'])\n",
    "\n",
    "    # Localize or convert to UTC\n",
    "    df_activities['Start Time'] = df_activities['Start Time'].dt.tz_localize('UTC') if df_activities['Start Time'].dt.tz is None else df_activities['Start Time'].dt.tz_convert('UTC')\n",
    "    df_activities['End Time'] = df_activities['End Time'].dt.tz_localize('UTC') if df_activities['End Time'].dt.tz is None else df_activities['End Time'].dt.tz_convert('UTC')\n",
    "\n",
    "    # Filter activities data for the specified day\n",
    "    filtered_activities = df_activities[(df_activities['Start Time'] >= start_dt) & (df_activities['End Time'] <= end_dt)]\n",
    "    # print(filtered_activities)\n",
    "    # Create time series for activities, modes, and emotions at 1 Hz\n",
    "    total_seconds = min(int((end_dt - start_dt).total_seconds()), len(df_hr) - 1)\n",
    "    timesteps = pd.date_range(start=start_dt, periods=total_seconds + 1, freq='S')\n",
    "\n",
    "    # Initialize data arrays\n",
    "    activities = np.full((1, total_seconds + 1), PAD_VALUE)\n",
    "    modes = np.full((1, total_seconds + 1), PAD_VALUE)\n",
    "    emotions = np.full((1, total_seconds + 1), df_hr['HR'].values, dtype=float)\n",
    "\n",
    "    # Unique tokens for activities and modes, ensuring they do not overlap\n",
    "    activity_tokens = {activity: i + TOKEN_OFFSET for i, activity in enumerate(filtered_activities['mainActivity'].unique())}\n",
    "    mode_tokens = {mode: i + TOKEN_OFFSET + len(activity_tokens) for i, mode in enumerate(filtered_activities['howTravelled'].unique())}  # Offset by number of activities\n",
    "\n",
    "    # Fill activity and mode arrays\n",
    "    for _, row in filtered_activities.iterrows():\n",
    "        start_sec = int((row['Start Time'] - start_dt).total_seconds())\n",
    "        end_sec = int((row['End Time'] - start_dt).total_seconds())\n",
    "        activities[0, start_sec:end_sec + 1] = activity_tokens.get(row['mainActivity'], PAD_VALUE)\n",
    "        modes[0, start_sec:end_sec + 1] = mode_tokens.get(row['howTravelled'], PAD_VALUE)\n",
    "\n",
    "    # Handling latitude and longitude with interpolation\n",
    "    latitudes = np.interp(\n",
    "        [t.timestamp() for t in timesteps],\n",
    "        [t.timestamp() for t in filtered_activities['Start Time']],\n",
    "        filtered_activities['lat'].fillna(LAT_LON_PAD)\n",
    "    )\n",
    "    longitudes = np.interp(\n",
    "        [t.timestamp() for t in timesteps],\n",
    "        [t.timestamp() for t in filtered_activities['Start Time']],\n",
    "        filtered_activities['lon'].fillna(LAT_LON_PAD)\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'emotions': emotions,\n",
    "        'latitudes': np.array([latitudes]),  # Shape as (1, n)\n",
    "        'longitudes': np.array([longitudes]),\n",
    "        'activities': activities,\n",
    "        'modes': modes\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotions shape: (1, 34610)\n",
      "Emotions data: [[71.   71.   70.33 ... 85.97 85.98 86.02]]\n",
      "Emotions does not contain any NaN values.\n",
      "Latitudes shape: (1, 34610)\n",
      "Latitudes data: [[ 0.          0.          0.         ... 55.73402467 55.73402467\n",
      "  55.73402467]]\n",
      "Latitudes does not contain any NaN values.\n",
      "Longitudes shape: (1, 34610)\n",
      "Longitudes data: [[ 0.         0.         0.        ... 12.5760144 12.5760144 12.5760144]]\n",
      "Longitudes does not contain any NaN values.\n",
      "Activities shape: (1, 34610)\n",
      "Activities data: [[0 0 0 ... 4 4 4]]\n",
      "Activities does not contain any NaN values.\n",
      "Modes shape: (1, 34610)\n",
      "Modes data: [[0 0 0 ... 7 7 7]]\n",
      "Modes does not contain any NaN values.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "csv_file = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\sample data\\diary.csv'\n",
    "hr_file = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\sample data\\participant\\0701\\HR.csv'\n",
    "dest_dir = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\sample data\\processed'\n",
    "start_time = 'Friday, January 7, 2022 12:36:56 PM'\n",
    "end_time = '2022-01-08 14:53:03 UTC'\n",
    "emotions, latitudes, longitudes, activities, modes = preprocess_data(csv_file, hr_file, start_time, end_time, dest_dir).values()\n",
    "\n",
    "# Function to check for NaN values in the data arrays\n",
    "def check_for_nans(array, name):\n",
    "    if np.isnan(array).any():\n",
    "        print(f\"{name} contains NaN values.\")\n",
    "    else:\n",
    "        print(f\"{name} does not contain any NaN values.\")\n",
    "\n",
    "# Print the data and their shapes, and check for NaN values\n",
    "print(\"Emotions shape:\", emotions.shape)\n",
    "print(\"Emotions data:\", emotions[:1])  # Show only the first row for brevity\n",
    "check_for_nans(emotions, \"Emotions\")\n",
    "\n",
    "print(\"Latitudes shape:\", latitudes.shape)\n",
    "print(\"Latitudes data:\", latitudes[:1])  # Show only the first row for brevity\n",
    "check_for_nans(latitudes, \"Latitudes\")\n",
    "\n",
    "print(\"Longitudes shape:\", longitudes.shape)\n",
    "print(\"Longitudes data:\", longitudes[:1])  # Show only the first row for brevity\n",
    "check_for_nans(longitudes, \"Longitudes\")\n",
    "\n",
    "print(\"Activities shape:\", activities.shape)\n",
    "print(\"Activities data:\", activities[:1])  # Show only the first row for brevity\n",
    "check_for_nans(activities, \"Activities\")\n",
    "\n",
    "print(\"Modes shape:\", modes.shape)\n",
    "print(\"Modes data:\", modes[:1])  # Show only the first row for brevity\n",
    "check_for_nans(modes, \"Modes\")\n",
    "\n",
    "# Sampling every 10th timepoint\n",
    "emotions_sampled = emotions[:, ::10]\n",
    "latitudes_sampled = latitudes[::10]\n",
    "longitudes_sampled = longitudes[::10]\n",
    "activities_sampled = activities[:, ::10]\n",
    "modes_sampled = modes[:, ::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ..., 55.73402467,\n",
       "        55.73402467, 55.73402467]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latitudes_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.       ,  0.       ,  0.       , ..., 12.5760144, 12.5760144,\n",
       "        12.5760144]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longitudes_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 4, 4, 4]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activities_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: preprocessed\\sample_0_segment_0.npz\n",
      "File saved: preprocessed\\sample_0_segment_1.npz\n",
      "File saved: preprocessed\\sample_0_segment_2.npz\n",
      "File saved: preprocessed\\sample_0_segment_3.npz\n",
      "File saved: preprocessed\\sample_0_segment_4.npz\n",
      "File saved: preprocessed\\sample_0_segment_5.npz\n",
      "File saved: preprocessed\\sample_0_segment_6.npz\n",
      "File saved: preprocessed\\sample_0_segment_7.npz\n",
      "File saved: preprocessed\\sample_0_segment_8.npz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from enum import Enum\n",
    "\n",
    "# Constants\n",
    "N_DISCRETE_VALUES = 4  \n",
    "N_SAMPLES = 1\n",
    "N_TIMESTEPS = 3461  # For the initial data generation\n",
    "PAD_VALUE = 0  # Define a padding value for activities and modes\n",
    "MAX_SIMU_TOKENS = 4 \n",
    "N_TRACKS = 4  # Number of \"instrument\" tracks\n",
    "N_BARS = 2  # Assuming 2 bars for simplicity, bars = days\n",
    "RESOLUTION = 8  # Assuming 8 timesteps per beat, adjust as needed, hours ?\n",
    "\n",
    "# Assuming the location track is the second track (index 1)\n",
    "LOCATION_TRACK_IDX = 1  # Adjust this index based on which track you want to use for location data\n",
    "\n",
    "class EdgeTypes(Enum):\n",
    "    TRACK = 0\n",
    "    ONSET = N_TRACKS\n",
    "    NEXT = N_TRACKS + 1\n",
    "\n",
    "# N_TRACKS track types + 1 onset edge type + 1 next edge type\n",
    "N_EDGE_TYPES = N_TRACKS + 2\n",
    "\n",
    "# Calculate window size and stride for sliding windows\n",
    "window_size = N_BARS * 4 * RESOLUTION\n",
    "stride = window_size // 2\n",
    "\n",
    "def preprocess_sample_data(dest_dir):\n",
    "\n",
    "    # Sampling every 10th timestep\n",
    "    emotions = emotions_sampled\n",
    "    latitudes = latitudes_sampled\n",
    "    longitudes = longitudes_sampled\n",
    "    activities = activities_sampled\n",
    "    modes = modes_sampled\n",
    "\n",
    "    for sample_idx in range(N_SAMPLES):\n",
    "        full_c_tensor = np.zeros((N_TRACKS, N_TIMESTEPS // 10, MAX_SIMU_TOKENS, 2), dtype=np.float32)\n",
    "        for t in range(N_TIMESTEPS // 10):\n",
    "            for track_idx, data in enumerate([emotions, latitudes, activities, modes]):\n",
    "                value = data[sample_idx, t]\n",
    "                if track_idx == LOCATION_TRACK_IDX:\n",
    "                    full_c_tensor[track_idx, t, 0, :] = [latitudes[sample_idx, t], 1]  # Latitude\n",
    "                    full_c_tensor[track_idx, t, 1, :] = [longitudes[sample_idx, t], 1]  # Longitude\n",
    "                else:\n",
    "                    full_c_tensor[track_idx, t, 0, :] = [value, 1]\n",
    "\n",
    "        full_s_tensor = np.any(full_c_tensor != 0, axis=(2, 3)).astype(int)\n",
    "\n",
    "        for start_idx in range(0, N_TIMESTEPS // 10 - window_size + 1, stride):\n",
    "            c_tensor_segment = full_c_tensor[:, start_idx:start_idx + window_size, :, :]\n",
    "            s_tensor_segment = full_s_tensor[:, start_idx:start_idx + window_size]\n",
    "            sample_filepath = os.path.join(dest_dir, f\"sample_{sample_idx}_segment_{start_idx//stride}.npz\")\n",
    "            try:\n",
    "                np.savez(sample_filepath, c_tensor=c_tensor_segment, s_tensor=s_tensor_segment)\n",
    "                print(f\"File saved: {sample_filepath}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to save {sample_filepath}: {e}\")\n",
    "\n",
    "csv_file = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\sample data\\diary.csv'\n",
    "dest_dir = 'preprocessed'\n",
    "preprocess_sample_data(dest_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of c_tensor: (4, 64, 4, 2)\n",
      "Shape of s_tensor: (4, 64)\n"
     ]
    }
   ],
   "source": [
    "file_path = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_0_segment_0.npz'\n",
    "\n",
    "# Load the .npz file\n",
    "data = np.load(file_path)\n",
    "\n",
    "# Access the tensors\n",
    "c_tensor = data['c_tensor']\n",
    "s_tensor = data['s_tensor']\n",
    "\n",
    "# Print their shapes\n",
    "print(f'Shape of c_tensor: {c_tensor.shape}')\n",
    "print(f'Shape of s_tensor: {s_tensor.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Values Testing Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_geographical_data(samples=1, n_timesteps=100):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Latitude and Longitude ranges for New York City\n",
    "    lat_range = (40.7128, 40.7480)\n",
    "    lon_range = (-74.0060, -73.9352)\n",
    "\n",
    "    # Generate random lat-lon coordinates\n",
    "    latitudes = np.random.uniform(low=lat_range[0], high=lat_range[1], size=(samples, n_timesteps))\n",
    "    longitudes = np.random.uniform(low=lon_range[0], high=lon_range[1], size=(samples, n_timesteps))\n",
    "\n",
    "    # Simulate % greenage (0 to 100%)\n",
    "    greenage = np.random.uniform(low=0, high=100, size=(samples, n_timesteps))\n",
    "\n",
    "    # Normalize the features\n",
    "    latitudes_normalized = (latitudes - lat_range[0]) / (lat_range[1] - lat_range[0])\n",
    "    longitudes_normalized = (longitudes - lon_range[0]) / (lon_range[1] - lon_range[0])\n",
    "    greenage_normalized = greenage / 100  # Since greenage is already within 0 to 100%\n",
    "\n",
    "    return latitudes_normalized, longitudes_normalized, greenage_normalized\n",
    "\n",
    "# Example usage\n",
    "latitudes, longitudes, greenage = generate_geographical_data(samples=1, n_timesteps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "csv_file = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\sample data\\diary.csv' \n",
    "df = pd.read_csv(csv_file)  \n",
    "\n",
    "# Assuming df is your original DataFrame\n",
    "lat_lon_df = df[['lat', 'lon']].copy()\n",
    "lat_lon_df.dropna(subset=['lat', 'lon'], inplace=True)\n",
    "\n",
    "# Rename 'lat' and 'lon' columns\n",
    "lat_lon_df.rename(columns={'lat': 'Latitude', 'lon': 'Longitude'}, inplace=True)\n",
    "\n",
    "# Save the modified DataFrame to a CSV file\n",
    "lat_lon_df.to_csv('lat_lon.csv', index=False)\n",
    "\n",
    "# Select only the first 3 rows\n",
    "first_three_rows = lat_lon_df.head(100)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "first_three_rows.to_csv('lat_lon_first_100_rows.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 3 0 3 0 4 0 3 0 3 0 1 0 0 0 1 0 5 0 5 0 3 0 6 0 3 0 3 0 4 0 6 0 4 0\n",
      "  3 0 5 0 7 0 1 0 6 0 0 1 0 0 1 0 3 0 6 0 3 0 1 0 1 0 3 0 3 0 8 0 0 0 0 0\n",
      "  0 3 0 3 0 3 0 1 0 5 0 4 0 4 0 3 0 5 0 0 0 0 0 1 0 4 0 4]]\n",
      "File saved: preprocessed\\sample_0_segment_0.npz\n",
      "File saved: preprocessed\\sample_0_segment_1.npz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "from enum import Enum\n",
    "\n",
    "# Constants\n",
    "N_DISCRETE_VALUES = 4  \n",
    "N_SAMPLES = 1\n",
    "N_TIMESTEPS = 100  # For the initial data generation\n",
    "PAD_VALUE = 0  # Define a padding value for activities and modes\n",
    "MAX_SIMU_TOKENS = 4 \n",
    "N_TRACKS = 4  # Number of \"instrument\" tracks\n",
    "N_BARS = 2  # Assuming 2 bars for simplicity, bars = days\n",
    "RESOLUTION = 8  # Assuming 8 timesteps per beat, adjust as needed, hours ?\n",
    "\n",
    "# Assuming the location track is the second track (index 1)\n",
    "LOCATION_TRACK_IDX = 1  # Adjust this index based on which track you want to use for location data\n",
    "\n",
    "class EdgeTypes(Enum):\n",
    "    TRACK = 0 # This has to be interpreted as the starting index\n",
    "    ONSET = N_TRACKS\n",
    "    NEXT = N_TRACKS + 1\n",
    "\n",
    "# N_TRACKS track types + 1 onset edge type + 1 next edge type\n",
    "N_EDGE_TYPES = N_TRACKS + 2\n",
    "\n",
    "def generate_sample_data():\n",
    "    # Generate emotions and locations with random values\n",
    "    emotions = np.random.randint(0, N_DISCRETE_VALUES, (N_SAMPLES, N_TIMESTEPS))\n",
    "    locations = np.random.randint(0, N_DISCRETE_VALUES, (N_SAMPLES, N_TIMESTEPS))\n",
    "    return emotions, locations\n",
    "\n",
    "# Generate or load real data for emotions, locations, activities, and modes\n",
    "# This is a placeholder. Replace it with actual data loading or generation\n",
    "emotions, locations = generate_sample_data()  # You need to define this function\n",
    "latitudes, longitudes, greenage = generate_geographical_data()  \n",
    "\n",
    "# Calculate window size and stride for sliding windows\n",
    "window_size = N_BARS * 4 * RESOLUTION\n",
    "stride = window_size // 2\n",
    "\n",
    "def preprocess_sample_data(csv_file, dest_dir):\n",
    "    # Load CSV and preprocess activities and modes\n",
    "    df = pd.read_csv(csv_file)\n",
    "    activities_map = {activity: i for i, activity in enumerate(df['mainActivity'].unique(), start=1)}\n",
    "    modes_map = {mode: i for i, mode in enumerate(df['howTravelled'].unique(), start=1)}\n",
    "    df['activity_int'] = df['mainActivity'].fillna('PAD').map(lambda x: activities_map.get(x, PAD_VALUE))\n",
    "    df['mode_int'] = df['howTravelled'].fillna('PAD').map(lambda x: modes_map.get(x, PAD_VALUE))\n",
    "        \n",
    "    activities = np.full((N_SAMPLES, N_TIMESTEPS), PAD_VALUE, dtype=int)\n",
    "    modes = np.full((N_SAMPLES, N_TIMESTEPS), PAD_VALUE, dtype=int)\n",
    "    activities[:min(N_SAMPLES, len(df)), :] = df['activity_int'].head(N_TIMESTEPS)\n",
    "    modes[:min(N_SAMPLES, len(df)), :] = df['mode_int'].head(N_TIMESTEPS)\n",
    "    print(activities)\n",
    "        \n",
    "    for sample_idx in range(N_SAMPLES):\n",
    "        # Initialize tensors for this sample with the full length first\n",
    "        full_c_tensor = np.zeros((N_TRACKS, N_TIMESTEPS, MAX_SIMU_TOKENS, 2), dtype=np.float32)\n",
    "\n",
    "        # Populate the full tensors\n",
    "        for t in range(N_TIMESTEPS):\n",
    "            for track_idx, data in enumerate([emotions, locations, activities, modes]):\n",
    "                value = data[sample_idx, t]\n",
    "                if track_idx == LOCATION_TRACK_IDX:\n",
    "                    # Populate latitude, longitude, and greenage in the location track\n",
    "                    full_c_tensor[track_idx, t, 0, :] = [value, 1]  # Emotion/Location value and duration\n",
    "                    full_c_tensor[track_idx, t, 1, :] = [latitudes[sample_idx, t], 1]  # Latitude\n",
    "                    # print(full_c_tensor[track_idx, t, 1, :])\n",
    "                    full_c_tensor[track_idx, t, 2, :] = [longitudes[sample_idx, t], 1]  # Longitude\n",
    "                    full_c_tensor[track_idx, t, 3, :] = [greenage[sample_idx, t], 1]  # Greenage\n",
    "                else:\n",
    "                    # Populate other tracks with their respective data\n",
    "                    full_c_tensor[track_idx, t, 0, :] = [value, 1]  # Example of populating non-location data\n",
    "\n",
    "        # Calculate full_s_tensor based on non-zero values in full_c_tensor for the current sample\n",
    "        full_s_tensor = np.any(full_c_tensor != 0, axis=(2, 3)).astype(int)\n",
    "\n",
    "        # Windowing over time\n",
    "        for start_idx in range(0, N_TIMESTEPS - window_size + 1, stride):\n",
    "            c_tensor_segment = full_c_tensor[:, start_idx:start_idx + window_size, :, :]\n",
    "            s_tensor_segment = full_s_tensor[:, start_idx:start_idx + window_size]\n",
    "\n",
    "            # Save the tensors for this segment to an .npz file\n",
    "            sample_filepath = os.path.join(dest_dir, f\"sample_{sample_idx}_segment_{start_idx//stride}.npz\")\n",
    "            try:\n",
    "                np.savez(sample_filepath, c_tensor=c_tensor_segment, s_tensor=s_tensor_segment)\n",
    "                print(f\"File saved: {sample_filepath}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to save {sample_filepath}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "csv_file = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\sample data\\diary.csv'   # Update this path to your CSV file\n",
    "dest_dir = 'preprocessed'  # Destination directory for preprocessed data\n",
    "preprocess_sample_data(csv_file, dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of c_tensor: (4, 64, 4, 2)\n",
      "Shape of s_tensor: (4, 64)\n"
     ]
    }
   ],
   "source": [
    "file_path = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_1_segment_0.npz'\n",
    "\n",
    "# Load the .npz file\n",
    "data = np.load(file_path)\n",
    "\n",
    "# Access the tensors\n",
    "c_tensor = data['c_tensor']\n",
    "s_tensor = data['s_tensor']\n",
    "\n",
    "# Print their shapes\n",
    "print(f'Shape of c_tensor: {c_tensor.shape}')\n",
    "print(f'Shape of s_tensor: {s_tensor.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        ],\n",
       "       [0.5085707 , 1.        ],\n",
       "       [0.92775226, 1.        ],\n",
       "       [0.6958991 , 1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_tensor[1, 3, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.collate import collate\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "\n",
    "def get_node_labels(s_tensor, ones_idxs):\n",
    "    # Build a tensor which has node labels in place of each activation in the\n",
    "    # stucture tensor\n",
    "    labels = torch.zeros_like(s_tensor, dtype=torch.long, \n",
    "                              device=s_tensor.device)\n",
    "    n_nodes = len(ones_idxs[0])\n",
    "    labels[ones_idxs] = torch.arange(n_nodes, device=s_tensor.device)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_track_edges(s_tensor, ones_idxs=None, node_labels=None):\n",
    "\n",
    "    track_edges = []\n",
    "\n",
    "    if ones_idxs is None:\n",
    "        # Indices where the binary structure tensor is active\n",
    "        ones_idxs = torch.nonzero(s_tensor, as_tuple=True)\n",
    "\n",
    "    if node_labels is None:\n",
    "        node_labels = get_node_labels(s_tensor, ones_idxs)\n",
    "\n",
    "    # For each track, add direct and inverse edges between consecutive nodes\n",
    "    for track in range(s_tensor.size(0)):\n",
    "        # List of active timesteps in the current track\n",
    "        tss = list(ones_idxs[1][ones_idxs[0] == track])\n",
    "        edge_type = EdgeTypes.TRACK.value + track\n",
    "        edges = [\n",
    "            # Edge tuple: (u, v, type, ts_distance). Zip is used to obtain\n",
    "            # consecutive active timesteps. Edges in different tracks have\n",
    "            # different types.\n",
    "            (node_labels[track, t1],\n",
    "             node_labels[track, t2], edge_type, t2 - t1)\n",
    "            for t1, t2 in zip(tss[:-1], tss[1:])\n",
    "        ]\n",
    "        inverse_edges = [(u, v, t, d) for (v, u, t, d) in edges]\n",
    "        track_edges.extend(edges + inverse_edges)\n",
    "\n",
    "    return torch.tensor(track_edges, dtype=torch.long)\n",
    "\n",
    "\n",
    "def get_onset_edges(s_tensor, ones_idxs=None, node_labels=None):\n",
    "\n",
    "    onset_edges = []\n",
    "    edge_type = EdgeTypes.ONSET.value\n",
    "\n",
    "    if ones_idxs is None:\n",
    "        # Indices where the binary structure tensor is active\n",
    "        ones_idxs = torch.nonzero(s_tensor, as_tuple=True)\n",
    "\n",
    "    if node_labels is None:\n",
    "        node_labels = get_node_labels(s_tensor, ones_idxs)\n",
    "\n",
    "    # Add direct and inverse edges between nodes played in the same timestep\n",
    "    for ts in range(s_tensor.size(1)):\n",
    "        # List of active tracks in the current timestep\n",
    "        tracks = list(ones_idxs[0][ones_idxs[1] == ts])\n",
    "        # Obtain all possible pairwise combinations of active tracks\n",
    "        combinations = list(itertools.combinations(tracks, 2))\n",
    "        edges = [\n",
    "            # Edge tuple: (u, v, type, ts_distance(=0)).\n",
    "            (node_labels[track1, ts], node_labels[track2, ts], edge_type, 0)\n",
    "            for track1, track2 in combinations\n",
    "        ]\n",
    "        inverse_edges = [(u, v, t, d) for (v, u, t, d) in edges]\n",
    "        onset_edges.extend(edges + inverse_edges)\n",
    "\n",
    "    return torch.tensor(onset_edges, dtype=torch.long)\n",
    "\n",
    "\n",
    "def get_next_edges(s_tensor, ones_idxs=None, node_labels=None):\n",
    "\n",
    "    next_edges = []\n",
    "    edge_type = EdgeTypes.NEXT.value\n",
    "\n",
    "    if ones_idxs is None:\n",
    "        # Indices where the binary structure tensor is active\n",
    "        ones_idxs = torch.nonzero(s_tensor, as_tuple=True)\n",
    "\n",
    "    if node_labels is None:\n",
    "        node_labels = get_node_labels(s_tensor, ones_idxs)\n",
    "\n",
    "    # List of active timesteps\n",
    "    tss = torch.nonzero(torch.any(s_tensor.bool(), dim=0)).squeeze()\n",
    "    if tss.dim() == 0:\n",
    "        return torch.tensor([], dtype=torch.long)\n",
    "\n",
    "    for i in range(tss.size(0)-1):\n",
    "        # Get consecutive active timesteps\n",
    "        t1, t2 = tss[i], tss[i+1]\n",
    "        # Get all the active tracks in the two timesteps\n",
    "        t1_tracks = ones_idxs[0][ones_idxs[1] == t1]\n",
    "        t2_tracks = ones_idxs[0][ones_idxs[1] == t2]\n",
    "\n",
    "        # Combine the source and destination tracks, removing combinations with\n",
    "        # the same source and destination track (since these represent track\n",
    "        # edges).\n",
    "        tracks_product = list(itertools.product(t1_tracks, t2_tracks))\n",
    "        tracks_product = [(track1, track2)\n",
    "                          for (track1, track2) in tracks_product\n",
    "                          if track1 != track2]\n",
    "        # Edge tuple: (u, v, type, ts_distance).\n",
    "        edges = [(node_labels[track1, t1], node_labels[track2, t2],\n",
    "                  edge_type, t2 - t1)\n",
    "                 for track1, track2 in tracks_product]\n",
    "\n",
    "        next_edges.extend(edges)\n",
    "\n",
    "    return torch.tensor(next_edges, dtype=torch.long)\n",
    "\n",
    "\n",
    "def get_track_features(s_tensor):\n",
    "\n",
    "    # Indices where the binary structure tensor is active\n",
    "    ones_idxs = torch.nonzero(s_tensor)\n",
    "\n",
    "    n_nodes = len(ones_idxs)\n",
    "    tracks = ones_idxs[:, 0]\n",
    "    n_tracks = s_tensor.size(0)\n",
    "\n",
    "    # The feature n_nodes x n_tracks tensor contains one-hot tracks\n",
    "    # representations for each node\n",
    "    features = torch.zeros((n_nodes, n_tracks))\n",
    "    features[torch.arange(n_nodes), tracks] = 1\n",
    "\n",
    "    return features\n",
    "\n",
    "def custom_collate(data_list):\n",
    "    batch = Batch.from_data_list(data_list)\n",
    "    if hasattr(batch, 'batch'):\n",
    "        batch.bars = batch.batch.clone()  # Optionally clone to ensure no accidental modification\n",
    "    return batch\n",
    "\n",
    "\n",
    "def graph_from_tensor(s_tensor):\n",
    "\n",
    "    bars = []\n",
    "\n",
    "    # Iterate over bars and construct a graph for each bar\n",
    "    for i in range(s_tensor.size(0)):\n",
    "\n",
    "        bar = s_tensor[i]\n",
    "\n",
    "        # If the bar contains no activations, add a fake one to avoid having \n",
    "        # to deal with empty graphs\n",
    "        if not torch.any(bar):\n",
    "            bar[0, 0] = 1\n",
    "\n",
    "        # Get edges from boolean activations\n",
    "        track_edges = get_track_edges(bar)\n",
    "        onset_edges = get_onset_edges(bar)\n",
    "        next_edges = get_next_edges(bar)\n",
    "        edges = [track_edges, onset_edges, next_edges]\n",
    "\n",
    "        # Concatenate edge tensors (N x 4) (if any)\n",
    "        is_edgeless = (len(track_edges) == 0 and\n",
    "                       len(onset_edges) == 0 and\n",
    "                       len(next_edges) == 0)\n",
    "        if not is_edgeless:\n",
    "            edge_list = torch.cat([x for x in edges\n",
    "                                   if torch.numel(x) > 0])\n",
    "\n",
    "        # Adapt tensor to torch_geometric's Data\n",
    "        # If no edges, add fake self-edge\n",
    "        # edge_list[:, :2] contains source and destination node labels\n",
    "        # edge_list[:, 2:] contains edge types and timestep distances\n",
    "        edge_index = (edge_list[:, :2].t().contiguous() if not is_edgeless else\n",
    "                      torch.LongTensor([[0], [0]]))\n",
    "        attrs = (edge_list[:, 2:] if not is_edgeless else\n",
    "                 torch.Tensor([[0, 0]]))\n",
    "\n",
    "        # Add one hot timestep distance to edge attributes\n",
    "        edge_attr = torch.zeros(attrs.size(0), s_tensor.shape[-1] + 1)\n",
    "        edge_attr[:, 0] = attrs[:, 0]\n",
    "        edge_attr[torch.arange(edge_attr.size(0)),\n",
    "                   attrs.long()[:, 1] + 1] = 1\n",
    "        # print(type(edge_attr))\n",
    "\n",
    "        node_features = get_track_features(bar)\n",
    "        is_drum = node_features[:, 0].bool()\n",
    "        num_nodes = torch.sum(bar, dtype=torch.long)\n",
    "        bars.append(Data(edge_index=edge_index, edge_attr=edge_attr,\n",
    "                         num_nodes=num_nodes, node_features=node_features,\n",
    "                         is_drum=is_drum).to(s_tensor.device))\n",
    "\n",
    "    # Use custom collate function to merge all Data objects into a single Batch\n",
    "    #graph = custom_collate(bars)\n",
    "\n",
    "    # Manage the batch attribute as bars, if it exists\n",
    "    #if hasattr(graph, 'batch'):\n",
    "    #    graph.bars = graph.batch.clone()  # Cloning to ensure that original batch numbers are preserved\n",
    "    #    # print(\"Bars attribute added to graph\")\n",
    "\n",
    "    #return graph\n",
    "\n",
    "    # Merge the graphs corresponding to different bars into a single big graph\n",
    "    graph, _, _ = collate(\n",
    "        Data,\n",
    "        data_list=bars,\n",
    "        increment=True,\n",
    "        add_batch=True\n",
    "    )\n",
    "\n",
    "    # Change bars assignment vector name (otherwise, Dataloader's collate\n",
    "    # would overwrite graphs.batch)\n",
    "    graph.bars = graph.batch\n",
    "\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Dataset\n",
    "\n",
    "N_PITCH_TOKENS = 131\n",
    "N_DUR_TOKENS = 99\n",
    "D_TOKEN_PAIR = N_PITCH_TOKENS + N_DUR_TOKENS\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "N_PITCH_TOKENS = 131\n",
    "N_DUR_TOKENS = 99\n",
    "D_TOKEN_PAIR = N_PITCH_TOKENS + N_DUR_TOKENS\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "class PolyphemusDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dir, n_bars=2):\n",
    "        self.dir = dir\n",
    "        self.files = [entry.path for entry in os.scandir(self.dir) if entry.is_file()]\n",
    "        self.len = len(self.files)\n",
    "        self.n_bars = n_bars\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load tensors\n",
    "        sample_path = self.files[idx]\n",
    "        data = np.load(sample_path, allow_pickle=True)\n",
    "        c_tensor = torch.tensor(data[\"c_tensor\"], dtype=torch.float32)\n",
    "        print(c_tensor.shape)\n",
    "        s_tensor = torch.tensor(data[\"s_tensor\"], dtype=torch.bool)\n",
    "        print(s_tensor.shape)\n",
    "\n",
    "        # Split continuous and categorical data\n",
    "        continuous_data = c_tensor[0:2, :, :, :]\n",
    "        print(continuous_data.shape)\n",
    "        categorical_data = c_tensor[2:4, :, :, :]\n",
    "        print(categorical_data.shape)\n",
    "        # Reshape for `n_bars` alignment\n",
    "        # Ensure that `n_timesteps` is divisible evenly by `n_bars`\n",
    "        n_timesteps_per_bar = c_tensor.shape[1] // self.n_bars\n",
    "\n",
    "        continuous_data = continuous_data.reshape(-1, self.n_bars, n_timesteps_per_bar, continuous_data.shape[2], continuous_data.shape[3])\n",
    "        continuous_data = continuous_data.permute(1, 0, 2, 3, 4)  # Reorder to (n_bars, batch, timesteps, features, feature_dims)\n",
    "\n",
    "        categorical_data = categorical_data.reshape(-1, self.n_bars, n_timesteps_per_bar, categorical_data.shape[2], categorical_data.shape[3])\n",
    "        categorical_data = categorical_data.permute(1, 0, 2, 3, 4)  # Reorder similarly\n",
    "\n",
    "        s_tensor = s_tensor.reshape(-1, self.n_bars, n_timesteps_per_bar)\n",
    "        s_tensor = s_tensor.permute(1, 0, 2)  # Reorder for structure tensor\n",
    "\n",
    "        # Process pitches as one-hot encoding\n",
    "        pitches = categorical_data[..., 0]\n",
    "        onehot_p = torch.zeros((pitches.numel(), N_PITCH_TOKENS), dtype=torch.float32)\n",
    "        onehot_p[torch.arange(pitches.numel()), pitches.long().flatten()] = 1.\n",
    "        onehot_p = onehot_p.view(*pitches.shape, N_PITCH_TOKENS)\n",
    "\n",
    "        # Assuming durations follow pitches in the second channel of the original categorical data\n",
    "        durs = categorical_data[..., 1]\n",
    "        onehot_d = torch.zeros((durs.numel(), N_DUR_TOKENS), dtype=torch.float32)\n",
    "        onehot_d[torch.arange(durs.numel()), durs.long().flatten()] = 1.\n",
    "        onehot_d = onehot_d.view(*durs.shape, N_DUR_TOKENS)\n",
    "    \n",
    "        # Match dimensions for concatenation\n",
    "        max_features = D_TOKEN_PAIR\n",
    "        pad_size = D_TOKEN_PAIR - continuous_data.shape[-1]\n",
    "        continuous_data = pad(continuous_data, (0, pad_size), \"constant\", 0)\n",
    "\n",
    "\n",
    "        print(onehot_p.shape)\n",
    "        print(onehot_d.shape)\n",
    "        print(continuous_data.shape)\n",
    "        c_tensor = torch.cat((onehot_p, onehot_d), dim=-1)\n",
    "\n",
    "        # Concatenate tensors\n",
    "        c_tensor = torch.cat([c_tensor, continuous_data], dim=0)\n",
    "\n",
    "        print(c_tensor.shape)\n",
    "\n",
    "        # Build graph structure from structure tensor\n",
    "        graph = graph_from_tensor(s_tensor)\n",
    "        #print(type(graph.edge_attr))  # This should print <class 'torch.Tensor'>\n",
    "\n",
    "        # Filter silences in order to get a sparse representation\n",
    "        c_tensor = c_tensor.reshape(-1, c_tensor.shape[-2], c_tensor.shape[-1])\n",
    "        c_tensor = c_tensor[s_tensor.reshape(-1).bool()]\n",
    "\n",
    "        graph.c_tensor = c_tensor\n",
    "        graph.s_tensor = s_tensor.float()\n",
    "\n",
    "\n",
    "        return graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Dataset\n",
    "\n",
    "# Assuming constants are defined somewhere in your codebase\n",
    "N_PITCH_TOKENS = 131\n",
    "N_DUR_TOKENS = 99\n",
    "\n",
    "# Import the PolyphemusDataset class definition here\n",
    "\n",
    "# Define the directory where your dataset is located\n",
    "data_dir = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed'\n",
    "\n",
    "# Create an instance of the PolyphemusDataset\n",
    "dataset = PolyphemusDataset(dir=data_dir, n_bars=2)\n",
    "\n",
    "# Access the first item in the dataset to check edge_attr\n",
    "graph = dataset[0]  # This will call the __getitem__ method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "Graph: Data(edge_index=[2, 2008], edge_attr=[2008, 33], num_nodes=256, node_features=[256, 4], is_drum=[256], batch=[256], ptr=[3], bars=[256], c_tensor=[256, 4, 230], s_tensor=[2, 4, 32])\n",
      "Graph.c_tensor shape: torch.Size([256, 4, 230])\n",
      "Graph.s_tensor shape: torch.Size([2, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Dataset\n",
    "\n",
    "# Constants for one-hot encoding\n",
    "N_PITCH_TOKENS = 131\n",
    "N_DUR_TOKENS = 99\n",
    "D_TOKEN_PAIR = N_PITCH_TOKENS + N_DUR_TOKENS\n",
    "\n",
    "# PolyphemusDataset class definition here (as provided in your question)\n",
    "\n",
    "# Create an instance of the PolyphemusDataset\n",
    "dir = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed'\n",
    "dataset = PolyphemusDataset(dir, n_bars=2)\n",
    "\n",
    "# Load a sample from the dataset\n",
    "sample_index = 0  # For example, load the first sample\n",
    "graph = dataset[sample_index]\n",
    "\n",
    "# Inspect the graph and its tensors\n",
    "print(\"Graph:\", graph)\n",
    "print(\"Graph.c_tensor shape:\", graph.c_tensor.shape)\n",
    "print(\"Graph.s_tensor shape:\", graph.s_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "from torch_sparse import SparseTensor, masked_select_nnz\n",
    "from torch_geometric.typing import OptTensor, Adj\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "from torch_geometric.nn.glob import GlobalAttention\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.nn.conv import RGCNConv\n",
    "\n",
    "@torch.jit._overload\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    # type: (Tensor, Tensor) -> Tensor\n",
    "    pass\n",
    "\n",
    "\n",
    "@torch.jit._overload\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    # type: (SparseTensor, Tensor) -> SparseTensor\n",
    "    pass\n",
    "\n",
    "\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    if isinstance(edge_index, Tensor):\n",
    "        return edge_index[:, edge_mask]\n",
    "    else:\n",
    "        return masked_select_nnz(edge_index, edge_mask, layout='coo')\n",
    "\n",
    "\n",
    "def masked_edge_attr(edge_attr, edge_mask):\n",
    "    return edge_attr[edge_mask, :]\n",
    "\n",
    "\n",
    "class GCL(RGCNConv):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, num_relations, nn,\n",
    "                 dropout=0.1, **kwargs):\n",
    "        super().__init__(in_channels=in_channels, out_channels=out_channels,\n",
    "                         num_relations=num_relations, **kwargs)\n",
    "        self.nn = nn\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_edge_nn()\n",
    "\n",
    "    def reset_edge_nn(self):\n",
    "        reset(self.nn)\n",
    "\n",
    "    def forward(self, x: Union[OptTensor, Tuple[OptTensor, Tensor]],\n",
    "                edge_index: Adj, edge_type: OptTensor = None,\n",
    "                edge_attr: OptTensor = None):\n",
    "        # print(f\"Before processing: x shape: {x.shape}, edge_index shape: {edge_index.shape}, edge_type shape: {edge_type.shape}, edge_attr shape: {edge_attr.shape}\")\n",
    "        # Convert input features to a pair of node features or node indices.\n",
    "        x_l: OptTensor = None\n",
    "        if isinstance(x, tuple):\n",
    "            x_l = x[0]\n",
    "        else:\n",
    "            x_l = x\n",
    "        if x_l is None:\n",
    "            x_l = torch.arange(self.in_channels_l, device=self.weight.device)\n",
    "\n",
    "        x_r: Tensor = x_l\n",
    "        if isinstance(x, tuple):\n",
    "            x_r = x[1]\n",
    "\n",
    "        size = (x_l.size(0), x_r.size(0))\n",
    "\n",
    "        if isinstance(edge_index, SparseTensor):\n",
    "            edge_type = edge_index.storage.value()\n",
    "        assert edge_type is not None\n",
    "\n",
    "        # propagate_type: (x: Tensor)\n",
    "        out = torch.zeros(x_r.size(0), self.out_channels, device=x_r.device)\n",
    "        weight = self.weight\n",
    "\n",
    "        # Basis-decomposition\n",
    "        if self.num_bases is not None:\n",
    "            weight = (self.comp @ weight.view(self.num_bases, -1)).view(\n",
    "                self.num_relations, self.in_channels_l, self.out_channels)\n",
    "\n",
    "        # Block-diagonal-decomposition\n",
    "        if self.num_blocks is not None:\n",
    "\n",
    "            if x_l.dtype == torch.long and self.num_blocks is not None:\n",
    "                raise ValueError('Block-diagonal decomposition not supported '\n",
    "                                 'for non-continuous input features.')\n",
    "\n",
    "            for i in range(self.num_relations):\n",
    "                tmp = masked_edge_index(edge_index, edge_type == i)\n",
    "                h = self.propagate(tmp, x=x_l, size=size)\n",
    "                h = h.view(-1, weight.size(1), weight.size(2))\n",
    "                h = torch.einsum('abc,bcd->abd', h, weight[i])\n",
    "                out += h.contiguous().view(-1, self.out_channels)\n",
    "\n",
    "        else:\n",
    "            # No regularization/Basis-decomposition\n",
    "            for i in range(self.num_relations):\n",
    "                tmp = masked_edge_index(edge_index, edge_type == i)\n",
    "                attr = masked_edge_attr(edge_attr, edge_type == i)\n",
    "\n",
    "                if x_l.dtype == torch.long:\n",
    "                    out += self.propagate(tmp, x=weight[i, x_l], size=size)\n",
    "                else:\n",
    "                    h = self.propagate(tmp, x=x_l, size=size,\n",
    "                                       edge_attr=attr)\n",
    "                    out = out + (h @ weight[i])\n",
    "\n",
    "        root = self.root\n",
    "        if root is not None:\n",
    "            out += root[x_r] if x_r.dtype == torch.long else x_r @ root\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "        # print(f\"After processing: Output shape: {out.shape}\")\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "\n",
    "        # Use edge nn to compute weight tensor from edge attributes\n",
    "        # (=onehot timestep distances between nodes)\n",
    "        # print(f\"Message: x_j shape: {x_j.shape}, edge_attr shape: {edge_attr.shape}\")\n",
    "        weights = self.nn(edge_attr)\n",
    "        weights = weights[..., :self.in_channels_l]\n",
    "        weights = weights.view(-1, self.in_channels_l)\n",
    "\n",
    "        out = x_j * weights\n",
    "        # print(f\"Message output shape: {out.shape}\")\n",
    "        out = F.relu(out)\n",
    "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=256, hidden_dim=256, output_dim=256,\n",
    "                 num_layers=2, activation=True, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        if num_layers == 1:\n",
    "            self.layers.append(nn.Linear(input_dim, output_dim))\n",
    "        else:\n",
    "            # Input layer (1) + Intermediate layers (n-2) + Output layer (1)\n",
    "            self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            for _ in range(num_layers - 2):\n",
    "                self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        self.activation = activation\n",
    "        self.p = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.dropout(x, p=self.p, training=self.training)\n",
    "            x = layer(x)\n",
    "            if self.activation:\n",
    "                x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=256, hidden_dim=256, n_layers=3,\n",
    "                 num_relations=3, num_dists=32, batch_norm=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.norm_layers = nn.ModuleList()\n",
    "        edge_nn = nn.Linear(num_dists, input_dim)\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.layers.append(GCL(input_dim, hidden_dim, num_relations, edge_nn))\n",
    "        if self.batch_norm:\n",
    "            self.norm_layers.append(BatchNorm(hidden_dim))\n",
    "\n",
    "        for i in range(n_layers-1):\n",
    "            self.layers.append(GCL(hidden_dim, hidden_dim,\n",
    "                                   num_relations, edge_nn))\n",
    "            if self.batch_norm:\n",
    "                self.norm_layers.append(BatchNorm(hidden_dim))\n",
    "\n",
    "        self.p = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        # print(type(edge_attr))\n",
    "        edge_type = edge_attr[:, 0]\n",
    "        edge_attr = edge_attr[:, 1:]\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            # print(f\"Layer {i}: Input shape: {x.shape}\")\n",
    "            residual = x\n",
    "            x = F.dropout(x, p=self.p, training=self.training)\n",
    "            x = self.layers[i](x, edge_index, edge_type, edge_attr)\n",
    "\n",
    "            if self.batch_norm:\n",
    "                x = self.norm_layers[i](x)\n",
    "\n",
    "            x = F.relu(x)\n",
    "            x = residual + x\n",
    "            # print(f\"Layer {i}: Output shape: {x.shape}\")\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, output_dim=64, dense_dim=64, batch_norm=False,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        if batch_norm:\n",
    "            self.conv = nn.Sequential(\n",
    "                # From (4 x 32) to (8 x 4 x 32)\n",
    "                nn.Conv2d(1, 8, 3, padding=1),\n",
    "                nn.BatchNorm2d(8),\n",
    "                nn.ReLU(True),\n",
    "                # From (8 x 4 x 32) to (8 x 4 x 8)\n",
    "                nn.MaxPool2d((1, 4), stride=(1, 4)),\n",
    "                # From (8 x 4 x 8) to (16 x 4 x 8)\n",
    "                nn.Conv2d(8, 16, 3, padding=1),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(True)\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(1, 8, 3, padding=1),\n",
    "                nn.ReLU(True),\n",
    "                nn.MaxPool2d((1, 4), stride=(1, 4)),\n",
    "                nn.Conv2d(8, 16, 3, padding=1),\n",
    "                nn.ReLU(True)\n",
    "            )\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "        # Linear layers\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(16 * 4 * 8, dense_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dense_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=64, dense_dim=64, batch_norm=False,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear decompressors\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(input_dim, dense_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dense_dim, 16 * 4 * 8),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(16, 4, 8))\n",
    "\n",
    "        # Upsample and convolutional layers\n",
    "        if batch_norm:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=(1, 4), mode='nearest'),\n",
    "                nn.Conv2d(16, 8, 3, padding=1),\n",
    "                nn.BatchNorm2d(8),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(8, 1, 3, padding=1)\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=(1, 4), mode='nearest'),\n",
    "                nn.Conv2d(16, 8, 3, padding=1),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(8, 1, 3, padding=1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.conv(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ContentEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        #self.device = device  # Store the device as an instance variable\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        # Pitch and duration embedding layers (separate layers for drums\n",
    "        # and non drums)\n",
    "        self.non_drums_pitch_emb = nn.Linear(N_PITCH_TOKENS, \n",
    "                                             self.d//2)\n",
    "        self.drums_pitch_emb = nn.Linear(N_PITCH_TOKENS, self.d//2)\n",
    "        self.dur_emb = nn.Linear(N_DUR_TOKENS, self.d//2)\n",
    "\n",
    "        # Batch norm layers\n",
    "        self.bn_non_drums = nn.BatchNorm1d(num_features=self.d//2)\n",
    "        self.bn_drums = nn.BatchNorm1d(num_features=self.d//2)\n",
    "        self.bn_dur = nn.BatchNorm1d(num_features=self.d//2)\n",
    "\n",
    "        self.chord_encoder = nn.Linear(\n",
    "            self.d * (MAX_SIMU_TOKENS-1), self.d)\n",
    "\n",
    "        self.graph_encoder = GCN(\n",
    "            dropout=self.dropout,\n",
    "            input_dim=self.d,\n",
    "            hidden_dim=self.d,\n",
    "            n_layers=self.gnn_n_layers,\n",
    "            num_relations=N_EDGE_TYPES,\n",
    "            batch_norm=self.batch_norm\n",
    "        )\n",
    "\n",
    "        # Soft attention node-aggregation layer\n",
    "        gate_nn = nn.Sequential(\n",
    "            MLP(input_dim=self.d, output_dim=1, num_layers=1,\n",
    "                activation=False, dropout=self.dropout),\n",
    "            nn.BatchNorm1d(1)\n",
    "        )\n",
    "        self.graph_attention = GlobalAttention(gate_nn)\n",
    "\n",
    "        self.bars_encoder = nn.Linear(self.n_bars * self.d, self.d)\n",
    "    \n",
    "    def forward(self, graph):\n",
    "        c_tensor = graph.c_tensor\n",
    "        print(c_tensor.shape)\n",
    "        # Discard SOS token\n",
    "        c_tensor = c_tensor[:, 1:, :]\n",
    "\n",
    "        # Get drums and non drums tensors\n",
    "        drums = c_tensor[graph.is_drum]\n",
    "        non_drums = c_tensor[torch.logical_not(graph.is_drum)]\n",
    "\n",
    "        # Compute drums embeddings\n",
    "        sz = drums.size()\n",
    "        drums_pitch = self.drums_pitch_emb(\n",
    "            drums[..., :N_PITCH_TOKENS])\n",
    "        drums_pitch = self.bn_drums(drums_pitch.view(-1, self.d//2))\n",
    "        drums_pitch = drums_pitch.view(sz[0], sz[1], self.d//2)\n",
    "        drums_dur = self.dur_emb(drums[..., N_PITCH_TOKENS:])\n",
    "        drums_dur = self.bn_dur(drums_dur.view(-1, self.d//2))\n",
    "        drums_dur = drums_dur.view(sz[0], sz[1], self.d//2)\n",
    "        drums = torch.cat((drums_pitch, drums_dur), dim=-1)\n",
    "        # n_nodes x MAX_SIMU_TOKENS x d\n",
    "\n",
    "        # Compute non drums embeddings\n",
    "        sz = non_drums.size()\n",
    "        non_drums_pitch = self.non_drums_pitch_emb(\n",
    "            non_drums[..., :N_PITCH_TOKENS]\n",
    "        )\n",
    "        non_drums_pitch = self.bn_non_drums(non_drums_pitch.view(-1, self.d//2))\n",
    "        non_drums_pitch = non_drums_pitch.view(sz[0], sz[1], self.d//2)\n",
    "        non_drums_dur = self.dur_emb(non_drums[..., N_PITCH_TOKENS:])\n",
    "        non_drums_dur = self.bn_dur(non_drums_dur.view(-1, self.d//2))\n",
    "        non_drums_dur = non_drums_dur.view(sz[0], sz[1], self.d//2)\n",
    "        non_drums = torch.cat((non_drums_pitch, non_drums_dur), dim=-1)\n",
    "        # n_nodes x MAX_SIMU_TOKENS x d\n",
    "\n",
    "        # Compute chord embeddings (drums and non drums)\n",
    "        drums = self.chord_encoder(\n",
    "            drums.view(-1, self.d * (MAX_SIMU_TOKENS-1))\n",
    "        )\n",
    "        non_drums = self.chord_encoder(\n",
    "            non_drums.view(-1, self.d * (MAX_SIMU_TOKENS-1))\n",
    "        )\n",
    "        drums = F.relu(drums)\n",
    "        non_drums = F.relu(non_drums)\n",
    "        drums = self.dropout_layer(drums)\n",
    "        non_drums = self.dropout_layer(non_drums)\n",
    "        # n_nodes x d\n",
    "\n",
    "        # Merge drums and non drums\n",
    "        out = torch.zeros((c_tensor.size(0), self.d), device=self.device,\n",
    "                          dtype=drums.dtype)\n",
    "        out[graph.is_drum] = drums\n",
    "        out[torch.logical_not(graph.is_drum)] = non_drums\n",
    "        # n_nodes x d\n",
    "\n",
    "        # Set initial graph node states to intermediate chord representations \n",
    "        # and pass through GCN\n",
    "        graph.x = out\n",
    "        graph.distinct_bars = graph.bars + self.n_bars*graph.batch\n",
    "        out = self.graph_encoder(graph)\n",
    "        # n_nodes x d\n",
    "\n",
    "        # Aggregate final node states into bar encodings with soft attention\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            out = self.graph_attention(out, batch=graph.distinct_bars)\n",
    "        # bs x n_bars x d\n",
    "\n",
    "        out = out.view(-1, self.n_bars * self.d)\n",
    "        # bs x (n_bars*d)\n",
    "        z_c = self.bars_encoder(out)\n",
    "        # bs x d\n",
    "        \n",
    "        return z_c\n",
    "\n",
    "\n",
    "class StructureEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.cnn_encoder = CNNEncoder(\n",
    "            dense_dim=self.d,\n",
    "            output_dim=self.d,\n",
    "            dropout=self.dropout,\n",
    "            batch_norm=self.batch_norm\n",
    "        )\n",
    "        self.bars_encoder = nn.Linear(self.n_bars * self.d, self.d)\n",
    "        # self.bars_encoder = nn.Linear(self.d, self.d)\n",
    "    \n",
    "    def forward(self, graph):\n",
    "        s_tensor = graph.s_tensor\n",
    "        # print(f\"Initial s_tensor shape: {s_tensor.shape}\")  # Initial shape: [bs, N_TRACKS, feature_dim]\n",
    "\n",
    "        # Adjust the reshaping for CNN input\n",
    "        # We assume feature_dim is the total dimension for each track, needing to be reshaped\n",
    "        # to fit CNN expectations. If your CNN expects [batch_size, channels, height, width],\n",
    "        # and here you interpret N_TRACKS as channels, your reshaping needs to reflect this.\n",
    "        s_tensor_reshaped = s_tensor.view(-1, N_TRACKS , self.resolution* 4 )  # Reshape to include a channel dimension\n",
    "\n",
    "        # Feed into CNN, the expected shape might need to adjust based on CNN's expected input dimensions\n",
    "        out = self.cnn_encoder(s_tensor_reshaped)\n",
    "        # print(f\"After CNN encoder, out shape: {out.shape}\")  # Check CNN output shape\n",
    "\n",
    "        # Correct the reshaping to re-establish batch dimension\n",
    "        out = out.view(-1, self.n_bars * self.d)  # Reshape to [bs, n_bars * d]\n",
    "        # print(f\"After view, out shape: {out.shape}\")\n",
    "        # print(f\"Shape before linear layer: {out.shape}\")\n",
    "        z_s = self.bars_encoder(out)\n",
    "        # print(f\"Final z_s shape: {z_s.shape}\")  # Expect [bs, d]\n",
    "\n",
    "        return z_s\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.s_encoder = StructureEncoder(**kwargs)\n",
    "        self.c_encoder = ContentEncoder(**kwargs)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        # Linear layer that merges content and structure representations\n",
    "        self.linear_merge = nn.Linear(2*self.d, self.d)\n",
    "        self.bn_linear_merge = nn.BatchNorm1d(num_features=self.d)\n",
    "\n",
    "        self.linear_mu = nn.Linear(self.d, self.d)\n",
    "        self.linear_log_var = nn.Linear(self.d, self.d)\n",
    "\n",
    "    def forward(self, graph):\n",
    "        \n",
    "        z_s = self.s_encoder(graph)\n",
    "        z_c = self.c_encoder(graph)\n",
    "        \n",
    "        # Merge content and structure representations\n",
    "        # print(f\"z_c shape: {z_c.shape}\")\n",
    "        # print(f\"z_s shape: {z_s.shape}\")\n",
    "        z_g = torch.cat((z_c, z_s), dim=1)\n",
    "        z_g = self.dropout_layer(z_g)\n",
    "        z_g = self.linear_merge(z_g)\n",
    "        z_g = self.bn_linear_merge(z_g)\n",
    "        z_g = F.relu(z_g)\n",
    "\n",
    "        # Compute mu and log(std^2)\n",
    "        z_g = self.dropout_layer(z_g)\n",
    "        mu = self.linear_mu(z_g)\n",
    "        log_var = self.linear_log_var(z_g)\n",
    "\n",
    "        return mu, log_var\n",
    "\n",
    "\n",
    "class StructureDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.bars_decoder = nn.Linear(self.d, self.d * self.n_bars)\n",
    "        #self.bars_decoder = nn.Linear(self.d, self.d)\n",
    "        self.cnn_decoder = CNNDecoder(\n",
    "            input_dim=self.d,\n",
    "            dense_dim=self.d,\n",
    "            dropout=self.dropout,\n",
    "            batch_norm=self.batch_norm\n",
    "        )\n",
    "\n",
    "    def forward(self, z_s):\n",
    "        # z_s: bs x d\n",
    "        out = self.bars_decoder(z_s)  # bs x (n_bars*d)\n",
    "        # out = self.cnn_decoder(out)\n",
    "        out = self.cnn_decoder(out.reshape(-1, self.d))\n",
    "        out = out.view(z_s.size(0), self.n_bars, N_TRACKS, -1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ContentDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.bars_decoder = nn.Linear(self.d, self.d * self.n_bars)\n",
    "\n",
    "        self.graph_decoder = GCN(\n",
    "            dropout=self.dropout,\n",
    "            input_dim=self.d,\n",
    "            hidden_dim=self.d,\n",
    "            n_layers=self.gnn_n_layers,\n",
    "            num_relations=N_EDGE_TYPES,\n",
    "            batch_norm=self.batch_norm\n",
    "        )\n",
    "\n",
    "        self.chord_decoder = nn.Linear(\n",
    "            self.d, self.d*(MAX_SIMU_TOKENS-1))\n",
    "\n",
    "        # Pitch and duration (un)embedding linear layers\n",
    "        self.drums_pitch_emb = nn.Linear(self.d//2, N_PITCH_TOKENS)\n",
    "        self.non_drums_pitch_emb = nn.Linear(\n",
    "            self.d//2, N_PITCH_TOKENS)\n",
    "        self.dur_emb = nn.Linear(self.d//2, N_DUR_TOKENS)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "    def forward(self, z_c, s):\n",
    "\n",
    "        out = self.bars_decoder(z_c)  # bs x (n_bars*d)\n",
    "\n",
    "        # Initialize node features with corresponding z_bar\n",
    "        # and propagate with GNN\n",
    "        s.distinct_bars = s.bars + self.n_bars*s.batch\n",
    "        _, counts = torch.unique(s.distinct_bars, return_counts=True)\n",
    "        out = out.view(-1, self.d)\n",
    "        out = torch.repeat_interleave(out, counts, axis=0)  # n_nodes x d\n",
    "        s.x = out\n",
    "        out = self.graph_decoder(s)  # n_nodes x d\n",
    "\n",
    "        out = self.chord_decoder(out)  # n_nodes x (MAX_SIMU_TOKENS*d)\n",
    "        out = out.view(-1, MAX_SIMU_TOKENS-1, self.d)\n",
    "\n",
    "        drums = out[s.is_drum]  # n_nodes_drums x MAX_SIMU_TOKENS x d\n",
    "        non_drums = out[torch.logical_not(s.is_drum)]\n",
    "        # n_nodes_non_drums x MAX_SIMU_TOKENS x d\n",
    "\n",
    "        # Obtain final pitch and dur logits (softmax will be applied\n",
    "        # outside forward)\n",
    "        non_drums = self.dropout_layer(non_drums)\n",
    "        drums = self.dropout_layer(drums)\n",
    "\n",
    "        drums_pitch = self.drums_pitch_emb(drums[..., :self.d//2])\n",
    "        drums_dur = self.dur_emb(drums[..., self.d//2:])\n",
    "        drums = torch.cat((drums_pitch, drums_dur), dim=-1)\n",
    "        # n_nodes_drums x MAX_SIMU_TOKENS x d_token\n",
    "\n",
    "        non_drums_pitch = self.non_drums_pitch_emb(non_drums[..., :self.d//2])\n",
    "        non_drums_dur = self.dur_emb(non_drums[..., self.d//2:])\n",
    "        non_drums = torch.cat((non_drums_pitch, non_drums_dur), dim=-1)\n",
    "        # n_nodes_non_drums x MAX_SIMU_TOKENS x d_token\n",
    "\n",
    "        # Merge drums and non-drums in the final output tensor\n",
    "        d_token = D_TOKEN_PAIR\n",
    "        out = torch.zeros((s.num_nodes, MAX_SIMU_TOKENS-1, d_token),\n",
    "                          device=self.device, dtype=drums.dtype)\n",
    "        out[s.is_drum] = drums\n",
    "        out[torch.logical_not(s.is_drum)] = non_drums\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.lin_decoder = nn.Linear(self.d, 2 * self.d)\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features=2*self.d)\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        self.s_decoder = StructureDecoder(**kwargs)\n",
    "        self.c_decoder = ContentDecoder(**kwargs)\n",
    "\n",
    "        self.sigmoid_thresh = 0.5\n",
    "\n",
    "    def _structure_from_binary(self, s_tensor):\n",
    "\n",
    "        # Create graph structures for each batch\n",
    "        s = []\n",
    "        for i in range(s_tensor.size(0)):\n",
    "            s.append(graph_from_tensor(s_tensor[i]))\n",
    "\n",
    "        # Create batch of graphs from single graphs\n",
    "        s = Batch.from_data_list(s, exclude_keys=['batch'])\n",
    "        s = s.to(next(self.parameters()).device)\n",
    "\n",
    "        return s\n",
    "\n",
    "    def _binary_from_logits(self, s_logits):\n",
    "\n",
    "        # Hard threshold instead of sampling gives more pleasant results\n",
    "        s_tensor = torch.sigmoid(s_logits)\n",
    "        s_tensor[s_tensor >= self.sigmoid_thresh] = 1\n",
    "        s_tensor[s_tensor < self.sigmoid_thresh] = 0\n",
    "        s_tensor = s_tensor.bool()\n",
    "        \n",
    "        # Avoid empty bars by creating a fake activation for each empty\n",
    "        # (n_tracks x n_timesteps) bar matrix in position [0, 0]\n",
    "        empty_mask = ~s_tensor.any(dim=-1).any(dim=-1)\n",
    "        idxs = torch.nonzero(empty_mask, as_tuple=True)\n",
    "        s_tensor[idxs + (0, 0)] = True\n",
    "\n",
    "        return s_tensor\n",
    "\n",
    "    def _structure_from_logits(self, s_logits):\n",
    "\n",
    "        # Compute binary structure tensor from logits and build torch geometric\n",
    "        # structure from binary tensor\n",
    "        s_tensor = self._binary_from_logits(s_logits)\n",
    "        s = self._structure_from_binary(s_tensor)\n",
    "\n",
    "        return s\n",
    "\n",
    "    def forward(self, z, s=None):\n",
    "\n",
    "        # Obtain z_s and z_c from z\n",
    "        z = self.lin_decoder(z)\n",
    "        z = self.batch_norm(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.dropout(z)  # bs x (2*d)\n",
    "        z_s, z_c = z[:, :self.d], z[:, self.d:]\n",
    "\n",
    "        # Obtain the tensor containing structure logits\n",
    "        s_logits = self.s_decoder(z_s)\n",
    "\n",
    "        if s is None:\n",
    "            # Build torch geometric graph structure from structure logits.\n",
    "            # This step involves non differentiable operations.\n",
    "            # No gradients pass through here.\n",
    "            s = self._structure_from_logits(s_logits.detach())\n",
    "\n",
    "        # Obtain the tensor containing content logits\n",
    "        c_logits = self.c_decoder(z_c, s)\n",
    "\n",
    "        return s_logits, c_logits\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(**kwargs)\n",
    "        self.decoder = Decoder(**kwargs)\n",
    "\n",
    "    def forward(self, graph):\n",
    "\n",
    "        # Encoder pass\n",
    "        mu, log_var = self.encoder(graph)\n",
    "\n",
    "        # Reparameterization trick\n",
    "        z = torch.exp(0.5 * log_var)\n",
    "        z = z * torch.randn_like(z)\n",
    "        z = z + mu\n",
    "\n",
    "        # Decoder pass\n",
    "        out = self.decoder(z, graph)\n",
    "\n",
    "        return out, mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PitchToken(Enum):\n",
    "    SOS = 128\n",
    "    EOS = 129\n",
    "    PAD = 130\n",
    "\n",
    "\n",
    "N_PITCH_TOKENS = 131\n",
    "MAX_PITCH_TOKEN = 127\n",
    "\n",
    "\n",
    "# Duration tokens have values in the range [0, 98]. Tokens from 0 to 95 have to\n",
    "# be interpreted as durations from 1 to 96 timesteps.\n",
    "class DurationToken(Enum):\n",
    "    SOS = 96\n",
    "    EOS = 97\n",
    "    PAD = 98\n",
    "    \n",
    "def append_dict(dest_d, source_d):\n",
    "\n",
    "    for k, v in source_d.items():\n",
    "        dest_d[k].append(v)\n",
    "\n",
    "def print_divider():\n",
    "    print('—' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from statistics import mean\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "import pprint\n",
    "import math\n",
    "\n",
    "#import constants\n",
    "#from constants import PitchToken, DurationToken\n",
    "#from utils import append_dict, print_divider\n",
    "\n",
    "\n",
    "class StepBetaScheduler():\n",
    "    def __init__(self, anneal_start, beta_max, step_size, anneal_end):\n",
    "        self.anneal_start = anneal_start\n",
    "        self.beta_max = beta_max\n",
    "        self.step_size = step_size\n",
    "        self.anneal_end = anneal_end\n",
    "\n",
    "        self.update_steps = 0\n",
    "        self.beta = 0\n",
    "        n_steps = self.beta_max // self.step_size\n",
    "        self.inc_every = (self.anneal_end-self.anneal_start) // n_steps\n",
    "\n",
    "    def step(self):\n",
    "        self.update_steps += 1\n",
    "\n",
    "        if (self.update_steps >= self.anneal_start or\n",
    "                self.update_steps < self.anneal_end):\n",
    "            # If we are annealing, update beta according to current step\n",
    "            curr_step = (self.update_steps-self.anneal_start) // self.inc_every\n",
    "            self.beta = self.step_size * (curr_step+1)\n",
    "            \n",
    "        return self.beta\n",
    "\n",
    "\n",
    "class ExpDecayLRScheduler():\n",
    "    def __init__(self, optimizer, peak_lr, warmup_steps, final_lr_scale,\n",
    "                 decay_steps):\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.peak_lr = peak_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.decay_steps = decay_steps\n",
    "\n",
    "        # Find the decay factor needed to reach the specified\n",
    "        # learning rate scale after decay_steps steps\n",
    "        self.decay_factor = -math.log(final_lr_scale) / self.decay_steps\n",
    "\n",
    "        self.update_steps = 0\n",
    "\n",
    "    def set_lr(self, optimizer, lr):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def step(self):\n",
    "        self.update_steps += 1\n",
    "\n",
    "        if self.update_steps <= self.warmup_steps:\n",
    "            self.lr = self.peak_lr\n",
    "        else:\n",
    "            # Decay lr exponentially\n",
    "            steps_after_warmup = self.update_steps - self. warmup_steps\n",
    "            self.lr = \\\n",
    "                self.peak_lr * math.exp(-self.decay_factor*steps_after_warmup)\n",
    "\n",
    "        self.set_lr(self.optimizer, self.lr)\n",
    "\n",
    "        return self.lr\n",
    "\n",
    "\n",
    "class PolyphemusTrainer():\n",
    "\n",
    "    def __init__(self, model_dir, model, optimizer, init_lr=1e-4,\n",
    "                 lr_scheduler=None, beta_scheduler=None, device=None, \n",
    "                 print_every=1, save_every=1, eval_every=100, \n",
    "                 iters_to_accumulate=1, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.model_dir = model_dir\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.init_lr = init_lr\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.beta_scheduler = beta_scheduler\n",
    "        self.device = device if device is not None else torch.device(\"cpu\")\n",
    "        self.cuda = True if self.device.type == 'cuda' else False\n",
    "        self.print_every = print_every\n",
    "        self.save_every = save_every\n",
    "        self.eval_every = eval_every\n",
    "        self.iters_to_accumulate = iters_to_accumulate\n",
    "\n",
    "        # Losses (ignoring PAD tokens)\n",
    "        self.bce_unreduced = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        self.ce_p = nn.CrossEntropyLoss(ignore_index=PitchToken.PAD.value)\n",
    "        self.ce_d = nn.CrossEntropyLoss(ignore_index=DurationToken.PAD.value)\n",
    "\n",
    "        # Training stats\n",
    "        self.tr_losses = defaultdict(list)\n",
    "        self.tr_accuracies = defaultdict(list)\n",
    "        self.val_losses = defaultdict(list)\n",
    "        self.val_accuracies = defaultdict(list)\n",
    "        self.lrs = []\n",
    "        self.betas = []\n",
    "        self.times = []\n",
    "\n",
    "    def train(self, trainloader, validloader=None, epochs=100, early_exit=None):\n",
    "\n",
    "        self.tot_batches = 0\n",
    "        self.beta = 0\n",
    "        self.min_val_loss = np.inf\n",
    "\n",
    "        start = time.time()\n",
    "        self.times.append(start)\n",
    "\n",
    "        self.model.train()\n",
    "        scaler = torch.cuda.amp.GradScaler() if self.cuda else None\n",
    "        self.optimizer.zero_grad()\n",
    "        progress_bar = tqdm(range(len(trainloader)))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.cur_epoch = epoch\n",
    "            for batch_idx, graph in enumerate(trainloader):\n",
    "                self.cur_batch_idx = batch_idx\n",
    "\n",
    "                # Move batch of graphs to device. Note: a single graph here\n",
    "                # represents a bar in the original sequence.\n",
    "                graph = graph.to(self.device)\n",
    "                s_tensor, c_tensor = graph.s_tensor, graph.c_tensor\n",
    "\n",
    "                with torch.cuda.amp.autocast(enabled=self.cuda):\n",
    "                    # Forward pass to obtain mu, log(sigma^2), computed by the\n",
    "                    # encoder, and structure and content logits, computed by the\n",
    "                    # decoder\n",
    "                    (s_logits, c_logits), mu, log_var = self.model(graph)\n",
    "\n",
    "                    # Compute losses\n",
    "                    tot_loss, losses = self._losses(\n",
    "                        s_tensor, s_logits,\n",
    "                        c_tensor, c_logits,\n",
    "                        mu, log_var\n",
    "                    )\n",
    "                    tot_loss = tot_loss / self.iters_to_accumulate\n",
    "\n",
    "                # Backpropagation\n",
    "                if self.cuda:\n",
    "                    scaler.scale(tot_loss).backward()\n",
    "                else:\n",
    "                    tot_loss.backward()\n",
    "\n",
    "                # Update weights with accumulated gradients\n",
    "                if (self.tot_batches + 1) % self.iters_to_accumulate == 0:\n",
    "\n",
    "                    if self.cuda:\n",
    "                        scaler.step(self.optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        self.optimizer.step()\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                    # Update lr and beta\n",
    "                    if self.lr_scheduler is not None:\n",
    "                        self.lr_scheduler.step()\n",
    "                    if self.beta_scheduler is not None:\n",
    "                        self.beta_scheduler.step()\n",
    "\n",
    "                # Compute accuracies\n",
    "                accs = self._accuracies(\n",
    "                    s_tensor, s_logits,\n",
    "                    c_tensor, c_logits,\n",
    "                    graph.is_drum\n",
    "                )\n",
    "\n",
    "                # Update the stats\n",
    "                append_dict(self.tr_losses, losses)\n",
    "                append_dict(self.tr_accuracies, accs)\n",
    "                last_lr = (self.lr_scheduler.lr\n",
    "                           if self.lr_scheduler is not None else self.init_lr)\n",
    "                self.lrs.append(last_lr)\n",
    "                self.betas.append(self.beta)\n",
    "                now = time.time()\n",
    "                self.times.append(now)\n",
    "\n",
    "                # Print stats\n",
    "                if (self.tot_batches + 1) % self.print_every == 0:\n",
    "                    print(\"Training on batch {}/{} of epoch {}/{} complete.\"\n",
    "                          .format(batch_idx+1,\n",
    "                                  len(trainloader),\n",
    "                                  epoch+1,\n",
    "                                  epochs))\n",
    "                    self._print_stats()\n",
    "                    print_divider()\n",
    "\n",
    "                # Eval on VL every `eval_every` gradient updates\n",
    "                if (validloader is not None and\n",
    "                        (self.tot_batches + 1) % self.eval_every == 0):\n",
    "\n",
    "                    # Evaluate on VL\n",
    "                    print(\"\\nEvaluating on validation set...\\n\")\n",
    "                    val_losses, val_accuracies = self.evaluate(validloader)\n",
    "\n",
    "                    # Update stats\n",
    "                    append_dict(self.val_losses, val_losses)\n",
    "                    append_dict(self.val_accuracies, val_accuracies)\n",
    "\n",
    "                    print(\"Val losses:\")\n",
    "                    print(val_losses)\n",
    "                    print(\"Val accuracies:\")\n",
    "                    print(val_accuracies)\n",
    "\n",
    "                    # Save model if VL loss (tot) reached a new minimum\n",
    "                    # Example check before accessing 'tot'\n",
    "                    if 'tot' in val_losses:\n",
    "                        tot_loss = val_losses['tot']\n",
    "                    else:\n",
    "                        # Handle the missing key, e.g., set a default value or raise a more informative error\n",
    "                        tot_loss = 0  # or some default value\n",
    "                        print(\"Key 'tot' not found in val_losses. Setting tot_loss to 0.\")\n",
    "\n",
    "                    if tot_loss < self.min_val_loss:\n",
    "                        print(\"\\nValidation loss improved.\")\n",
    "                        print(\"Saving new best model to disk...\\n\")\n",
    "                        self._save_model('best_model')\n",
    "                        self.min_val_loss = tot_loss\n",
    "\n",
    "                    self.model.train()\n",
    "\n",
    "                progress_bar.update(1)\n",
    "\n",
    "                # Save model and stats on disk\n",
    "                if (self.save_every > 0 and\n",
    "                        (self.tot_batches + 1) % self.save_every == 0):\n",
    "                    self._save_model('checkpoint')\n",
    "\n",
    "                # Stop prematurely if early_exit is set and reached\n",
    "                if (early_exit is not None and\n",
    "                        (self.tot_batches + 1) > early_exit):\n",
    "                    break\n",
    "\n",
    "                self.tot_batches += 1\n",
    "\n",
    "        end = time.time()\n",
    "        hours, rem = divmod(end-start, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        print(\"Training completed in (h:m:s): {:0>2}:{:0>2}:{:05.2f}\"\n",
    "              .format(int(hours), int(minutes), seconds))\n",
    "\n",
    "        self._save_model('checkpoint')\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        print(f\"Starting evaluation with {len(loader)} batches...\")\n",
    "        losses = defaultdict(list)\n",
    "        accs = defaultdict(list)\n",
    "\n",
    "        self.model.eval()\n",
    "        progress_bar = tqdm(range(len(loader)))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _, graph in enumerate(loader):\n",
    "\n",
    "                # Get the inputs and move them to device\n",
    "                graph = graph.to(self.device)\n",
    "                s_tensor, c_tensor = graph.s_tensor, graph.c_tensor\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    # Forward pass, get the reconstructions\n",
    "                    (s_logits, c_logits), mu, log_var = self.model(graph)\n",
    "\n",
    "                    _, losses_b = self._losses(\n",
    "                        s_tensor, s_logits,\n",
    "                        c_tensor, c_logits,\n",
    "                        mu, log_var\n",
    "                    )\n",
    "\n",
    "                accs_b = self._accuracies(\n",
    "                    s_tensor, s_logits,\n",
    "                    c_tensor, c_logits,\n",
    "                    graph.is_drum\n",
    "                )\n",
    "\n",
    "                # Save losses and accuracies\n",
    "                append_dict(losses, losses_b)\n",
    "                append_dict(accs, accs_b)\n",
    "\n",
    "                progress_bar.update(1)\n",
    "\n",
    "        # Compute avg losses and accuracies\n",
    "        avg_losses = {}\n",
    "        for k, l in losses.items():\n",
    "            avg_losses[k] = mean(l)\n",
    "\n",
    "        avg_accs = {}\n",
    "        for k, l in accs.items():\n",
    "            avg_accs[k] = mean(l)\n",
    "\n",
    "        return avg_losses, avg_accs\n",
    "\n",
    "    def _losses(self, s_tensor, s_logits, c_tensor, c_logits, mu, log_var):\n",
    "\n",
    "        # Do not consider SOS token\n",
    "        c_tensor = c_tensor[..., 1:, :]\n",
    "        c_logits = c_logits.reshape(-1, c_logits.size(-1))\n",
    "        c_tensor = c_tensor.reshape(-1, c_tensor.size(-1))\n",
    "\n",
    "        # Reshape logits to match s_tensor dimensions:\n",
    "        # n_graphs (in batch) x n_tracks x n_timesteps\n",
    "        s_logits = s_tensor.reshape(-1, *s_logits.shape[2:])\n",
    "\n",
    "        # Binary structure tensor loss (binary cross entropy)\n",
    "        s_loss = self.bce_unreduced(\n",
    "            s_logits.view(-1), s_tensor.view(-1).float())\n",
    "        s_loss = torch.mean(s_loss)\n",
    "\n",
    "        # Content tensor loss (pitches)\n",
    "        # argmax is used to obtain token ids from onehot rep\n",
    "        pitch_logits = c_logits[:, :N_PITCH_TOKENS]\n",
    "        pitch_true = c_tensor[:, :N_PITCH_TOKENS].argmax(dim=1)\n",
    "        pitch_loss = self.ce_p(pitch_logits, pitch_true)\n",
    "\n",
    "        # Content tensor loss (durations)\n",
    "        dur_logits = c_logits[:, N_PITCH_TOKENS:]\n",
    "        dur_true = c_tensor[:, N_PITCH_TOKENS:].argmax(dim=1)\n",
    "        dur_loss = self.ce_d(dur_logits, dur_true)\n",
    "\n",
    "        # Kullback-Leibler divergence loss\n",
    "        # Derivation in Kingma, Diederik P., and Max Welling. \"Auto-encoding\n",
    "        # variational bayes.\" (2013), Appendix B.\n",
    "        # (https://arxiv.org/pdf/1312.6114.pdf)\n",
    "        kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(),\n",
    "                                    dim=1)\n",
    "        kld_loss = torch.mean(kld_loss)\n",
    "\n",
    "        # Reconstruction loss and total loss\n",
    "        rec_loss = pitch_loss + dur_loss + s_loss\n",
    "        tot_loss = rec_loss + self.beta*kld_loss\n",
    "\n",
    "        losses = {\n",
    "            'tot': tot_loss.item(),\n",
    "            'pitch': pitch_loss.item(),\n",
    "            'dur': dur_loss.item(),\n",
    "            'structure': s_loss.item(),\n",
    "            'reconstruction': rec_loss.item(),\n",
    "            'kld': kld_loss.item(),\n",
    "            'beta*kld': self.beta*kld_loss.item()\n",
    "        }\n",
    "\n",
    "        return tot_loss, losses\n",
    "\n",
    "    def _accuracies(self, s_tensor, s_logits, c_tensor, c_logits, is_drum):\n",
    "\n",
    "        # Do not consider SOS token\n",
    "        c_tensor = c_tensor[..., 1:, :]\n",
    "\n",
    "        # Reshape logits to match s_tensor dimensions:\n",
    "        # n_graphs (in batch) x n_tracks x n_timesteps\n",
    "        s_logits = s_tensor.reshape(-1, *s_logits.shape[2:])\n",
    "\n",
    "        # Note accuracy considers both pitches and durations\n",
    "        note_acc = self._note_accuracy(c_logits, c_tensor)\n",
    "\n",
    "        pitch_acc = self._pitch_accuracy(c_logits, c_tensor)\n",
    "\n",
    "        # Compute pitch accuracies for drums and non drums separately\n",
    "        pitch_acc_drums = self._pitch_accuracy(\n",
    "            c_logits, c_tensor, drums=True, is_drum=is_drum\n",
    "        )\n",
    "        pitch_acc_non_drums = self._pitch_accuracy(\n",
    "            c_logits, c_tensor, drums=False, is_drum=is_drum\n",
    "        )\n",
    "\n",
    "        dur_acc = self._duration_accuracy(c_logits, c_tensor)\n",
    "\n",
    "        s_acc = self._structure_accuracy(s_logits, s_tensor)\n",
    "        s_precision = self._structure_precision(s_logits, s_tensor)\n",
    "        s_recall = self._structure_recall(s_logits, s_tensor)\n",
    "        s_f1 = (2*s_recall*s_precision / (s_recall+s_precision))\n",
    "\n",
    "        accs = {\n",
    "            'note': note_acc.item(),\n",
    "            'pitch': pitch_acc.item(),\n",
    "            'pitch_drums': pitch_acc_drums.item(),\n",
    "            'pitch_non_drums': pitch_acc_non_drums.item(),\n",
    "            'dur': dur_acc.item(),\n",
    "            's_acc': s_acc.item(),\n",
    "            's_precision': s_precision.item(),\n",
    "            's_recall': s_recall.item(),\n",
    "            's_f1': s_f1.item()\n",
    "        }\n",
    "\n",
    "        return accs\n",
    "\n",
    "    def _pitch_accuracy(self, c_logits, c_tensor, drums=None, is_drum=None):\n",
    "\n",
    "        # When drums is None, just compute the global pitch accuracy without\n",
    "        # distinguishing between drum and non drum pitches\n",
    "        if drums is not None:\n",
    "            if drums:\n",
    "                c_logits = c_logits[is_drum]\n",
    "                c_tensor = c_tensor[is_drum]\n",
    "            else:\n",
    "                c_logits = c_logits[torch.logical_not(is_drum)]\n",
    "                c_tensor = c_tensor[torch.logical_not(is_drum)]\n",
    "\n",
    "        # Apply softmax to obtain pitch reconstructions\n",
    "        pitch_rec = c_logits[..., :N_PITCH_TOKENS]\n",
    "        pitch_rec = F.softmax(pitch_rec, dim=-1)\n",
    "        pitch_rec = torch.argmax(pitch_rec, dim=-1)\n",
    "\n",
    "        pitch_true = c_tensor[..., :N_PITCH_TOKENS]\n",
    "        pitch_true = torch.argmax(pitch_true, dim=-1)\n",
    "\n",
    "        # Do not consider PAD tokens when computing accuracies\n",
    "        not_pad = (pitch_true != PitchToken.PAD.value)\n",
    "\n",
    "        correct = (pitch_rec == pitch_true)\n",
    "        correct = torch.logical_and(correct, not_pad)\n",
    "\n",
    "        return torch.sum(correct) / torch.sum(not_pad)\n",
    "\n",
    "    def _duration_accuracy(self, c_logits, c_tensor):\n",
    "\n",
    "        # Apply softmax to obtain reconstructed durations\n",
    "        dur_rec = c_logits[..., N_PITCH_TOKENS:]\n",
    "        dur_rec = F.softmax(dur_rec, dim=-1)\n",
    "        dur_rec = torch.argmax(dur_rec, dim=-1)\n",
    "\n",
    "        dur_true = c_tensor[..., N_PITCH_TOKENS:]\n",
    "        dur_true = torch.argmax(dur_true, dim=-1)\n",
    "\n",
    "        # Do not consider PAD tokens when computing accuracies\n",
    "        not_pad = (dur_true != DurationToken.PAD.value)\n",
    "\n",
    "        correct = (dur_rec == dur_true)\n",
    "        correct = torch.logical_and(correct, not_pad)\n",
    "\n",
    "        return torch.sum(correct) / torch.sum(not_pad)\n",
    "\n",
    "    def _note_accuracy(self, c_logits, c_tensor):\n",
    "\n",
    "        # Apply softmax to obtain pitch reconstructions\n",
    "        pitch_rec = c_logits[..., :N_PITCH_TOKENS]\n",
    "        pitch_rec = F.softmax(pitch_rec, dim=-1)\n",
    "        pitch_rec = torch.argmax(pitch_rec, dim=-1)\n",
    "\n",
    "        pitch_true = c_tensor[..., :N_PITCH_TOKENS]\n",
    "        pitch_true = torch.argmax(pitch_true, dim=-1)\n",
    "\n",
    "        not_pad_p = (pitch_true != PitchToken.PAD.value)\n",
    "\n",
    "        correct_p = (pitch_rec == pitch_true)\n",
    "        correct_p = torch.logical_and(correct_p, not_pad_p)\n",
    "\n",
    "        dur_rec = c_logits[..., N_PITCH_TOKENS:]\n",
    "        dur_rec = F.softmax(dur_rec, dim=-1)\n",
    "        dur_rec = torch.argmax(dur_rec, dim=-1)\n",
    "\n",
    "        dur_true = c_tensor[..., N_PITCH_TOKENS:]\n",
    "        dur_true = torch.argmax(dur_true, dim=-1)\n",
    "\n",
    "        not_pad_d = (dur_true != DurationToken.PAD.value)\n",
    "\n",
    "        correct_d = (dur_rec == dur_true)\n",
    "        correct_d = torch.logical_and(correct_d, not_pad_d)\n",
    "\n",
    "        note_accuracy = torch.sum(\n",
    "            torch.logical_and(correct_p, correct_d)) / torch.sum(not_pad_p)\n",
    "\n",
    "        return note_accuracy\n",
    "\n",
    "    def _structure_accuracy(self, s_logits, s_tensor):\n",
    "\n",
    "        s_logits = torch.sigmoid(s_logits)\n",
    "        s_logits[s_logits < 0.5] = 0\n",
    "        s_logits[s_logits >= 0.5] = 1\n",
    "\n",
    "        return torch.sum(s_logits == s_tensor) / s_tensor.numel()\n",
    "\n",
    "    def _structure_precision(self, s_logits, s_tensor):\n",
    "\n",
    "        s_logits = torch.sigmoid(s_logits)\n",
    "        s_logits[s_logits < 0.5] = 0\n",
    "        s_logits[s_logits >= 0.5] = 1\n",
    "\n",
    "        tp = torch.sum(s_tensor[s_logits == 1])\n",
    "\n",
    "        return tp / torch.sum(s_logits)\n",
    "\n",
    "    def _structure_recall(self, s_logits, s_tensor):\n",
    "\n",
    "        s_logits = torch.sigmoid(s_logits)\n",
    "        s_logits[s_logits < 0.5] = 0\n",
    "        s_logits[s_logits >= 0.5] = 1\n",
    "\n",
    "        tp = torch.sum(s_tensor[s_logits == 1])\n",
    "\n",
    "        return tp / torch.sum(s_tensor)\n",
    "\n",
    "    def _save_model(self, filename):\n",
    "\n",
    "        path = os.path.join(self.model_dir, filename)\n",
    "        print(\"Saving model to disk...\")\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': self.cur_epoch,\n",
    "            'batch': self.cur_batch_idx,\n",
    "            'tot_batches': self.tot_batches,\n",
    "            'betas': self.betas,\n",
    "            'min_val_loss': self.min_val_loss,\n",
    "            'print_every': self.print_every,\n",
    "            'save_every': self.save_every,\n",
    "            'eval_every': self.eval_every,\n",
    "            'lrs': self.lrs,\n",
    "            'tr_losses': self.tr_losses,\n",
    "            'tr_accuracies': self.tr_accuracies,\n",
    "            'val_losses': self.val_losses,\n",
    "            'val_accuracies': self.val_accuracies,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict()\n",
    "        }, path)\n",
    "\n",
    "        print(\"The model has been successfully saved.\")\n",
    "\n",
    "    def _print_stats(self):\n",
    "\n",
    "        hours, rem = divmod(self.times[-1]-self.times[0], 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        print(\"Elapsed time from start (h:m:s): {:0>2}:{:0>2}:{:05.2f}\"\n",
    "              .format(int(hours), int(minutes), seconds))\n",
    "\n",
    "        # Take mean of the last non-printed batches for each loss and accuracy\n",
    "        avg_losses = {}\n",
    "        for k, l in self.tr_losses.items():\n",
    "            v = mean(l[-self.print_every:])\n",
    "            avg_losses[k] = round(v, 2)\n",
    "\n",
    "        avg_accs = {}\n",
    "        for k, l in self.tr_accuracies.items():\n",
    "            v = mean(l[-self.print_every:])\n",
    "            avg_accs[k] = round(v, 2)\n",
    "\n",
    "        print(\"Losses:\")\n",
    "        pprint.pprint(avg_losses, indent=2)\n",
    "\n",
    "        print(\"Accuracies:\")\n",
    "        pprint.pprint(avg_accs, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "import random\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "\n",
    "\n",
    "def print_params(model):\n",
    "\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "\n",
    "    for name, parameter in model.named_parameters():\n",
    "\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params += param\n",
    "\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Parameters: {total_params}\")\n",
    "\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the configuration file C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\training.json...\n",
      "Preparing datasets and dataloaders...\n",
      "Creating the model and moving it to cpu device...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s222445\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------+------------+\n",
      "|                           Modules                           | Parameters |\n",
      "+-------------------------------------------------------------+------------+\n",
      "|         encoder.s_encoder.cnn_encoder.conv.0.weight         |     72     |\n",
      "|          encoder.s_encoder.cnn_encoder.conv.0.bias          |     8      |\n",
      "|         encoder.s_encoder.cnn_encoder.conv.1.weight         |     8      |\n",
      "|          encoder.s_encoder.cnn_encoder.conv.1.bias          |     8      |\n",
      "|         encoder.s_encoder.cnn_encoder.conv.4.weight         |    1152    |\n",
      "|          encoder.s_encoder.cnn_encoder.conv.4.bias          |     16     |\n",
      "|         encoder.s_encoder.cnn_encoder.conv.5.weight         |     16     |\n",
      "|          encoder.s_encoder.cnn_encoder.conv.5.bias          |     16     |\n",
      "|          encoder.s_encoder.cnn_encoder.lin.1.weight         |   262144   |\n",
      "|           encoder.s_encoder.cnn_encoder.lin.1.bias          |    512     |\n",
      "|          encoder.s_encoder.cnn_encoder.lin.4.weight         |   262144   |\n",
      "|           encoder.s_encoder.cnn_encoder.lin.4.bias          |    512     |\n",
      "|            encoder.s_encoder.bars_encoder.weight            |   524288   |\n",
      "|             encoder.s_encoder.bars_encoder.bias             |    512     |\n",
      "|         encoder.c_encoder.non_drums_pitch_emb.weight        |   33536    |\n",
      "|          encoder.c_encoder.non_drums_pitch_emb.bias         |    256     |\n",
      "|           encoder.c_encoder.drums_pitch_emb.weight          |   33536    |\n",
      "|            encoder.c_encoder.drums_pitch_emb.bias           |    256     |\n",
      "|               encoder.c_encoder.dur_emb.weight              |   25344    |\n",
      "|                encoder.c_encoder.dur_emb.bias               |    256     |\n",
      "|            encoder.c_encoder.bn_non_drums.weight            |    256     |\n",
      "|             encoder.c_encoder.bn_non_drums.bias             |    256     |\n",
      "|              encoder.c_encoder.bn_drums.weight              |    256     |\n",
      "|               encoder.c_encoder.bn_drums.bias               |    256     |\n",
      "|               encoder.c_encoder.bn_dur.weight               |    256     |\n",
      "|                encoder.c_encoder.bn_dur.bias                |    256     |\n",
      "|            encoder.c_encoder.chord_encoder.weight           |   786432   |\n",
      "|             encoder.c_encoder.chord_encoder.bias            |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.0.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.0.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.0.bias        |    512     |\n",
      "|      encoder.c_encoder.graph_encoder.layers.0.nn.weight     |   16384    |\n",
      "|       encoder.c_encoder.graph_encoder.layers.0.nn.bias      |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.1.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.1.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.1.bias        |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.2.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.2.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.2.bias        |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.3.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.3.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.3.bias        |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.4.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.4.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.4.bias        |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.5.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.5.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.5.bias        |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.6.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.6.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.6.bias        |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.7.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.7.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.7.bias        |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.0.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.0.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.1.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.1.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.2.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.2.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.3.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.3.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.4.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.4.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.5.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.5.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.6.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.6.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.7.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.7.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_attention.gate_nn.0.layers.0.weight |    512     |\n",
      "|  encoder.c_encoder.graph_attention.gate_nn.0.layers.0.bias  |     1      |\n",
      "|      encoder.c_encoder.graph_attention.gate_nn.1.weight     |     1      |\n",
      "|       encoder.c_encoder.graph_attention.gate_nn.1.bias      |     1      |\n",
      "|            encoder.c_encoder.bars_encoder.weight            |   524288   |\n",
      "|             encoder.c_encoder.bars_encoder.bias             |    512     |\n",
      "|                 encoder.linear_merge.weight                 |   524288   |\n",
      "|                  encoder.linear_merge.bias                  |    512     |\n",
      "|                encoder.bn_linear_merge.weight               |    512     |\n",
      "|                 encoder.bn_linear_merge.bias                |    512     |\n",
      "|                   encoder.linear_mu.weight                  |   262144   |\n",
      "|                    encoder.linear_mu.bias                   |    512     |\n",
      "|                encoder.linear_log_var.weight                |   262144   |\n",
      "|                 encoder.linear_log_var.bias                 |    512     |\n",
      "|                  decoder.lin_decoder.weight                 |   524288   |\n",
      "|                   decoder.lin_decoder.bias                  |    1024    |\n",
      "|                  decoder.batch_norm.weight                  |    1024    |\n",
      "|                   decoder.batch_norm.bias                   |    1024    |\n",
      "|            decoder.s_decoder.bars_decoder.weight            |   524288   |\n",
      "|             decoder.s_decoder.bars_decoder.bias             |    1024    |\n",
      "|          decoder.s_decoder.cnn_decoder.lin.1.weight         |   262144   |\n",
      "|           decoder.s_decoder.cnn_decoder.lin.1.bias          |    512     |\n",
      "|          decoder.s_decoder.cnn_decoder.lin.4.weight         |   262144   |\n",
      "|           decoder.s_decoder.cnn_decoder.lin.4.bias          |    512     |\n",
      "|         decoder.s_decoder.cnn_decoder.conv.1.weight         |    1152    |\n",
      "|          decoder.s_decoder.cnn_decoder.conv.1.bias          |     8      |\n",
      "|         decoder.s_decoder.cnn_decoder.conv.2.weight         |     8      |\n",
      "|          decoder.s_decoder.cnn_decoder.conv.2.bias          |     8      |\n",
      "|         decoder.s_decoder.cnn_decoder.conv.4.weight         |     72     |\n",
      "|          decoder.s_decoder.cnn_decoder.conv.4.bias          |     1      |\n",
      "|            decoder.c_decoder.bars_decoder.weight            |   524288   |\n",
      "|             decoder.c_decoder.bars_decoder.bias             |    1024    |\n",
      "|       decoder.c_decoder.graph_decoder.layers.0.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.0.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.0.bias        |    512     |\n",
      "|      decoder.c_decoder.graph_decoder.layers.0.nn.weight     |   16384    |\n",
      "|       decoder.c_decoder.graph_decoder.layers.0.nn.bias      |    512     |\n",
      "|       decoder.c_decoder.graph_decoder.layers.1.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.1.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.1.bias        |    512     |\n",
      "|       decoder.c_decoder.graph_decoder.layers.2.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.2.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.2.bias        |    512     |\n",
      "|       decoder.c_decoder.graph_decoder.layers.3.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.3.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.3.bias        |    512     |\n",
      "|       decoder.c_decoder.graph_decoder.layers.4.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.4.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.4.bias        |    512     |\n",
      "|       decoder.c_decoder.graph_decoder.layers.5.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.5.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.5.bias        |    512     |\n",
      "|       decoder.c_decoder.graph_decoder.layers.6.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.6.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.6.bias        |    512     |\n",
      "|       decoder.c_decoder.graph_decoder.layers.7.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.7.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.7.bias        |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.0.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.0.module.bias  |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.1.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.1.module.bias  |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.2.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.2.module.bias  |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.3.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.3.module.bias  |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.4.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.4.module.bias  |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.5.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.5.module.bias  |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.6.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.6.module.bias  |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.7.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.7.module.bias  |    512     |\n",
      "|            decoder.c_decoder.chord_decoder.weight           |   786432   |\n",
      "|             decoder.c_decoder.chord_decoder.bias            |    1536    |\n",
      "|           decoder.c_decoder.drums_pitch_emb.weight          |   33536    |\n",
      "|            decoder.c_decoder.drums_pitch_emb.bias           |    131     |\n",
      "|         decoder.c_decoder.non_drums_pitch_emb.weight        |   33536    |\n",
      "|          decoder.c_decoder.non_drums_pitch_emb.bias         |    131     |\n",
      "|               decoder.c_decoder.dur_emb.weight              |   25344    |\n",
      "|                decoder.c_decoder.dur_emb.bias               |     99     |\n",
      "+-------------------------------------------------------------+------------+\n",
      "Total Trainable Parameters: 35913309\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([512, 4, 230])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:02<00:04,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 1/3 of epoch 1/5 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:02.38\n",
      "Losses:\n",
      "{ 'beta*kld': 0.0,\n",
      "  'dur': 6.04,\n",
      "  'kld': 53.9,\n",
      "  'pitch': 5.89,\n",
      "  'reconstruction': 12.24,\n",
      "  'structure': 0.31,\n",
      "  'tot': 12.24}\n",
      "Accuracies:\n",
      "{ 'dur': 0.0,\n",
      "  'note': 0.0,\n",
      "  'pitch': 0.0,\n",
      "  'pitch_drums': 0.0,\n",
      "  'pitch_non_drums': 0.0,\n",
      "  's_acc': 1.0,\n",
      "  's_f1': 1.0,\n",
      "  's_precision': 1.0,\n",
      "  's_recall': 1.0}\n",
      "————————————————————————————————————————\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([512, 4, 230])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:04<00:02,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 2/3 of epoch 1/5 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:04.34\n",
      "Losses:\n",
      "{ 'beta*kld': 0.0,\n",
      "  'dur': 5.99,\n",
      "  'kld': 59.03,\n",
      "  'pitch': 5.87,\n",
      "  'reconstruction': 12.17,\n",
      "  'structure': 0.31,\n",
      "  'tot': 12.17}\n",
      "Accuracies:\n",
      "{ 'dur': 0.0,\n",
      "  'note': 0.0,\n",
      "  'pitch': 0.0,\n",
      "  'pitch_drums': 0.0,\n",
      "  'pitch_non_drums': 0.0,\n",
      "  's_acc': 1.0,\n",
      "  's_f1': 1.0,\n",
      "  's_precision': 1.0,\n",
      "  's_recall': 1.0}\n",
      "————————————————————————————————————————\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([512, 4, 230])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:06<00:00,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 3/3 of epoch 1/5 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:06.23\n",
      "Losses:\n",
      "{ 'beta*kld': 0.0,\n",
      "  'dur': 4.63,\n",
      "  'kld': 51.14,\n",
      "  'pitch': 4.79,\n",
      "  'reconstruction': 9.73,\n",
      "  'structure': 0.31,\n",
      "  'tot': 9.73}\n",
      "Accuracies:\n",
      "{ 'dur': 0.07,\n",
      "  'note': 0.0,\n",
      "  'pitch': 0.0,\n",
      "  'pitch_drums': 0.0,\n",
      "  'pitch_non_drums': 0.0,\n",
      "  's_acc': 1.0,\n",
      "  's_f1': 1.0,\n",
      "  's_precision': 1.0,\n",
      "  's_recall': 1.0}\n",
      "————————————————————————————————————————\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([512, 4, 230])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:08,  1.94s/it]                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 1/3 of epoch 2/5 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:08.06\n",
      "Losses:\n",
      "{ 'beta*kld': 0.0,\n",
      "  'dur': 3.29,\n",
      "  'kld': 48.34,\n",
      "  'pitch': 3.76,\n",
      "  'reconstruction': 7.36,\n",
      "  'structure': 0.31,\n",
      "  'tot': 7.36}\n",
      "Accuracies:\n",
      "{ 'dur': 0.29,\n",
      "  'note': 0.06,\n",
      "  'pitch': 0.21,\n",
      "  'pitch_drums': 0.0,\n",
      "  'pitch_non_drums': 0.28,\n",
      "  's_acc': 1.0,\n",
      "  's_f1': 1.0,\n",
      "  's_precision': 1.0,\n",
      "  's_recall': 1.0}\n",
      "————————————————————————————————————————\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([512, 4, 230])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:09,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 2/3 of epoch 2/5 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:10.02\n",
      "Losses:\n",
      "{ 'beta*kld': 0.0,\n",
      "  'dur': 2.09,\n",
      "  'kld': 48.69,\n",
      "  'pitch': 2.82,\n",
      "  'reconstruction': 5.23,\n",
      "  'structure': 0.31,\n",
      "  'tot': 5.23}\n",
      "Accuracies:\n",
      "{ 'dur': 0.5,\n",
      "  'note': 0.27,\n",
      "  'pitch': 0.58,\n",
      "  'pitch_drums': 0.0,\n",
      "  'pitch_non_drums': 0.77,\n",
      "  's_acc': 1.0,\n",
      "  's_f1': 1.0,\n",
      "  's_precision': 1.0,\n",
      "  's_recall': 1.0}\n",
      "————————————————————————————————————————\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([512, 4, 230])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:12,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 3/3 of epoch 2/5 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:12.14\n",
      "Losses:\n",
      "{ 'beta*kld': 0.0,\n",
      "  'dur': 1.1,\n",
      "  'kld': 45.69,\n",
      "  'pitch': 2.02,\n",
      "  'reconstruction': 3.43,\n",
      "  'structure': 0.31,\n",
      "  'tot': 3.43}\n",
      "Accuracies:\n",
      "{ 'dur': 0.99,\n",
      "  'note': 0.68,\n",
      "  'pitch': 0.68,\n",
      "  'pitch_drums': 0.08,\n",
      "  'pitch_non_drums': 0.88,\n",
      "  's_acc': 1.0,\n",
      "  's_f1': 1.0,\n",
      "  's_precision': 1.0,\n",
      "  's_recall': 1.0}\n",
      "————————————————————————————————————————\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([512, 4, 230])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:14,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 1/3 of epoch 3/5 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:14.45\n",
      "Losses:\n",
      "{ 'beta*kld': 0.0,\n",
      "  'dur': 0.48,\n",
      "  'kld': 46.55,\n",
      "  'pitch': 1.49,\n",
      "  'reconstruction': 2.28,\n",
      "  'structure': 0.31,\n",
      "  'tot': 2.28}\n",
      "Accuracies:\n",
      "{ 'dur': 1.0,\n",
      "  'note': 0.72,\n",
      "  'pitch': 0.72,\n",
      "  'pitch_drums': 0.23,\n",
      "  'pitch_non_drums': 0.89,\n",
      "  's_acc': 1.0,\n",
      "  's_f1': 1.0,\n",
      "  's_precision': 1.0,\n",
      "  's_recall': 1.0}\n",
      "————————————————————————————————————————\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([512, 4, 230])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:16,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 2/3 of epoch 3/5 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:16.64\n",
      "Losses:\n",
      "{ 'beta*kld': 0.0,\n",
      "  'dur': 0.18,\n",
      "  'kld': 41.3,\n",
      "  'pitch': 1.11,\n",
      "  'reconstruction': 1.6,\n",
      "  'structure': 0.31,\n",
      "  'tot': 1.6}\n",
      "Accuracies:\n",
      "{ 'dur': 1.0,\n",
      "  'note': 0.77,\n",
      "  'pitch': 0.77,\n",
      "  'pitch_drums': 0.41,\n",
      "  'pitch_non_drums': 0.89,\n",
      "  's_acc': 1.0,\n",
      "  's_f1': 1.0,\n",
      "  's_precision': 1.0,\n",
      "  's_recall': 1.0}\n",
      "————————————————————————————————————————\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([512, 4, 230])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:18,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 3/3 of epoch 3/5 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:18.78\n",
      "Losses:\n",
      "{ 'beta*kld': 0.0,\n",
      "  'dur': 0.07,\n",
      "  'kld': 42.64,\n",
      "  'pitch': 0.8,\n",
      "  'reconstruction': 1.18,\n",
      "  'structure': 0.31,\n",
      "  'tot': 1.18}\n",
      "Accuracies:\n",
      "{ 'dur': 1.0,\n",
      "  'note': 0.85,\n",
      "  'pitch': 0.85,\n",
      "  'pitch_drums': 0.55,\n",
      "  'pitch_non_drums': 0.95,\n",
      "  's_acc': 1.0,\n",
      "  's_f1': 1.0,\n",
      "  's_precision': 1.0,\n",
      "  's_recall': 1.0}\n",
      "————————————————————————————————————————\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([512, 4, 230])\n",
      "Training on batch 1/3 of epoch 4/5 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:20.86\n",
      "Losses:\n",
      "{ 'beta*kld': 0.0,\n",
      "  'dur': 0.02,\n",
      "  'kld': 42.98,\n",
      "  'pitch': 0.62,\n",
      "  'reconstruction': 0.96,\n",
      "  'structure': 0.31,\n",
      "  'tot': 0.96}\n",
      "Accuracies:\n",
      "{ 'dur': 1.0,\n",
      "  'note': 0.84,\n",
      "  'pitch': 0.84,\n",
      "  'pitch_drums': 0.54,\n",
      "  'pitch_non_drums': 0.94,\n",
      "  's_acc': 1.0,\n",
      "  's_f1': 1.0,\n",
      "  's_precision': 1.0,\n",
      "  's_recall': 1.0}\n",
      "————————————————————————————————————————\n",
      "\n",
      "Evaluating on validation set...\n",
      "\n",
      "Starting evaluation with 0 batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val losses:\n",
      "{}\n",
      "Val accuracies:\n",
      "{}\n",
      "Key 'tot' not found in val_losses. Setting tot_loss to 0.\n",
      "\n",
      "Validation loss improved.\n",
      "Saving new best model to disk...\n",
      "\n",
      "Saving model to disk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [00:21,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has been successfully saved.\n",
      "Saving model to disk...\n",
      "The model has been successfully saved.\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([512, 4, 230])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:24,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 2/3 of epoch 4/5 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:24.28\n",
      "Losses:\n",
      "{ 'beta*kld': 0.0,\n",
      "  'dur': 0.01,\n",
      "  'kld': 44.01,\n",
      "  'pitch': 0.44,\n",
      "  'reconstruction': 0.76,\n",
      "  'structure': 0.31,\n",
      "  'tot': 0.76}\n",
      "Accuracies:\n",
      "{ 'dur': 1.0,\n",
      "  'note': 0.88,\n",
      "  'pitch': 0.88,\n",
      "  'pitch_drums': 0.59,\n",
      "  'pitch_non_drums': 0.97,\n",
      "  's_acc': 1.0,\n",
      "  's_f1': 1.0,\n",
      "  's_precision': 1.0,\n",
      "  's_recall': 1.0}\n",
      "————————————————————————————————————————\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([512, 4, 230])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:26,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 3/3 of epoch 4/5 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:26.28\n",
      "Losses:\n",
      "{ 'beta*kld': 0.0,\n",
      "  'dur': 0.0,\n",
      "  'kld': 48.18,\n",
      "  'pitch': 0.26,\n",
      "  'reconstruction': 0.58,\n",
      "  'structure': 0.31,\n",
      "  'tot': 0.58}\n",
      "Accuracies:\n",
      "{ 'dur': 1.0,\n",
      "  'note': 1.0,\n",
      "  'pitch': 1.0,\n",
      "  'pitch_drums': 1.0,\n",
      "  'pitch_non_drums': 1.0,\n",
      "  's_acc': 1.0,\n",
      "  's_f1': 1.0,\n",
      "  's_precision': 1.0,\n",
      "  's_recall': 1.0}\n",
      "————————————————————————————————————————\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([512, 4, 230])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:28,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 1/3 of epoch 5/5 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:28.40\n",
      "Losses:\n",
      "{ 'beta*kld': 0.0,\n",
      "  'dur': 0.0,\n",
      "  'kld': 45.26,\n",
      "  'pitch': 0.18,\n",
      "  'reconstruction': 0.49,\n",
      "  'structure': 0.31,\n",
      "  'tot': 0.49}\n",
      "Accuracies:\n",
      "{ 'dur': 1.0,\n",
      "  'note': 0.99,\n",
      "  'pitch': 0.99,\n",
      "  'pitch_drums': 1.0,\n",
      "  'pitch_non_drums': 0.98,\n",
      "  's_acc': 1.0,\n",
      "  's_f1': 1.0,\n",
      "  's_precision': 1.0,\n",
      "  's_recall': 1.0}\n",
      "————————————————————————————————————————\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([512, 4, 230])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:30,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 2/3 of epoch 5/5 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:30.57\n",
      "Losses:\n",
      "{ 'beta*kld': 0.0,\n",
      "  'dur': 0.0,\n",
      "  'kld': 44.11,\n",
      "  'pitch': 0.1,\n",
      "  'reconstruction': 0.42,\n",
      "  'structure': 0.31,\n",
      "  'tot': 0.42}\n",
      "Accuracies:\n",
      "{ 'dur': 1.0,\n",
      "  'note': 0.99,\n",
      "  'pitch': 0.99,\n",
      "  'pitch_drums': 1.0,\n",
      "  'pitch_non_drums': 0.99,\n",
      "  's_acc': 1.0,\n",
      "  's_f1': 1.0,\n",
      "  's_precision': 1.0,\n",
      "  's_recall': 1.0}\n",
      "————————————————————————————————————————\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([4, 64, 4, 2])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 64, 4, 2])\n",
      "torch.Size([2, 2, 32, 4, 131])\n",
      "torch.Size([2, 2, 32, 4, 99])\n",
      "torch.Size([2, 2, 32, 4, 230])\n",
      "torch.Size([4, 2, 32, 4, 230])\n",
      "torch.Size([512, 4, 230])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:32,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 3/3 of epoch 5/5 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:32.84\n",
      "Losses:\n",
      "{ 'beta*kld': 0.0,\n",
      "  'dur': 0.0,\n",
      "  'kld': 45.43,\n",
      "  'pitch': 0.08,\n",
      "  'reconstruction': 0.39,\n",
      "  'structure': 0.31,\n",
      "  'tot': 0.39}\n",
      "Accuracies:\n",
      "{ 'dur': 1.0,\n",
      "  'note': 0.98,\n",
      "  'pitch': 0.98,\n",
      "  'pitch_drums': 1.0,\n",
      "  'pitch_non_drums': 0.97,\n",
      "  's_acc': 1.0,\n",
      "  's_f1': 1.0,\n",
      "  's_precision': 1.0,\n",
      "  's_recall': 1.0}\n",
      "————————————————————————————————————————\n",
      "Training completed in (h:m:s): 00:00:32.84\n",
      "Saving model to disk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:33,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has been successfully saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "# Ensure you have imported PolyphemusDataset, VAE, set_seed, print_params, print_divider, PolyphemusTrainer, ExpDecayLRScheduler, StepBetaScheduler correctly\n",
    "\n",
    "# Configuration and setup\n",
    "dataset_dir = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed'\n",
    "output_dir = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\output'\n",
    "config_file = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\training.json'\n",
    "model_name = 'testing_continuous lat lon emotions'  # or None to use a UUID\n",
    "save_every = 10\n",
    "print_every = 1\n",
    "eval_flag = True  # Set to False if you don't want to perform evaluation\n",
    "eval_every = 10  \n",
    "use_gpu = False\n",
    "gpu_id = 0\n",
    "num_workers = 0\n",
    "tr_split = 0.7\n",
    "vl_split = 0.1\n",
    "max_epochs = 5\n",
    "seed = 42  # or None\n",
    "\n",
    "# Set seed for reproducibility\n",
    "if seed is not None:\n",
    "    set_seed(seed)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "#if use_gpu:\n",
    "#    torch.cuda.set_device(gpu_id)\n",
    "\n",
    "# Load training configuration from JSON file\n",
    "print(f\"Loading the configuration file {config_file}...\")\n",
    "with open(config_file, 'r') as f:\n",
    "    training_config = json.load(f)\n",
    "\n",
    "n_bars = training_config['model']['n_bars']\n",
    "batch_size = training_config['batch_size']\n",
    "\n",
    "# Prepare datasets and dataloaders\n",
    "print(\"Preparing datasets and dataloaders...\")\n",
    "dataset = PolyphemusDataset(dataset_dir, n_bars)\n",
    "\n",
    "tr_len = int(tr_split * len(dataset))\n",
    "vl_len = int(vl_split * len(dataset)) if eval_flag else 0\n",
    "ts_len = len(dataset) - tr_len - vl_len\n",
    "lengths = (tr_len, vl_len, ts_len) if eval_flag else (tr_len, len(dataset) - tr_len)\n",
    "\n",
    "split = random_split(dataset, lengths)\n",
    "tr_set, vl_set = split[0], (split[1] if eval_flag else None)\n",
    "\n",
    "trainloader = DataLoader(tr_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "validloader = DataLoader(vl_set, batch_size=batch_size, shuffle=False, num_workers=num_workers) if eval_flag else None\n",
    "\n",
    "# Model setup\n",
    "model_dir = os.path.join(output_dir, model_name or str(uuid.uuid1()))\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=False)\n",
    "\n",
    "print(f\"Creating the model and moving it to {device} device...\")\n",
    "vae = VAE(**training_config['model'], device=device).to(device)\n",
    "print_params(vae)\n",
    "\n",
    "# Optimizer and schedulers setup\n",
    "optimizer = optim.Adam(vae.parameters(), **training_config['optimizer'])\n",
    "lr_scheduler = ExpDecayLRScheduler(optimizer=optimizer, **training_config['lr_scheduler'])\n",
    "beta_scheduler = StepBetaScheduler(**training_config['beta_scheduler'])\n",
    "\n",
    "# Save configuration\n",
    "config_path = os.path.join(model_dir, 'configuration.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(training_config, f)\n",
    "\n",
    "# Training\n",
    "print(\"Starting training...\")\n",
    "trainer = PolyphemusTrainer(\n",
    "    model_dir=model_dir,\n",
    "    model=vae,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    beta_scheduler=beta_scheduler,\n",
    "    save_every=save_every,\n",
    "    print_every=print_every,\n",
    "    eval_every=eval_every,\n",
    "    device=device\n",
    ")\n",
    "trainer.train(trainloader, validloader=validloader, epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import torch\n",
    "\n",
    "def plot_structure(s_tensor, save_dir=None, name='structure'):\n",
    "\n",
    "    lines_linewidth = 1\n",
    "    axes_linewidth = 1\n",
    "    font_size = 14\n",
    "    fformat = 'svg'\n",
    "    dpi = 200\n",
    "\n",
    "    n_bars = s_tensor.shape[0]\n",
    "    figsize = (3 * n_bars, 3)\n",
    "\n",
    "    n_timesteps = s_tensor.size(2)\n",
    "    resolution = n_timesteps // 4\n",
    "    s_tensor = s_tensor.permute(1, 0, 2)\n",
    "    s_tensor = s_tensor.reshape(s_tensor.shape[0], -1)\n",
    "\n",
    "    with mpl.rc_context({'lines.linewidth': lines_linewidth,\n",
    "                         'axes.linewidth': axes_linewidth,\n",
    "                         'font.size': font_size}):\n",
    "\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.pcolormesh(s_tensor, edgecolors='k', linewidth=1)\n",
    "        ax = plt.gca()\n",
    "\n",
    "        plt.xticks(range(0, s_tensor.shape[1], resolution),\n",
    "                   range(1, 4*n_bars + 1))\n",
    "        plt.yticks(range(0, s_tensor.shape[0]), TRACKS)\n",
    "\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, name + \".\" + fformat),\n",
    "                        format=fformat, dpi=dpi)\n",
    "    \n",
    "    \n",
    "def plot_stats(stat_names, stats_tr, stats_val=None, eval_every=None, \n",
    "               labels=None, rx=None, ry=None):\n",
    "\n",
    "    for i, stat in enumerate(stat_names):\n",
    "\n",
    "        label = stat if not labels else labels[i]\n",
    "\n",
    "        plt.plot(range(1, len(stats_tr[stat])+1), stats_tr[stat],\n",
    "                label=label+' (TR)')\n",
    "\n",
    "        if stats_val:\n",
    "            plt.plot(range(eval_every, len(stats_tr[stat])+1, eval_every),\n",
    "                    stats_val[stat], '.', label=label+' (VL)')\n",
    "\n",
    "    plt.grid()\n",
    "\n",
    "    plt.ylim(ry) if ry else plt.ylim(0)\n",
    "    plt.xlim(rx) if rx else plt.xlim(0)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "# Dictionary that maps loss statistic name to plot label \n",
    "loss_labels = {\n",
    "    'tot': 'Total Loss',\n",
    "    'structure': 'Structure',\n",
    "    'pitch': 'Pitches',\n",
    "    'dur': 'Duration',\n",
    "    'reconstruction': 'Reconstruction Term',\n",
    "    'kld': 'KLD',\n",
    "    'beta*kld': 'beta * KLD'\n",
    "}\n",
    "\n",
    "\n",
    "def plot_losses(model_dir, losses, plot_val=False):\n",
    "    \n",
    "    checkpoint_path = os.path.join(model_dir, 'checkpoint')\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    labels = [loss_labels[loss] for loss in losses]\n",
    "    \n",
    "    tr_losses = checkpoint['tr_losses']\n",
    "    val_losses = checkpoint['val_losses'] if plot_val == True else None\n",
    "    eval_every = checkpoint['eval_every'] if plot_val == True else None\n",
    "\n",
    "    plot_stats(losses, tr_losses, stats_val=val_losses,\n",
    "               eval_every=eval_every, labels=labels, rx=(0))\n",
    "    \n",
    "\n",
    "# Dictionary that maps accuracy statistic name to plot label \n",
    "accuracy_labels = {\n",
    "    's_acc': 'Struct. Accuracy',\n",
    "    's_precision': 'Struct. Precision',\n",
    "    's_recall': 'Struct. Recall',\n",
    "    's_f1': 'Struct. F1',\n",
    "    'pitch': 'Pitch Accuracy',\n",
    "    'pitch_drums': 'Pitch Accuracy (Drums)',\n",
    "    'pitch_non_drums': 'Pitch Accuracy (Non Drums)',\n",
    "    'dur': 'Duration Accuracy',\n",
    "    'note': 'Note Accuracy'\n",
    "}\n",
    "\n",
    "\n",
    "def plot_accuracies(model_dir, accuracies, plot_val=False):\n",
    "    \n",
    "    checkpoint_path = os.path.join(model_dir, 'checkpoint')\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    labels = [accuracy_labels[accuracy] for accuracy in accuracies]\n",
    "    \n",
    "    tr_accuracies = checkpoint['tr_accuracies']\n",
    "    val_accuracies = checkpoint['val_accuracies'] if plot_val == True else None\n",
    "    eval_every = checkpoint['eval_every'] if plot_val == True else None\n",
    "\n",
    "    plot_stats(accuracies, tr_accuracies, stats_val=val_accuracies,\n",
    "               eval_every=eval_every, labels=labels, ry=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming your model's directory is 'model_dir'\n",
    "plot_losses(model_dir='output/testing_continuous lat lon emotions', losses=['tot', 'structure', 'pitch'], plot_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming your model's directory is 'model_dir'\n",
    "plot_accuracies(model_dir='output/testing_continuous lat lon emotions', accuracies=['s_acc', 'pitch', 'dur'], plot_val=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_original_data(data, title='Original Data', samples=5):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(samples):\n",
    "        plt.subplot(samples, 1, i+1) \n",
    "        plt.plot(data[i], marker='o', linestyle='-')\n",
    "        plt.title(f\"{title} for Sample {i+1}\")\n",
    "        plt.ylabel('Value')\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_original_data(emotions, title='Original Emotions')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
