{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "from enum import Enum\n",
    "\n",
    "# Constants definitions\n",
    "MAX_SIMU_TOKENS = 16  \n",
    "N_TRACKS = 4  # Number of \"instrument\" tracks\n",
    "N_BARS = 2  # Assuming 2 bars for simplicity, adjust as needed\n",
    "N_TIMESTEPS = 100  # Number of timesteps per bar, adjust as needed\n",
    "RESOLUTION = 8  # Assuming 8 timesteps per beat, adjust as needed\n",
    "\n",
    "# This enum contains edge type indices for each edge type\n",
    "class EdgeTypes(Enum):\n",
    "    TRACK = 0 # This has to be interpreted as the starting index\n",
    "    ONSET = N_TRACKS\n",
    "    NEXT = N_TRACKS + 1\n",
    "\n",
    "# N_TRACKS track types + 1 onset edge type + 1 next edge type\n",
    "N_EDGE_TYPES = N_TRACKS + 2\n",
    "\n",
    "# Assuming these are defined elsewhere or simplifying for this example\n",
    "N_DISCRETE_VALUES = 128\n",
    "N_SAMPLES = 100\n",
    "\n",
    "def generate_sample_data(num_samples, n_timesteps):\n",
    "    np.random.seed(42)\n",
    "    emotions = np.random.randint(0, N_DISCRETE_VALUES, (num_samples, n_timesteps))\n",
    "    locations = np.random.randint(0, N_DISCRETE_VALUES, (num_samples, n_timesteps))\n",
    "    activities = np.random.randint(0, N_DISCRETE_VALUES, (num_samples, n_timesteps))\n",
    "    modes = np.random.randint(0, N_DISCRETE_VALUES, (num_samples, n_timesteps))\n",
    "    return emotions, locations, activities, modes\n",
    "\n",
    "def preprocess_sample_data(emotions, locations, activities, modes, dest_dir):\n",
    "    num_samples = emotions.shape[0]\n",
    "    n_timesteps = emotions.shape[1]\n",
    "    window_size = N_BARS * 4 * RESOLUTION  # Calculate window size\n",
    "\n",
    "    for sample_idx in range(num_samples):\n",
    "        # Initialize tensors for this sample with the full length first\n",
    "        full_c_tensor = np.zeros((N_TRACKS, n_timesteps, MAX_SIMU_TOKENS, 2), dtype=np.int16)\n",
    "        full_s_tensor = np.zeros((N_TRACKS, n_timesteps), dtype=bool)\n",
    "\n",
    "        # Populate the full tensors\n",
    "        for t in range(n_timesteps):\n",
    "            for track_idx, data in enumerate([emotions, locations, activities, modes]):\n",
    "                value = data[sample_idx, t]\n",
    "                full_c_tensor[track_idx, t, 0, 0] = value  # Pitch as emotion value\n",
    "                full_c_tensor[track_idx, t, 0, 1] = 1  # Duration as 1 for simplicity\n",
    "                full_s_tensor[track_idx, t] = True  # Note played at this timestep\n",
    "\n",
    "        # Windowing over time\n",
    "        for start_idx in range(0, n_timesteps - window_size + 1, window_size):\n",
    "            c_tensor_segment = full_c_tensor[:, start_idx:start_idx + window_size, :, :]\n",
    "            s_tensor_segment = full_s_tensor[:, start_idx:start_idx + window_size]\n",
    "\n",
    "            # Save the tensors for this segment to an .npz file\n",
    "            sample_filepath = os.path.join(dest_dir, f\"sample_{sample_idx}_segment_{start_idx//window_size}.npz\")\n",
    "            try:\n",
    "                np.savez(sample_filepath, c_tensor=c_tensor_segment, s_tensor=s_tensor_segment)\n",
    "                print(f\"File saved: {sample_filepath}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to save {sample_filepath}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_0_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_1_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_2_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_3_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_4_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_5_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_6_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_7_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_8_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_9_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_10_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_11_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_12_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_13_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_14_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_15_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_16_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_17_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_18_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_19_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_20_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_21_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_22_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_23_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_24_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_25_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_26_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_27_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_28_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_29_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_30_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_31_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_32_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_33_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_34_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_35_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_36_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_37_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_38_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_39_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_40_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_41_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_42_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_43_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_44_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_45_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_46_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_47_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_48_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_49_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_50_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_51_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_52_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_53_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_54_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_55_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_56_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_57_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_58_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_59_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_60_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_61_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_62_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_63_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_64_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_65_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_66_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_67_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_68_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_69_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_70_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_71_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_72_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_73_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_74_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_75_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_76_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_77_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_78_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_79_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_80_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_81_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_82_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_83_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_84_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_85_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_86_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_87_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_88_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_89_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_90_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_91_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_92_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_93_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_94_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_95_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_96_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_97_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_98_segment_0.npz\n",
      "File saved: C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_99_segment_0.npz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Fixed directory\n",
    "dest_dir = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed'\n",
    "\n",
    "# Ensure the destination directory exists\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.makedirs(dest_dir)\n",
    "\n",
    "# Generate sample data\n",
    "emotions, locations, activities, modes = generate_sample_data(N_SAMPLES, N_TIMESTEPS)\n",
    "\n",
    "# Preprocess and save the sample data\n",
    "preprocess_sample_data(emotions, locations, activities, modes, dest_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of c_tensor: (4, 64, 16, 2)\n",
      "Shape of s_tensor: (4, 64)\n"
     ]
    }
   ],
   "source": [
    "file_path = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_0_segment_0.npz'\n",
    "\n",
    "# Load the .npz file\n",
    "data = np.load(file_path)\n",
    "\n",
    "# Access the tensors\n",
    "c_tensor = data['c_tensor']\n",
    "s_tensor = data['s_tensor']\n",
    "\n",
    "# Print their shapes\n",
    "print(f'Shape of c_tensor: {c_tensor.shape}')\n",
    "print(f'Shape of s_tensor: {s_tensor.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.collate import collate\n",
    "\n",
    "\n",
    "\n",
    "def get_node_labels(s_tensor, ones_idxs):\n",
    "    # Build a tensor which has node labels in place of each activation in the\n",
    "    # stucture tensor\n",
    "    labels = torch.zeros_like(s_tensor, dtype=torch.long, \n",
    "                              device=s_tensor.device)\n",
    "    n_nodes = len(ones_idxs[0])\n",
    "    labels[ones_idxs] = torch.arange(n_nodes, device=s_tensor.device)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_track_edges(s_tensor, ones_idxs=None, node_labels=None):\n",
    "\n",
    "    track_edges = []\n",
    "\n",
    "    if ones_idxs is None:\n",
    "        # Indices where the binary structure tensor is active\n",
    "        ones_idxs = torch.nonzero(s_tensor, as_tuple=True)\n",
    "\n",
    "    if node_labels is None:\n",
    "        node_labels = get_node_labels(s_tensor, ones_idxs)\n",
    "\n",
    "    # For each track, add direct and inverse edges between consecutive nodes\n",
    "    for track in range(s_tensor.size(0)):\n",
    "        # List of active timesteps in the current track\n",
    "        tss = list(ones_idxs[1][ones_idxs[0] == track])\n",
    "        edge_type = EdgeTypes.TRACK.value + track\n",
    "        edges = [\n",
    "            # Edge tuple: (u, v, type, ts_distance). Zip is used to obtain\n",
    "            # consecutive active timesteps. Edges in different tracks have\n",
    "            # different types.\n",
    "            (node_labels[track, t1],\n",
    "             node_labels[track, t2], edge_type, t2 - t1)\n",
    "            for t1, t2 in zip(tss[:-1], tss[1:])\n",
    "        ]\n",
    "        inverse_edges = [(u, v, t, d) for (v, u, t, d) in edges]\n",
    "        track_edges.extend(edges + inverse_edges)\n",
    "\n",
    "    return torch.tensor(track_edges, dtype=torch.long)\n",
    "\n",
    "\n",
    "def get_onset_edges(s_tensor, ones_idxs=None, node_labels=None):\n",
    "\n",
    "    onset_edges = []\n",
    "    edge_type = EdgeTypes.ONSET.value\n",
    "\n",
    "    if ones_idxs is None:\n",
    "        # Indices where the binary structure tensor is active\n",
    "        ones_idxs = torch.nonzero(s_tensor, as_tuple=True)\n",
    "\n",
    "    if node_labels is None:\n",
    "        node_labels = get_node_labels(s_tensor, ones_idxs)\n",
    "\n",
    "    # Add direct and inverse edges between nodes played in the same timestep\n",
    "    for ts in range(s_tensor.size(1)):\n",
    "        # List of active tracks in the current timestep\n",
    "        tracks = list(ones_idxs[0][ones_idxs[1] == ts])\n",
    "        # Obtain all possible pairwise combinations of active tracks\n",
    "        combinations = list(itertools.combinations(tracks, 2))\n",
    "        edges = [\n",
    "            # Edge tuple: (u, v, type, ts_distance(=0)).\n",
    "            (node_labels[track1, ts], node_labels[track2, ts], edge_type, 0)\n",
    "            for track1, track2 in combinations\n",
    "        ]\n",
    "        inverse_edges = [(u, v, t, d) for (v, u, t, d) in edges]\n",
    "        onset_edges.extend(edges + inverse_edges)\n",
    "\n",
    "    return torch.tensor(onset_edges, dtype=torch.long)\n",
    "\n",
    "\n",
    "def get_next_edges(s_tensor, ones_idxs=None, node_labels=None):\n",
    "\n",
    "    next_edges = []\n",
    "    edge_type = EdgeTypes.NEXT.value\n",
    "\n",
    "    if ones_idxs is None:\n",
    "        # Indices where the binary structure tensor is active\n",
    "        ones_idxs = torch.nonzero(s_tensor, as_tuple=True)\n",
    "\n",
    "    if node_labels is None:\n",
    "        node_labels = get_node_labels(s_tensor, ones_idxs)\n",
    "\n",
    "    # List of active timesteps\n",
    "    tss = torch.nonzero(torch.any(s_tensor.bool(), dim=0)).squeeze()\n",
    "    if tss.dim() == 0:\n",
    "        return torch.tensor([], dtype=torch.long)\n",
    "\n",
    "    for i in range(tss.size(0)-1):\n",
    "        # Get consecutive active timesteps\n",
    "        t1, t2 = tss[i], tss[i+1]\n",
    "        # Get all the active tracks in the two timesteps\n",
    "        t1_tracks = ones_idxs[0][ones_idxs[1] == t1]\n",
    "        t2_tracks = ones_idxs[0][ones_idxs[1] == t2]\n",
    "\n",
    "        # Combine the source and destination tracks, removing combinations with\n",
    "        # the same source and destination track (since these represent track\n",
    "        # edges).\n",
    "        tracks_product = list(itertools.product(t1_tracks, t2_tracks))\n",
    "        tracks_product = [(track1, track2)\n",
    "                          for (track1, track2) in tracks_product\n",
    "                          if track1 != track2]\n",
    "        # Edge tuple: (u, v, type, ts_distance).\n",
    "        edges = [(node_labels[track1, t1], node_labels[track2, t2],\n",
    "                  edge_type, t2 - t1)\n",
    "                 for track1, track2 in tracks_product]\n",
    "\n",
    "        next_edges.extend(edges)\n",
    "\n",
    "    return torch.tensor(next_edges, dtype=torch.long)\n",
    "\n",
    "\n",
    "def get_track_features(s_tensor):\n",
    "\n",
    "    # Indices where the binary structure tensor is active\n",
    "    ones_idxs = torch.nonzero(s_tensor)\n",
    "\n",
    "    n_nodes = len(ones_idxs)\n",
    "    tracks = ones_idxs[:, 0]\n",
    "    n_tracks = s_tensor.size(0)\n",
    "\n",
    "    # The feature n_nodes x n_tracks tensor contains one-hot tracks\n",
    "    # representations for each node\n",
    "    features = torch.zeros((n_nodes, n_tracks))\n",
    "    features[torch.arange(n_nodes), tracks] = 1\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def graph_from_tensor(s_tensor):\n",
    "\n",
    "    bars = []\n",
    "\n",
    "    # Iterate over bars and construct a graph for each bar\n",
    "    for i in range(s_tensor.size(0)):\n",
    "\n",
    "        bar = s_tensor[i]\n",
    "\n",
    "        # If the bar contains no activations, add a fake one to avoid having \n",
    "        # to deal with empty graphs\n",
    "        if not torch.any(bar):\n",
    "            bar[0, 0] = 1\n",
    "\n",
    "        # Get edges from boolean activations\n",
    "        track_edges = get_track_edges(bar)\n",
    "        onset_edges = get_onset_edges(bar)\n",
    "        next_edges = get_next_edges(bar)\n",
    "        edges = [track_edges, onset_edges, next_edges]\n",
    "\n",
    "        # Concatenate edge tensors (N x 4) (if any)\n",
    "        is_edgeless = (len(track_edges) == 0 and\n",
    "                       len(onset_edges) == 0 and\n",
    "                       len(next_edges) == 0)\n",
    "        if not is_edgeless:\n",
    "            edge_list = torch.cat([x for x in edges\n",
    "                                   if torch.numel(x) > 0])\n",
    "\n",
    "        # Adapt tensor to torch_geometric's Data\n",
    "        # If no edges, add fake self-edge\n",
    "        # edge_list[:, :2] contains source and destination node labels\n",
    "        # edge_list[:, 2:] contains edge types and timestep distances\n",
    "        edge_index = (edge_list[:, :2].t().contiguous() if not is_edgeless else\n",
    "                      torch.LongTensor([[0], [0]]))\n",
    "        attrs = (edge_list[:, 2:] if not is_edgeless else\n",
    "                 torch.Tensor([[0, 0]]))\n",
    "\n",
    "        # Add one hot timestep distance to edge attributes\n",
    "        edge_attrs = torch.zeros(attrs.size(0), s_tensor.shape[-1] + 1)\n",
    "        edge_attrs[:, 0] = attrs[:, 0]\n",
    "        edge_attrs[torch.arange(edge_attrs.size(0)),\n",
    "                   attrs.long()[:, 1] + 1] = 1\n",
    "\n",
    "        node_features = get_track_features(bar)\n",
    "        is_drum = node_features[:, 0].bool()\n",
    "        num_nodes = torch.sum(bar, dtype=torch.long)\n",
    "\n",
    "        bars.append(Data(edge_index=edge_index, edge_attrs=edge_attrs,\n",
    "                         num_nodes=num_nodes, node_features=node_features,\n",
    "                         is_drum=is_drum).to(s_tensor.device))\n",
    "\n",
    "    # Merge the graphs corresponding to different bars into a single big graph\n",
    "    graph, _, _ = collate(\n",
    "        Data,\n",
    "        data_list=bars,\n",
    "        increment=True,\n",
    "        add_batch=True\n",
    "    )\n",
    "\n",
    "    # Change bars assignment vector name (otherwise, Dataloader's collate\n",
    "    # would overwrite graphs.batch)\n",
    "    graph.bars = graph.batch\n",
    "\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Dataset\n",
    "\n",
    "N_PITCH_TOKENS = 131\n",
    "N_DUR_TOKENS = 99\n",
    "D_TOKEN_PAIR = N_PITCH_TOKENS + N_DUR_TOKENS\n",
    "\n",
    "\n",
    "class PolyphemusDataset(Dataset):\n",
    "    def __init__(self, dir, n_bars=2):\n",
    "        self.dir = dir\n",
    "        self.files = [entry.path for entry in os.scandir(self.dir) if entry.is_file()]\n",
    "        self.len = len(self.files)\n",
    "        self.n_bars = n_bars\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load tensors\n",
    "        sample_path = self.files[idx]  # Directly use the path string\n",
    "        data = np.load(sample_path)\n",
    "        c_tensor = torch.tensor(data[\"c_tensor\"], dtype=torch.long)\n",
    "        s_tensor = torch.tensor(data[\"s_tensor\"], dtype=torch.bool)\n",
    "\n",
    "        # From (n_tracks x n_timesteps x ...)\n",
    "        # to (n_bars x n_tracks x n_timesteps x ...)\n",
    "        c_tensor = c_tensor.reshape(c_tensor.shape[0], self.n_bars, -1,\n",
    "                                    c_tensor.shape[2], c_tensor.shape[3])\n",
    "        c_tensor = c_tensor.permute(1, 0, 2, 3, 4)\n",
    "        s_tensor = s_tensor.reshape(s_tensor.shape[0], self.n_bars, -1)\n",
    "        s_tensor = s_tensor.permute(1, 0, 2)\n",
    "\n",
    "        # From decimals to onehot (pitches)\n",
    "        pitches = c_tensor[..., 0]\n",
    "        onehot_p = torch.zeros(\n",
    "            (pitches.shape[0]*pitches.shape[1]*pitches.shape[2]*pitches.shape[3],\n",
    "             N_PITCH_TOKENS),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        onehot_p[torch.arange(0, onehot_p.shape[0]), pitches.reshape(-1)] = 1.\n",
    "        onehot_p = onehot_p.reshape(pitches.shape[0], pitches.shape[1],\n",
    "                                    pitches.shape[2], pitches.shape[3],\n",
    "                                    N_PITCH_TOKENS)\n",
    "\n",
    "        # From decimals to onehot (durations)\n",
    "        durs = c_tensor[..., 1]\n",
    "        onehot_d = torch.zeros(\n",
    "            (durs.shape[0]*durs.shape[1]*durs.shape[2]*durs.shape[3],\n",
    "             N_DUR_TOKENS),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        onehot_d[torch.arange(0, onehot_d.shape[0]), durs.reshape(-1)] = 1.\n",
    "        onehot_d = onehot_d.reshape(durs.shape[0], durs.shape[1],\n",
    "                                    durs.shape[2], durs.shape[3],\n",
    "                                    N_DUR_TOKENS)\n",
    "\n",
    "        # Concatenate pitches and durations\n",
    "        c_tensor = torch.cat((onehot_p, onehot_d), dim=-1)\n",
    "\n",
    "        # Build graph structure from structure tensor\n",
    "        graph = graph_from_tensor(s_tensor)\n",
    "\n",
    "        # Filter silences in order to get a sparse representation\n",
    "        c_tensor = c_tensor.reshape(-1, c_tensor.shape[-2], c_tensor.shape[-1])\n",
    "        c_tensor = c_tensor[s_tensor.reshape(-1).bool()]\n",
    "\n",
    "        graph.c_tensor = c_tensor\n",
    "        graph.s_tensor = s_tensor.float()\n",
    "\n",
    "        return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Dataset\n",
    "\n",
    "# Constants for one-hot encoding\n",
    "N_PITCH_TOKENS = 131\n",
    "N_DUR_TOKENS = 99\n",
    "D_TOKEN_PAIR = N_PITCH_TOKENS + N_DUR_TOKENS\n",
    "\n",
    "# PolyphemusDataset class definition here (as provided in your question)\n",
    "\n",
    "# Create an instance of the PolyphemusDataset\n",
    "dir = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed'\n",
    "dataset = PolyphemusDataset(dir, n_bars=2)\n",
    "\n",
    "# Load a sample from the dataset\n",
    "sample_index = 0  # For example, load the first sample\n",
    "graph = dataset[sample_index]\n",
    "\n",
    "# Inspect the graph and its tensors\n",
    "print(\"Graph:\", graph)\n",
    "print(\"Graph.c_tensor shape:\", graph.c_tensor.shape)\n",
    "print(\"Graph.s_tensor shape:\", graph.s_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "from torch_sparse import SparseTensor, masked_select_nnz\n",
    "from torch_geometric.typing import OptTensor, Adj\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "from torch_geometric.nn.glob import GlobalAttention\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.nn.conv import RGCNConv\n",
    "\n",
    "@torch.jit._overload\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    # type: (Tensor, Tensor) -> Tensor\n",
    "    pass\n",
    "\n",
    "\n",
    "@torch.jit._overload\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    # type: (SparseTensor, Tensor) -> SparseTensor\n",
    "    pass\n",
    "\n",
    "\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    if isinstance(edge_index, Tensor):\n",
    "        return edge_index[:, edge_mask]\n",
    "    else:\n",
    "        return masked_select_nnz(edge_index, edge_mask, layout='coo')\n",
    "\n",
    "\n",
    "def masked_edge_attrs(edge_attrs, edge_mask):\n",
    "    return edge_attrs[edge_mask, :]\n",
    "\n",
    "\n",
    "class GCL(RGCNConv):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, num_relations, nn,\n",
    "                 dropout=0.1, **kwargs):\n",
    "        super().__init__(in_channels=in_channels, out_channels=out_channels,\n",
    "                         num_relations=num_relations, **kwargs)\n",
    "        self.nn = nn\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_edge_nn()\n",
    "\n",
    "    def reset_edge_nn(self):\n",
    "        reset(self.nn)\n",
    "\n",
    "    def forward(self, x: Union[OptTensor, Tuple[OptTensor, Tensor]],\n",
    "                edge_index: Adj, edge_type: OptTensor = None,\n",
    "                edge_attr: OptTensor = None):\n",
    "\n",
    "        # Convert input features to a pair of node features or node indices.\n",
    "        x_l: OptTensor = None\n",
    "        if isinstance(x, tuple):\n",
    "            x_l = x[0]\n",
    "        else:\n",
    "            x_l = x\n",
    "        if x_l is None:\n",
    "            x_l = torch.arange(self.in_channels_l, device=self.weight.device)\n",
    "\n",
    "        x_r: Tensor = x_l\n",
    "        if isinstance(x, tuple):\n",
    "            x_r = x[1]\n",
    "\n",
    "        size = (x_l.size(0), x_r.size(0))\n",
    "\n",
    "        if isinstance(edge_index, SparseTensor):\n",
    "            edge_type = edge_index.storage.value()\n",
    "        assert edge_type is not None\n",
    "\n",
    "        # propagate_type: (x: Tensor)\n",
    "        out = torch.zeros(x_r.size(0), self.out_channels, device=x_r.device)\n",
    "        weight = self.weight\n",
    "\n",
    "        # Basis-decomposition\n",
    "        if self.num_bases is not None:\n",
    "            weight = (self.comp @ weight.view(self.num_bases, -1)).view(\n",
    "                self.num_relations, self.in_channels_l, self.out_channels)\n",
    "\n",
    "        # Block-diagonal-decomposition\n",
    "        if self.num_blocks is not None:\n",
    "\n",
    "            if x_l.dtype == torch.long and self.num_blocks is not None:\n",
    "                raise ValueError('Block-diagonal decomposition not supported '\n",
    "                                 'for non-continuous input features.')\n",
    "\n",
    "            for i in range(self.num_relations):\n",
    "                tmp = masked_edge_index(edge_index, edge_type == i)\n",
    "                h = self.propagate(tmp, x=x_l, size=size)\n",
    "                h = h.view(-1, weight.size(1), weight.size(2))\n",
    "                h = torch.einsum('abc,bcd->abd', h, weight[i])\n",
    "                out += h.contiguous().view(-1, self.out_channels)\n",
    "\n",
    "        else:\n",
    "            # No regularization/Basis-decomposition\n",
    "            for i in range(self.num_relations):\n",
    "                tmp = masked_edge_index(edge_index, edge_type == i)\n",
    "                attr = masked_edge_attrs(edge_attr, edge_type == i)\n",
    "\n",
    "                if x_l.dtype == torch.long:\n",
    "                    out += self.propagate(tmp, x=weight[i, x_l], size=size)\n",
    "                else:\n",
    "                    h = self.propagate(tmp, x=x_l, size=size,\n",
    "                                       edge_attr=attr)\n",
    "                    out = out + (h @ weight[i])\n",
    "\n",
    "        root = self.root\n",
    "        if root is not None:\n",
    "            out += root[x_r] if x_r.dtype == torch.long else x_r @ root\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "\n",
    "        # Use edge nn to compute weight tensor from edge attributes\n",
    "        # (=onehot timestep distances between nodes)\n",
    "        weights = self.nn(edge_attr)\n",
    "        weights = weights[..., :self.in_channels_l]\n",
    "        weights = weights.view(-1, self.in_channels_l)\n",
    "\n",
    "        out = x_j * weights\n",
    "        out = F.relu(out)\n",
    "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=256, hidden_dim=256, output_dim=256,\n",
    "                 num_layers=2, activation=True, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        if num_layers == 1:\n",
    "            self.layers.append(nn.Linear(input_dim, output_dim))\n",
    "        else:\n",
    "            # Input layer (1) + Intermediate layers (n-2) + Output layer (1)\n",
    "            self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            for _ in range(num_layers - 2):\n",
    "                self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        self.activation = activation\n",
    "        self.p = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.dropout(x, p=self.p, training=self.training)\n",
    "            x = layer(x)\n",
    "            if self.activation:\n",
    "                x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=256, hidden_dim=256, n_layers=3,\n",
    "                 num_relations=3, num_dists=32, batch_norm=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.norm_layers = nn.ModuleList()\n",
    "        edge_nn = nn.Linear(num_dists, input_dim)\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.layers.append(GCL(input_dim, hidden_dim, num_relations, edge_nn))\n",
    "        if self.batch_norm:\n",
    "            self.norm_layers.append(BatchNorm(hidden_dim))\n",
    "\n",
    "        for i in range(n_layers-1):\n",
    "            self.layers.append(GCL(hidden_dim, hidden_dim,\n",
    "                                   num_relations, edge_nn))\n",
    "            if self.batch_norm:\n",
    "                self.norm_layers.append(BatchNorm(hidden_dim))\n",
    "\n",
    "        self.p = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        x, edge_index, edge_attrs = data.x, data.edge_index, data.edge_attrs\n",
    "        edge_type = edge_attrs[:, 0]\n",
    "        edge_attr = edge_attrs[:, 1:]\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "\n",
    "            residual = x\n",
    "            x = F.dropout(x, p=self.p, training=self.training)\n",
    "            x = self.layers[i](x, edge_index, edge_type, edge_attr)\n",
    "\n",
    "            if self.batch_norm:\n",
    "                x = self.norm_layers[i](x)\n",
    "\n",
    "            x = F.relu(x)\n",
    "            x = residual + x\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, output_dim=256, dense_dim=256, batch_norm=False,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        if batch_norm:\n",
    "            self.conv = nn.Sequential(\n",
    "                # From (4 x 32) to (8 x 4 x 32)\n",
    "                nn.Conv2d(1, 8, 3, padding=1),\n",
    "                nn.BatchNorm2d(8),\n",
    "                nn.ReLU(True),\n",
    "                # From (8 x 4 x 32) to (8 x 4 x 8)\n",
    "                nn.MaxPool2d((1, 4), stride=(1, 4)),\n",
    "                # From (8 x 4 x 8) to (16 x 4 x 8)\n",
    "                nn.Conv2d(8, 16, 3, padding=1),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(True)\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(1, 8, 3, padding=1),\n",
    "                nn.ReLU(True),\n",
    "                nn.MaxPool2d((1, 4), stride=(1, 4)),\n",
    "                nn.Conv2d(8, 16, 3, padding=1),\n",
    "                nn.ReLU(True)\n",
    "            )\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "        # Linear layers\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(16 * 4 * 8, dense_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dense_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=256, dense_dim=256, batch_norm=False,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear decompressors\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(input_dim, dense_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dense_dim, 16 * 4 * 8),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(16, 4, 8))\n",
    "\n",
    "        # Upsample and convolutional layers\n",
    "        if batch_norm:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=(1, 4), mode='nearest'),\n",
    "                nn.Conv2d(16, 8, 3, padding=1),\n",
    "                nn.BatchNorm2d(8),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(8, 1, 3, padding=1)\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=(1, 4), mode='nearest'),\n",
    "                nn.Conv2d(16, 8, 3, padding=1),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(8, 1, 3, padding=1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.conv(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ContentEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        #self.device = device  # Store the device as an instance variable\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        # Pitch and duration embedding layers (separate layers for drums\n",
    "        # and non drums)\n",
    "        self.non_drums_pitch_emb = nn.Linear(N_PITCH_TOKENS, \n",
    "                                             self.d//2)\n",
    "        self.drums_pitch_emb = nn.Linear(N_PITCH_TOKENS, self.d//2)\n",
    "        self.dur_emb = nn.Linear(N_DUR_TOKENS, self.d//2)\n",
    "\n",
    "        # Batch norm layers\n",
    "        self.bn_non_drums = nn.BatchNorm1d(num_features=self.d//2)\n",
    "        self.bn_drums = nn.BatchNorm1d(num_features=self.d//2)\n",
    "        self.bn_dur = nn.BatchNorm1d(num_features=self.d//2)\n",
    "\n",
    "        self.chord_encoder = nn.Linear(\n",
    "            self.d * (MAX_SIMU_TOKENS-1), self.d)\n",
    "\n",
    "        self.graph_encoder = GCN(\n",
    "            dropout=self.dropout,\n",
    "            input_dim=self.d,\n",
    "            hidden_dim=self.d,\n",
    "            n_layers=self.gnn_n_layers,\n",
    "            num_relations=N_EDGE_TYPES,\n",
    "            batch_norm=self.batch_norm\n",
    "        )\n",
    "\n",
    "        # Soft attention node-aggregation layer\n",
    "        gate_nn = nn.Sequential(\n",
    "            MLP(input_dim=self.d, output_dim=1, num_layers=1,\n",
    "                activation=False, dropout=self.dropout),\n",
    "            nn.BatchNorm1d(1)\n",
    "        )\n",
    "        self.graph_attention = GlobalAttention(gate_nn)\n",
    "\n",
    "        self.bars_encoder = nn.Linear(self.n_bars * self.d, self.d)\n",
    "    \n",
    "    def forward(self, graph):\n",
    "        \n",
    "        c_tensor = graph.c_tensor\n",
    "\n",
    "        # Discard SOS token\n",
    "        c_tensor = c_tensor[:, 1:, :]\n",
    "\n",
    "        # Get drums and non drums tensors\n",
    "        drums = c_tensor[graph.is_drum]\n",
    "        non_drums = c_tensor[torch.logical_not(graph.is_drum)]\n",
    "\n",
    "        # Compute drums embeddings\n",
    "        sz = drums.size()\n",
    "        drums_pitch = self.drums_pitch_emb(\n",
    "            drums[..., :N_PITCH_TOKENS])\n",
    "        drums_pitch = self.bn_drums(drums_pitch.view(-1, self.d//2))\n",
    "        drums_pitch = drums_pitch.view(sz[0], sz[1], self.d//2)\n",
    "        drums_dur = self.dur_emb(drums[..., N_PITCH_TOKENS:])\n",
    "        drums_dur = self.bn_dur(drums_dur.view(-1, self.d//2))\n",
    "        drums_dur = drums_dur.view(sz[0], sz[1], self.d//2)\n",
    "        drums = torch.cat((drums_pitch, drums_dur), dim=-1)\n",
    "        # n_nodes x MAX_SIMU_TOKENS x d\n",
    "\n",
    "        # Compute non drums embeddings\n",
    "        sz = non_drums.size()\n",
    "        non_drums_pitch = self.non_drums_pitch_emb(\n",
    "            non_drums[..., :N_PITCH_TOKENS]\n",
    "        )\n",
    "        non_drums_pitch = self.bn_non_drums(non_drums_pitch.view(-1, self.d//2))\n",
    "        non_drums_pitch = non_drums_pitch.view(sz[0], sz[1], self.d//2)\n",
    "        non_drums_dur = self.dur_emb(non_drums[..., N_PITCH_TOKENS:])\n",
    "        non_drums_dur = self.bn_dur(non_drums_dur.view(-1, self.d//2))\n",
    "        non_drums_dur = non_drums_dur.view(sz[0], sz[1], self.d//2)\n",
    "        non_drums = torch.cat((non_drums_pitch, non_drums_dur), dim=-1)\n",
    "        # n_nodes x MAX_SIMU_TOKENS x d\n",
    "\n",
    "        # Compute chord embeddings (drums and non drums)\n",
    "        drums = self.chord_encoder(\n",
    "            drums.view(-1, self.d * (MAX_SIMU_TOKENS-1))\n",
    "        )\n",
    "        non_drums = self.chord_encoder(\n",
    "            non_drums.view(-1, self.d * (MAX_SIMU_TOKENS-1))\n",
    "        )\n",
    "        drums = F.relu(drums)\n",
    "        non_drums = F.relu(non_drums)\n",
    "        drums = self.dropout_layer(drums)\n",
    "        non_drums = self.dropout_layer(non_drums)\n",
    "        # n_nodes x d\n",
    "\n",
    "        # Merge drums and non drums\n",
    "        out = torch.zeros((c_tensor.size(0), self.d), device=self.device,\n",
    "                          dtype=drums.dtype)\n",
    "        out[graph.is_drum] = drums\n",
    "        out[torch.logical_not(graph.is_drum)] = non_drums\n",
    "        # n_nodes x d\n",
    "\n",
    "        # Set initial graph node states to intermediate chord representations \n",
    "        # and pass through GCN\n",
    "        graph.x = out\n",
    "        graph.distinct_bars = graph.bars + self.n_bars*graph.batch\n",
    "        out = self.graph_encoder(graph)\n",
    "        # n_nodes x d\n",
    "\n",
    "        # Aggregate final node states into bar encodings with soft attention\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            out = self.graph_attention(out, batch=graph.distinct_bars)\n",
    "        # bs x n_bars x d\n",
    "\n",
    "        out = out.view(-1, self.n_bars * self.d)\n",
    "        # bs x (n_bars*d)\n",
    "        z_c = self.bars_encoder(out)\n",
    "        # bs x d\n",
    "        \n",
    "        return z_c\n",
    "\n",
    "\n",
    "class StructureEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.cnn_encoder = CNNEncoder(\n",
    "            dense_dim=self.d,\n",
    "            output_dim=self.d,\n",
    "            dropout=self.dropout,\n",
    "            batch_norm=self.batch_norm\n",
    "        )\n",
    "        self.bars_encoder = nn.Linear(self.n_bars * self.d, self.d)\n",
    "    \n",
    "    def forward(self, graph):\n",
    "        \n",
    "        s_tensor = graph.s_tensor\n",
    "        # hard code to 8 right now but was self.resolution \n",
    "        out = self.cnn_encoder(s_tensor.view(-1, N_TRACKS,\n",
    "                                             8 * 4))\n",
    "        # (bs*n_bars) x d\n",
    "        out = out.view(-1, self.n_bars * self.d)\n",
    "        # bs x (n_bars*d)\n",
    "        z_s = self.bars_encoder(out)\n",
    "        # bs x d\n",
    "\n",
    "        return z_s\n",
    "    \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.s_encoder = StructureEncoder(**kwargs)\n",
    "        self.c_encoder = ContentEncoder(**kwargs)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        # Linear layer that merges content and structure representations\n",
    "        self.linear_merge = nn.Linear(2*self.d, self.d)\n",
    "        self.bn_linear_merge = nn.BatchNorm1d(num_features=self.d)\n",
    "\n",
    "        self.linear_mu = nn.Linear(self.d, self.d)\n",
    "        self.linear_log_var = nn.Linear(self.d, self.d)\n",
    "\n",
    "    def forward(self, graph):\n",
    "        \n",
    "        z_s = self.s_encoder(graph)\n",
    "        z_c = self.c_encoder(graph)\n",
    "        \n",
    "        # Merge content and structure representations\n",
    "        z_g = torch.cat((z_c, z_s), dim=1)\n",
    "        z_g = self.dropout_layer(z_g)\n",
    "        z_g = self.linear_merge(z_g)\n",
    "        z_g = self.bn_linear_merge(z_g)\n",
    "        z_g = F.relu(z_g)\n",
    "\n",
    "        # Compute mu and log(std^2)\n",
    "        z_g = self.dropout_layer(z_g)\n",
    "        mu = self.linear_mu(z_g)\n",
    "        log_var = self.linear_log_var(z_g)\n",
    "\n",
    "        return mu, log_var\n",
    "\n",
    "\n",
    "class StructureDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.bars_decoder = nn.Linear(self.d, self.d * self.n_bars)\n",
    "        self.cnn_decoder = CNNDecoder(\n",
    "            input_dim=self.d,\n",
    "            dense_dim=self.d,\n",
    "            dropout=self.dropout,\n",
    "            batch_norm=self.batch_norm\n",
    "        )\n",
    "\n",
    "    def forward(self, z_s):\n",
    "        # z_s: bs x d\n",
    "        out = self.bars_decoder(z_s)  # bs x (n_bars*d)\n",
    "        out = self.cnn_decoder(out.reshape(-1, self.d))\n",
    "        out = out.view(z_s.size(0), self.n_bars, N_TRACKS, -1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ContentDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.bars_decoder = nn.Linear(self.d, self.d * self.n_bars)\n",
    "\n",
    "        self.graph_decoder = GCN(\n",
    "            dropout=self.dropout,\n",
    "            input_dim=self.d,\n",
    "            hidden_dim=self.d,\n",
    "            n_layers=self.gnn_n_layers,\n",
    "            num_relations=N_EDGE_TYPES,\n",
    "            batch_norm=self.batch_norm\n",
    "        )\n",
    "\n",
    "        self.chord_decoder = nn.Linear(\n",
    "            self.d, self.d*(MAX_SIMU_TOKENS-1))\n",
    "\n",
    "        # Pitch and duration (un)embedding linear layers\n",
    "        self.drums_pitch_emb = nn.Linear(self.d//2, N_PITCH_TOKENS)\n",
    "        self.non_drums_pitch_emb = nn.Linear(\n",
    "            self.d//2, N_PITCH_TOKENS)\n",
    "        self.dur_emb = nn.Linear(self.d//2, N_DUR_TOKENS)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "    def forward(self, z_c, s):\n",
    "\n",
    "        out = self.bars_decoder(z_c)  # bs x (n_bars*d)\n",
    "\n",
    "        # Initialize node features with corresponding z_bar\n",
    "        # and propagate with GNN\n",
    "        s.distinct_bars = s.bars + self.n_bars*s.batch\n",
    "        _, counts = torch.unique(s.distinct_bars, return_counts=True)\n",
    "        out = out.view(-1, self.d)\n",
    "        out = torch.repeat_interleave(out, counts, axis=0)  # n_nodes x d\n",
    "        s.x = out\n",
    "        out = self.graph_decoder(s)  # n_nodes x d\n",
    "\n",
    "        out = self.chord_decoder(out)  # n_nodes x (MAX_SIMU_TOKENS*d)\n",
    "        out = out.view(-1, MAX_SIMU_TOKENS-1, self.d)\n",
    "\n",
    "        drums = out[s.is_drum]  # n_nodes_drums x MAX_SIMU_TOKENS x d\n",
    "        non_drums = out[torch.logical_not(s.is_drum)]\n",
    "        # n_nodes_non_drums x MAX_SIMU_TOKENS x d\n",
    "\n",
    "        # Obtain final pitch and dur logits (softmax will be applied\n",
    "        # outside forward)\n",
    "        non_drums = self.dropout_layer(non_drums)\n",
    "        drums = self.dropout_layer(drums)\n",
    "\n",
    "        drums_pitch = self.drums_pitch_emb(drums[..., :self.d//2])\n",
    "        drums_dur = self.dur_emb(drums[..., self.d//2:])\n",
    "        drums = torch.cat((drums_pitch, drums_dur), dim=-1)\n",
    "        # n_nodes_drums x MAX_SIMU_TOKENS x d_token\n",
    "\n",
    "        non_drums_pitch = self.non_drums_pitch_emb(non_drums[..., :self.d//2])\n",
    "        non_drums_dur = self.dur_emb(non_drums[..., self.d//2:])\n",
    "        non_drums = torch.cat((non_drums_pitch, non_drums_dur), dim=-1)\n",
    "        # n_nodes_non_drums x MAX_SIMU_TOKENS x d_token\n",
    "\n",
    "        # Merge drums and non-drums in the final output tensor\n",
    "        d_token = D_TOKEN_PAIR\n",
    "        out = torch.zeros((s.num_nodes, MAX_SIMU_TOKENS-1, d_token),\n",
    "                          device=self.device, dtype=drums.dtype)\n",
    "        out[s.is_drum] = drums\n",
    "        out[torch.logical_not(s.is_drum)] = non_drums\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.lin_decoder = nn.Linear(self.d, 2 * self.d)\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features=2*self.d)\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        self.s_decoder = StructureDecoder(**kwargs)\n",
    "        self.c_decoder = ContentDecoder(**kwargs)\n",
    "\n",
    "        self.sigmoid_thresh = 0.5\n",
    "\n",
    "    def _structure_from_binary(self, s_tensor):\n",
    "\n",
    "        # Create graph structures for each batch\n",
    "        s = []\n",
    "        for i in range(s_tensor.size(0)):\n",
    "            s.append(graph_from_tensor(s_tensor[i]))\n",
    "\n",
    "        # Create batch of graphs from single graphs\n",
    "        s = Batch.from_data_list(s, exclude_keys=['batch'])\n",
    "        s = s.to(next(self.parameters()).device)\n",
    "\n",
    "        return s\n",
    "\n",
    "    def _binary_from_logits(self, s_logits):\n",
    "\n",
    "        # Hard threshold instead of sampling gives more pleasant results\n",
    "        s_tensor = torch.sigmoid(s_logits)\n",
    "        s_tensor[s_tensor >= self.sigmoid_thresh] = 1\n",
    "        s_tensor[s_tensor < self.sigmoid_thresh] = 0\n",
    "        s_tensor = s_tensor.bool()\n",
    "        \n",
    "        # Avoid empty bars by creating a fake activation for each empty\n",
    "        # (n_tracks x n_timesteps) bar matrix in position [0, 0]\n",
    "        empty_mask = ~s_tensor.any(dim=-1).any(dim=-1)\n",
    "        idxs = torch.nonzero(empty_mask, as_tuple=True)\n",
    "        s_tensor[idxs + (0, 0)] = True\n",
    "\n",
    "        return s_tensor\n",
    "\n",
    "    def _structure_from_logits(self, s_logits):\n",
    "\n",
    "        # Compute binary structure tensor from logits and build torch geometric\n",
    "        # structure from binary tensor\n",
    "        s_tensor = self._binary_from_logits(s_logits)\n",
    "        s = self._structure_from_binary(s_tensor)\n",
    "\n",
    "        return s\n",
    "\n",
    "    def forward(self, z, s=None):\n",
    "\n",
    "        # Obtain z_s and z_c from z\n",
    "        z = self.lin_decoder(z)\n",
    "        z = self.batch_norm(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.dropout(z)  # bs x (2*d)\n",
    "        z_s, z_c = z[:, :self.d], z[:, self.d:]\n",
    "\n",
    "        # Obtain the tensor containing structure logits\n",
    "        s_logits = self.s_decoder(z_s)\n",
    "\n",
    "        if s is None:\n",
    "            # Build torch geometric graph structure from structure logits.\n",
    "            # This step involves non differentiable operations.\n",
    "            # No gradients pass through here.\n",
    "            s = self._structure_from_logits(s_logits.detach())\n",
    "\n",
    "        # Obtain the tensor containing content logits\n",
    "        c_logits = self.c_decoder(z_c, s)\n",
    "\n",
    "        return s_logits, c_logits\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(**kwargs)\n",
    "        self.decoder = Decoder(**kwargs)\n",
    "\n",
    "    def forward(self, graph):\n",
    "\n",
    "        # Encoder pass\n",
    "        mu, log_var = self.encoder(graph)\n",
    "\n",
    "        # Reparameterization trick\n",
    "        z = torch.exp(0.5 * log_var)\n",
    "        z = z * torch.randn_like(z)\n",
    "        z = z + mu\n",
    "\n",
    "        # Decoder pass\n",
    "        out = self.decoder(z, graph)\n",
    "\n",
    "        return out, mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PitchToken(Enum):\n",
    "    SOS = 128\n",
    "    EOS = 129\n",
    "    PAD = 130\n",
    "\n",
    "\n",
    "N_PITCH_TOKENS = 131\n",
    "MAX_PITCH_TOKEN = 127\n",
    "\n",
    "\n",
    "# Duration tokens have values in the range [0, 98]. Tokens from 0 to 95 have to\n",
    "# be interpreted as durations from 1 to 96 timesteps.\n",
    "class DurationToken(Enum):\n",
    "    SOS = 96\n",
    "    EOS = 97\n",
    "    PAD = 98\n",
    "    \n",
    "def append_dict(dest_d, source_d):\n",
    "\n",
    "    for k, v in source_d.items():\n",
    "        dest_d[k].append(v)\n",
    "\n",
    "def print_divider():\n",
    "    print('—' * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s222445\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from statistics import mean\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "import pprint\n",
    "import math\n",
    "\n",
    "#import constants\n",
    "#from constants import PitchToken, DurationToken\n",
    "#from utils import append_dict, print_divider\n",
    "\n",
    "\n",
    "class StepBetaScheduler():\n",
    "    def __init__(self, anneal_start, beta_max, step_size, anneal_end):\n",
    "        self.anneal_start = anneal_start\n",
    "        self.beta_max = beta_max\n",
    "        self.step_size = step_size\n",
    "        self.anneal_end = anneal_end\n",
    "\n",
    "        self.update_steps = 0\n",
    "        self.beta = 0\n",
    "        n_steps = self.beta_max // self.step_size\n",
    "        self.inc_every = (self.anneal_end-self.anneal_start) // n_steps\n",
    "\n",
    "    def step(self):\n",
    "        self.update_steps += 1\n",
    "\n",
    "        if (self.update_steps >= self.anneal_start or\n",
    "                self.update_steps < self.anneal_end):\n",
    "            # If we are annealing, update beta according to current step\n",
    "            curr_step = (self.update_steps-self.anneal_start) // self.inc_every\n",
    "            self.beta = self.step_size * (curr_step+1)\n",
    "            \n",
    "        return self.beta\n",
    "\n",
    "\n",
    "class ExpDecayLRScheduler():\n",
    "    def __init__(self, optimizer, peak_lr, warmup_steps, final_lr_scale,\n",
    "                 decay_steps):\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.peak_lr = peak_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.decay_steps = decay_steps\n",
    "\n",
    "        # Find the decay factor needed to reach the specified\n",
    "        # learning rate scale after decay_steps steps\n",
    "        self.decay_factor = -math.log(final_lr_scale) / self.decay_steps\n",
    "\n",
    "        self.update_steps = 0\n",
    "\n",
    "    def set_lr(self, optimizer, lr):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def step(self):\n",
    "        self.update_steps += 1\n",
    "\n",
    "        if self.update_steps <= self.warmup_steps:\n",
    "            self.lr = self.peak_lr\n",
    "        else:\n",
    "            # Decay lr exponentially\n",
    "            steps_after_warmup = self.update_steps - self. warmup_steps\n",
    "            self.lr = \\\n",
    "                self.peak_lr * math.exp(-self.decay_factor*steps_after_warmup)\n",
    "\n",
    "        self.set_lr(self.optimizer, self.lr)\n",
    "\n",
    "        return self.lr\n",
    "\n",
    "\n",
    "class PolyphemusTrainer():\n",
    "\n",
    "    def __init__(self, model_dir, model, optimizer, init_lr=1e-4,\n",
    "                 lr_scheduler=None, beta_scheduler=None, device=None, \n",
    "                 print_every=1, save_every=1, eval_every=100, \n",
    "                 iters_to_accumulate=1, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.model_dir = model_dir\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.init_lr = init_lr\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.beta_scheduler = beta_scheduler\n",
    "        self.device = device if device is not None else torch.device(\"cpu\")\n",
    "        self.cuda = True if self.device.type == 'cuda' else False\n",
    "        self.print_every = print_every\n",
    "        self.save_every = save_every\n",
    "        self.eval_every = eval_every\n",
    "        self.iters_to_accumulate = iters_to_accumulate\n",
    "\n",
    "        # Losses (ignoring PAD tokens)\n",
    "        self.bce_unreduced = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        self.ce_p = nn.CrossEntropyLoss(ignore_index=PitchToken.PAD.value)\n",
    "        self.ce_d = nn.CrossEntropyLoss(ignore_index=DurationToken.PAD.value)\n",
    "\n",
    "        # Training stats\n",
    "        self.tr_losses = defaultdict(list)\n",
    "        self.tr_accuracies = defaultdict(list)\n",
    "        self.val_losses = defaultdict(list)\n",
    "        self.val_accuracies = defaultdict(list)\n",
    "        self.lrs = []\n",
    "        self.betas = []\n",
    "        self.times = []\n",
    "\n",
    "    def train(self, trainloader, validloader=None, epochs=100, early_exit=None):\n",
    "\n",
    "        self.tot_batches = 0\n",
    "        self.beta = 0\n",
    "        self.min_val_loss = np.inf\n",
    "\n",
    "        start = time.time()\n",
    "        self.times.append(start)\n",
    "\n",
    "        self.model.train()\n",
    "        scaler = torch.cuda.amp.GradScaler() if self.cuda else None\n",
    "        self.optimizer.zero_grad()\n",
    "        progress_bar = tqdm(range(len(trainloader)))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.cur_epoch = epoch\n",
    "            for batch_idx, graph in enumerate(trainloader):\n",
    "                self.cur_batch_idx = batch_idx\n",
    "\n",
    "                # Move batch of graphs to device. Note: a single graph here\n",
    "                # represents a bar in the original sequence.\n",
    "                graph = graph.to(self.device)\n",
    "                s_tensor, c_tensor = graph.s_tensor, graph.c_tensor\n",
    "\n",
    "                with torch.cuda.amp.autocast(enabled=self.cuda):\n",
    "                    # Forward pass to obtain mu, log(sigma^2), computed by the\n",
    "                    # encoder, and structure and content logits, computed by the\n",
    "                    # decoder\n",
    "                    (s_logits, c_logits), mu, log_var = self.model(graph)\n",
    "\n",
    "                    # Compute losses\n",
    "                    tot_loss, losses = self._losses(\n",
    "                        s_tensor, s_logits,\n",
    "                        c_tensor, c_logits,\n",
    "                        mu, log_var\n",
    "                    )\n",
    "                    tot_loss = tot_loss / self.iters_to_accumulate\n",
    "\n",
    "                # Backpropagation\n",
    "                if self.cuda:\n",
    "                    scaler.scale(tot_loss).backward()\n",
    "                else:\n",
    "                    tot_loss.backward()\n",
    "\n",
    "                # Update weights with accumulated gradients\n",
    "                if (self.tot_batches + 1) % self.iters_to_accumulate == 0:\n",
    "\n",
    "                    if self.cuda:\n",
    "                        scaler.step(self.optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        self.optimizer.step()\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                    # Update lr and beta\n",
    "                    if self.lr_scheduler is not None:\n",
    "                        self.lr_scheduler.step()\n",
    "                    if self.beta_scheduler is not None:\n",
    "                        self.beta_scheduler.step()\n",
    "\n",
    "                # Compute accuracies\n",
    "                accs = self._accuracies(\n",
    "                    s_tensor, s_logits,\n",
    "                    c_tensor, c_logits,\n",
    "                    graph.is_drum\n",
    "                )\n",
    "\n",
    "                # Update the stats\n",
    "                append_dict(self.tr_losses, losses)\n",
    "                append_dict(self.tr_accuracies, accs)\n",
    "                last_lr = (self.lr_scheduler.lr\n",
    "                           if self.lr_scheduler is not None else self.init_lr)\n",
    "                self.lrs.append(last_lr)\n",
    "                self.betas.append(self.beta)\n",
    "                now = time.time()\n",
    "                self.times.append(now)\n",
    "\n",
    "                # Print stats\n",
    "                if (self.tot_batches + 1) % self.print_every == 0:\n",
    "                    print(\"Training on batch {}/{} of epoch {}/{} complete.\"\n",
    "                          .format(batch_idx+1,\n",
    "                                  len(trainloader),\n",
    "                                  epoch+1,\n",
    "                                  epochs))\n",
    "                    self._print_stats()\n",
    "                    print_divider()\n",
    "\n",
    "                # Eval on VL every `eval_every` gradient updates\n",
    "                if (validloader is not None and\n",
    "                        (self.tot_batches + 1) % self.eval_every == 0):\n",
    "\n",
    "                    # Evaluate on VL\n",
    "                    print(\"\\nEvaluating on validation set...\\n\")\n",
    "                    val_losses, val_accuracies = self.evaluate(validloader)\n",
    "\n",
    "                    # Update stats\n",
    "                    append_dict(self.val_losses, val_losses)\n",
    "                    append_dict(self.val_accuracies, val_accuracies)\n",
    "\n",
    "                    print(\"Val losses:\")\n",
    "                    print(val_losses)\n",
    "                    print(\"Val accuracies:\")\n",
    "                    print(val_accuracies)\n",
    "\n",
    "                    # Save model if VL loss (tot) reached a new minimum\n",
    "                    tot_loss = val_losses['tot']\n",
    "                    if tot_loss < self.min_val_loss:\n",
    "                        print(\"\\nValidation loss improved.\")\n",
    "                        print(\"Saving new best model to disk...\\n\")\n",
    "                        self._save_model('best_model')\n",
    "                        self.min_val_loss = tot_loss\n",
    "\n",
    "                    self.model.train()\n",
    "\n",
    "                progress_bar.update(1)\n",
    "\n",
    "                # Save model and stats on disk\n",
    "                if (self.save_every > 0 and\n",
    "                        (self.tot_batches + 1) % self.save_every == 0):\n",
    "                    self._save_model('checkpoint')\n",
    "\n",
    "                # Stop prematurely if early_exit is set and reached\n",
    "                if (early_exit is not None and\n",
    "                        (self.tot_batches + 1) > early_exit):\n",
    "                    break\n",
    "\n",
    "                self.tot_batches += 1\n",
    "\n",
    "        end = time.time()\n",
    "        hours, rem = divmod(end-start, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        print(\"Training completed in (h:m:s): {:0>2}:{:0>2}:{:05.2f}\"\n",
    "              .format(int(hours), int(minutes), seconds))\n",
    "\n",
    "        self._save_model('checkpoint')\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "\n",
    "        losses = defaultdict(list)\n",
    "        accs = defaultdict(list)\n",
    "\n",
    "        self.model.eval()\n",
    "        progress_bar = tqdm(range(len(loader)))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _, graph in enumerate(loader):\n",
    "\n",
    "                # Get the inputs and move them to device\n",
    "                graph = graph.to(self.device)\n",
    "                s_tensor, c_tensor = graph.s_tensor, graph.c_tensor\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    # Forward pass, get the reconstructions\n",
    "                    (s_logits, c_logits), mu, log_var = self.model(graph)\n",
    "\n",
    "                    _, losses_b = self._losses(\n",
    "                        s_tensor, s_logits,\n",
    "                        c_tensor, c_logits,\n",
    "                        mu, log_var\n",
    "                    )\n",
    "\n",
    "                accs_b = self._accuracies(\n",
    "                    s_tensor, s_logits,\n",
    "                    c_tensor, c_logits,\n",
    "                    graph.is_drum\n",
    "                )\n",
    "\n",
    "                # Save losses and accuracies\n",
    "                append_dict(losses, losses_b)\n",
    "                append_dict(accs, accs_b)\n",
    "\n",
    "                progress_bar.update(1)\n",
    "\n",
    "        # Compute avg losses and accuracies\n",
    "        avg_losses = {}\n",
    "        for k, l in losses.items():\n",
    "            avg_losses[k] = mean(l)\n",
    "\n",
    "        avg_accs = {}\n",
    "        for k, l in accs.items():\n",
    "            avg_accs[k] = mean(l)\n",
    "\n",
    "        return avg_losses, avg_accs\n",
    "\n",
    "    def _losses(self, s_tensor, s_logits, c_tensor, c_logits, mu, log_var):\n",
    "\n",
    "        # Do not consider SOS token\n",
    "        c_tensor = c_tensor[..., 1:, :]\n",
    "        c_logits = c_logits.reshape(-1, c_logits.size(-1))\n",
    "        c_tensor = c_tensor.reshape(-1, c_tensor.size(-1))\n",
    "\n",
    "        # Reshape logits to match s_tensor dimensions:\n",
    "        # n_graphs (in batch) x n_tracks x n_timesteps\n",
    "        s_logits = s_tensor.reshape(-1, *s_logits.shape[2:])\n",
    "\n",
    "        # Binary structure tensor loss (binary cross entropy)\n",
    "        s_loss = self.bce_unreduced(\n",
    "            s_logits.view(-1), s_tensor.view(-1).float())\n",
    "        s_loss = torch.mean(s_loss)\n",
    "\n",
    "        # Content tensor loss (pitches)\n",
    "        # argmax is used to obtain token ids from onehot rep\n",
    "        pitch_logits = c_logits[:, :N_PITCH_TOKENS]\n",
    "        pitch_true = c_tensor[:, :N_PITCH_TOKENS].argmax(dim=1)\n",
    "        pitch_loss = self.ce_p(pitch_logits, pitch_true)\n",
    "\n",
    "        # Content tensor loss (durations)\n",
    "        dur_logits = c_logits[:, N_PITCH_TOKENS:]\n",
    "        dur_true = c_tensor[:, N_PITCH_TOKENS:].argmax(dim=1)\n",
    "        dur_loss = self.ce_d(dur_logits, dur_true)\n",
    "\n",
    "        # Kullback-Leibler divergence loss\n",
    "        # Derivation in Kingma, Diederik P., and Max Welling. \"Auto-encoding\n",
    "        # variational bayes.\" (2013), Appendix B.\n",
    "        # (https://arxiv.org/pdf/1312.6114.pdf)\n",
    "        kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(),\n",
    "                                    dim=1)\n",
    "        kld_loss = torch.mean(kld_loss)\n",
    "\n",
    "        # Reconstruction loss and total loss\n",
    "        rec_loss = pitch_loss + dur_loss + s_loss\n",
    "        tot_loss = rec_loss + self.beta*kld_loss\n",
    "\n",
    "        losses = {\n",
    "            'tot': tot_loss.item(),\n",
    "            'pitch': pitch_loss.item(),\n",
    "            'dur': dur_loss.item(),\n",
    "            'structure': s_loss.item(),\n",
    "            'reconstruction': rec_loss.item(),\n",
    "            'kld': kld_loss.item(),\n",
    "            'beta*kld': self.beta*kld_loss.item()\n",
    "        }\n",
    "\n",
    "        return tot_loss, losses\n",
    "\n",
    "    def _accuracies(self, s_tensor, s_logits, c_tensor, c_logits, is_drum):\n",
    "\n",
    "        # Do not consider SOS token\n",
    "        c_tensor = c_tensor[..., 1:, :]\n",
    "\n",
    "        # Reshape logits to match s_tensor dimensions:\n",
    "        # n_graphs (in batch) x n_tracks x n_timesteps\n",
    "        s_logits = s_tensor.reshape(-1, *s_logits.shape[2:])\n",
    "\n",
    "        # Note accuracy considers both pitches and durations\n",
    "        note_acc = self._note_accuracy(c_logits, c_tensor)\n",
    "\n",
    "        pitch_acc = self._pitch_accuracy(c_logits, c_tensor)\n",
    "\n",
    "        # Compute pitch accuracies for drums and non drums separately\n",
    "        pitch_acc_drums = self._pitch_accuracy(\n",
    "            c_logits, c_tensor, drums=True, is_drum=is_drum\n",
    "        )\n",
    "        pitch_acc_non_drums = self._pitch_accuracy(\n",
    "            c_logits, c_tensor, drums=False, is_drum=is_drum\n",
    "        )\n",
    "\n",
    "        dur_acc = self._duration_accuracy(c_logits, c_tensor)\n",
    "\n",
    "        s_acc = self._structure_accuracy(s_logits, s_tensor)\n",
    "        s_precision = self._structure_precision(s_logits, s_tensor)\n",
    "        s_recall = self._structure_recall(s_logits, s_tensor)\n",
    "        s_f1 = (2*s_recall*s_precision / (s_recall+s_precision))\n",
    "\n",
    "        accs = {\n",
    "            'note': note_acc.item(),\n",
    "            'pitch': pitch_acc.item(),\n",
    "            'pitch_drums': pitch_acc_drums.item(),\n",
    "            'pitch_non_drums': pitch_acc_non_drums.item(),\n",
    "            'dur': dur_acc.item(),\n",
    "            's_acc': s_acc.item(),\n",
    "            's_precision': s_precision.item(),\n",
    "            's_recall': s_recall.item(),\n",
    "            's_f1': s_f1.item()\n",
    "        }\n",
    "\n",
    "        return accs\n",
    "\n",
    "    def _pitch_accuracy(self, c_logits, c_tensor, drums=None, is_drum=None):\n",
    "\n",
    "        # When drums is None, just compute the global pitch accuracy without\n",
    "        # distinguishing between drum and non drum pitches\n",
    "        if drums is not None:\n",
    "            if drums:\n",
    "                c_logits = c_logits[is_drum]\n",
    "                c_tensor = c_tensor[is_drum]\n",
    "            else:\n",
    "                c_logits = c_logits[torch.logical_not(is_drum)]\n",
    "                c_tensor = c_tensor[torch.logical_not(is_drum)]\n",
    "\n",
    "        # Apply softmax to obtain pitch reconstructions\n",
    "        pitch_rec = c_logits[..., :N_PITCH_TOKENS]\n",
    "        pitch_rec = F.softmax(pitch_rec, dim=-1)\n",
    "        pitch_rec = torch.argmax(pitch_rec, dim=-1)\n",
    "\n",
    "        pitch_true = c_tensor[..., :N_PITCH_TOKENS]\n",
    "        pitch_true = torch.argmax(pitch_true, dim=-1)\n",
    "\n",
    "        # Do not consider PAD tokens when computing accuracies\n",
    "        not_pad = (pitch_true != PitchToken.PAD.value)\n",
    "\n",
    "        correct = (pitch_rec == pitch_true)\n",
    "        correct = torch.logical_and(correct, not_pad)\n",
    "\n",
    "        return torch.sum(correct) / torch.sum(not_pad)\n",
    "\n",
    "    def _duration_accuracy(self, c_logits, c_tensor):\n",
    "\n",
    "        # Apply softmax to obtain reconstructed durations\n",
    "        dur_rec = c_logits[..., N_PITCH_TOKENS:]\n",
    "        dur_rec = F.softmax(dur_rec, dim=-1)\n",
    "        dur_rec = torch.argmax(dur_rec, dim=-1)\n",
    "\n",
    "        dur_true = c_tensor[..., N_PITCH_TOKENS:]\n",
    "        dur_true = torch.argmax(dur_true, dim=-1)\n",
    "\n",
    "        # Do not consider PAD tokens when computing accuracies\n",
    "        not_pad = (dur_true != DurationToken.PAD.value)\n",
    "\n",
    "        correct = (dur_rec == dur_true)\n",
    "        correct = torch.logical_and(correct, not_pad)\n",
    "\n",
    "        return torch.sum(correct) / torch.sum(not_pad)\n",
    "\n",
    "    def _note_accuracy(self, c_logits, c_tensor):\n",
    "\n",
    "        # Apply softmax to obtain pitch reconstructions\n",
    "        pitch_rec = c_logits[..., :N_PITCH_TOKENS]\n",
    "        pitch_rec = F.softmax(pitch_rec, dim=-1)\n",
    "        pitch_rec = torch.argmax(pitch_rec, dim=-1)\n",
    "\n",
    "        pitch_true = c_tensor[..., :N_PITCH_TOKENS]\n",
    "        pitch_true = torch.argmax(pitch_true, dim=-1)\n",
    "\n",
    "        not_pad_p = (pitch_true != PitchToken.PAD.value)\n",
    "\n",
    "        correct_p = (pitch_rec == pitch_true)\n",
    "        correct_p = torch.logical_and(correct_p, not_pad_p)\n",
    "\n",
    "        dur_rec = c_logits[..., N_PITCH_TOKENS:]\n",
    "        dur_rec = F.softmax(dur_rec, dim=-1)\n",
    "        dur_rec = torch.argmax(dur_rec, dim=-1)\n",
    "\n",
    "        dur_true = c_tensor[..., N_PITCH_TOKENS:]\n",
    "        dur_true = torch.argmax(dur_true, dim=-1)\n",
    "\n",
    "        not_pad_d = (dur_true != DurationToken.PAD.value)\n",
    "\n",
    "        correct_d = (dur_rec == dur_true)\n",
    "        correct_d = torch.logical_and(correct_d, not_pad_d)\n",
    "\n",
    "        note_accuracy = torch.sum(\n",
    "            torch.logical_and(correct_p, correct_d)) / torch.sum(not_pad_p)\n",
    "\n",
    "        return note_accuracy\n",
    "\n",
    "    def _structure_accuracy(self, s_logits, s_tensor):\n",
    "\n",
    "        s_logits = torch.sigmoid(s_logits)\n",
    "        s_logits[s_logits < 0.5] = 0\n",
    "        s_logits[s_logits >= 0.5] = 1\n",
    "\n",
    "        return torch.sum(s_logits == s_tensor) / s_tensor.numel()\n",
    "\n",
    "    def _structure_precision(self, s_logits, s_tensor):\n",
    "\n",
    "        s_logits = torch.sigmoid(s_logits)\n",
    "        s_logits[s_logits < 0.5] = 0\n",
    "        s_logits[s_logits >= 0.5] = 1\n",
    "\n",
    "        tp = torch.sum(s_tensor[s_logits == 1])\n",
    "\n",
    "        return tp / torch.sum(s_logits)\n",
    "\n",
    "    def _structure_recall(self, s_logits, s_tensor):\n",
    "\n",
    "        s_logits = torch.sigmoid(s_logits)\n",
    "        s_logits[s_logits < 0.5] = 0\n",
    "        s_logits[s_logits >= 0.5] = 1\n",
    "\n",
    "        tp = torch.sum(s_tensor[s_logits == 1])\n",
    "\n",
    "        return tp / torch.sum(s_tensor)\n",
    "\n",
    "    def _save_model(self, filename):\n",
    "\n",
    "        path = os.path.join(self.model_dir, filename)\n",
    "        print(\"Saving model to disk...\")\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': self.cur_epoch,\n",
    "            'batch': self.cur_batch_idx,\n",
    "            'tot_batches': self.tot_batches,\n",
    "            'betas': self.betas,\n",
    "            'min_val_loss': self.min_val_loss,\n",
    "            'print_every': self.print_every,\n",
    "            'save_every': self.save_every,\n",
    "            'eval_every': self.eval_every,\n",
    "            'lrs': self.lrs,\n",
    "            'tr_losses': self.tr_losses,\n",
    "            'tr_accuracies': self.tr_accuracies,\n",
    "            'val_losses': self.val_losses,\n",
    "            'val_accuracies': self.val_accuracies,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict()\n",
    "        }, path)\n",
    "\n",
    "        print(\"The model has been successfully saved.\")\n",
    "\n",
    "    def _print_stats(self):\n",
    "\n",
    "        hours, rem = divmod(self.times[-1]-self.times[0], 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        print(\"Elapsed time from start (h:m:s): {:0>2}:{:0>2}:{:05.2f}\"\n",
    "              .format(int(hours), int(minutes), seconds))\n",
    "\n",
    "        # Take mean of the last non-printed batches for each loss and accuracy\n",
    "        avg_losses = {}\n",
    "        for k, l in self.tr_losses.items():\n",
    "            v = mean(l[-self.print_every:])\n",
    "            avg_losses[k] = round(v, 2)\n",
    "\n",
    "        avg_accs = {}\n",
    "        for k, l in self.tr_accuracies.items():\n",
    "            v = mean(l[-self.print_every:])\n",
    "            avg_accs[k] = round(v, 2)\n",
    "\n",
    "        print(\"Losses:\")\n",
    "        pprint.pprint(avg_losses, indent=2)\n",
    "\n",
    "        print(\"Accuracies:\")\n",
    "        pprint.pprint(avg_accs, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "import random\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "\n",
    "\n",
    "def print_params(model):\n",
    "\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "\n",
    "    for name, parameter in model.named_parameters():\n",
    "\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params += param\n",
    "\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Parameters: {total_params}\")\n",
    "\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the provided training configuration.\n",
      "Preparing datasets and dataloaders...\n",
      "Creating the model and moving it on cpu device...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s222445\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "C:\\Users\\s222445\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------+------------+\n",
      "|                           Modules                           | Parameters |\n",
      "+-------------------------------------------------------------+------------+\n",
      "|         encoder.s_encoder.cnn_encoder.conv.0.weight         |     72     |\n",
      "|          encoder.s_encoder.cnn_encoder.conv.0.bias          |     8      |\n",
      "|         encoder.s_encoder.cnn_encoder.conv.1.weight         |     8      |\n",
      "|          encoder.s_encoder.cnn_encoder.conv.1.bias          |     8      |\n",
      "|         encoder.s_encoder.cnn_encoder.conv.4.weight         |    1152    |\n",
      "|          encoder.s_encoder.cnn_encoder.conv.4.bias          |     16     |\n",
      "|         encoder.s_encoder.cnn_encoder.conv.5.weight         |     16     |\n",
      "|          encoder.s_encoder.cnn_encoder.conv.5.bias          |     16     |\n",
      "|          encoder.s_encoder.cnn_encoder.lin.1.weight         |   262144   |\n",
      "|           encoder.s_encoder.cnn_encoder.lin.1.bias          |    512     |\n",
      "|          encoder.s_encoder.cnn_encoder.lin.4.weight         |   262144   |\n",
      "|           encoder.s_encoder.cnn_encoder.lin.4.bias          |    512     |\n",
      "|            encoder.s_encoder.bars_encoder.weight            |   524288   |\n",
      "|             encoder.s_encoder.bars_encoder.bias             |    512     |\n",
      "|         encoder.c_encoder.non_drums_pitch_emb.weight        |   33536    |\n",
      "|          encoder.c_encoder.non_drums_pitch_emb.bias         |    256     |\n",
      "|           encoder.c_encoder.drums_pitch_emb.weight          |   33536    |\n",
      "|            encoder.c_encoder.drums_pitch_emb.bias           |    256     |\n",
      "|               encoder.c_encoder.dur_emb.weight              |   25344    |\n",
      "|                encoder.c_encoder.dur_emb.bias               |    256     |\n",
      "|            encoder.c_encoder.bn_non_drums.weight            |    256     |\n",
      "|             encoder.c_encoder.bn_non_drums.bias             |    256     |\n",
      "|              encoder.c_encoder.bn_drums.weight              |    256     |\n",
      "|               encoder.c_encoder.bn_drums.bias               |    256     |\n",
      "|               encoder.c_encoder.bn_dur.weight               |    256     |\n",
      "|                encoder.c_encoder.bn_dur.bias                |    256     |\n",
      "|            encoder.c_encoder.chord_encoder.weight           |  3932160   |\n",
      "|             encoder.c_encoder.chord_encoder.bias            |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.0.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.0.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.0.bias        |    512     |\n",
      "|      encoder.c_encoder.graph_encoder.layers.0.nn.weight     |   16384    |\n",
      "|       encoder.c_encoder.graph_encoder.layers.0.nn.bias      |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.1.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.1.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.1.bias        |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.2.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.2.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.2.bias        |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.3.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.3.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.3.bias        |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.4.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.4.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.4.bias        |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.5.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.5.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.5.bias        |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.6.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.6.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.6.bias        |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.7.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.7.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.7.bias        |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.0.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.0.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.1.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.1.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.2.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.2.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.3.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.3.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.4.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.4.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.5.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.5.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.6.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.6.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.7.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.7.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_attention.gate_nn.0.layers.0.weight |    512     |\n",
      "|  encoder.c_encoder.graph_attention.gate_nn.0.layers.0.bias  |     1      |\n",
      "|      encoder.c_encoder.graph_attention.gate_nn.1.weight     |     1      |\n",
      "|       encoder.c_encoder.graph_attention.gate_nn.1.bias      |     1      |\n",
      "|            encoder.c_encoder.bars_encoder.weight            |   524288   |\n",
      "|             encoder.c_encoder.bars_encoder.bias             |    512     |\n",
      "|                 encoder.linear_merge.weight                 |   524288   |\n",
      "|                  encoder.linear_merge.bias                  |    512     |\n",
      "|                encoder.bn_linear_merge.weight               |    512     |\n",
      "|                 encoder.bn_linear_merge.bias                |    512     |\n",
      "|                   encoder.linear_mu.weight                  |   262144   |\n",
      "|                    encoder.linear_mu.bias                   |    512     |\n",
      "|                encoder.linear_log_var.weight                |   262144   |\n",
      "|                 encoder.linear_log_var.bias                 |    512     |\n",
      "|                  decoder.lin_decoder.weight                 |   524288   |\n",
      "|                   decoder.lin_decoder.bias                  |    1024    |\n",
      "|                  decoder.batch_norm.weight                  |    1024    |\n",
      "|                   decoder.batch_norm.bias                   |    1024    |\n",
      "|            decoder.s_decoder.bars_decoder.weight            |   524288   |\n",
      "|             decoder.s_decoder.bars_decoder.bias             |    1024    |\n",
      "|          decoder.s_decoder.cnn_decoder.lin.1.weight         |   262144   |\n",
      "|           decoder.s_decoder.cnn_decoder.lin.1.bias          |    512     |\n",
      "|          decoder.s_decoder.cnn_decoder.lin.4.weight         |   262144   |\n",
      "|           decoder.s_decoder.cnn_decoder.lin.4.bias          |    512     |\n",
      "|         decoder.s_decoder.cnn_decoder.conv.1.weight         |    1152    |\n",
      "|          decoder.s_decoder.cnn_decoder.conv.1.bias          |     8      |\n",
      "|         decoder.s_decoder.cnn_decoder.conv.2.weight         |     8      |\n",
      "|          decoder.s_decoder.cnn_decoder.conv.2.bias          |     8      |\n",
      "|         decoder.s_decoder.cnn_decoder.conv.4.weight         |     72     |\n",
      "|          decoder.s_decoder.cnn_decoder.conv.4.bias          |     1      |\n",
      "|            decoder.c_decoder.bars_decoder.weight            |   524288   |\n",
      "|             decoder.c_decoder.bars_decoder.bias             |    1024    |\n",
      "|       decoder.c_decoder.graph_decoder.layers.0.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.0.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.0.bias        |    512     |\n",
      "|      decoder.c_decoder.graph_decoder.layers.0.nn.weight     |   16384    |\n",
      "|       decoder.c_decoder.graph_decoder.layers.0.nn.bias      |    512     |\n",
      "|       decoder.c_decoder.graph_decoder.layers.1.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.1.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.1.bias        |    512     |\n",
      "|       decoder.c_decoder.graph_decoder.layers.2.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.2.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.2.bias        |    512     |\n",
      "|       decoder.c_decoder.graph_decoder.layers.3.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.3.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.3.bias        |    512     |\n",
      "|       decoder.c_decoder.graph_decoder.layers.4.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.4.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.4.bias        |    512     |\n",
      "|       decoder.c_decoder.graph_decoder.layers.5.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.5.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.5.bias        |    512     |\n",
      "|       decoder.c_decoder.graph_decoder.layers.6.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.6.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.6.bias        |    512     |\n",
      "|       decoder.c_decoder.graph_decoder.layers.7.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.7.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.7.bias        |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.0.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.0.module.bias  |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.1.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.1.module.bias  |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.2.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.2.module.bias  |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.3.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.3.module.bias  |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.4.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.4.module.bias  |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.5.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.5.module.bias  |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.6.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.6.module.bias  |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.7.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.7.module.bias  |    512     |\n",
      "|            decoder.c_decoder.chord_decoder.weight           |  3932160   |\n",
      "|             decoder.c_decoder.chord_decoder.bias            |    7680    |\n",
      "|           decoder.c_decoder.drums_pitch_emb.weight          |   33536    |\n",
      "|            decoder.c_decoder.drums_pitch_emb.bias           |    131     |\n",
      "|         decoder.c_decoder.non_drums_pitch_emb.weight        |   33536    |\n",
      "|          decoder.c_decoder.non_drums_pitch_emb.bias         |    131     |\n",
      "|               decoder.c_decoder.dur_emb.weight              |   25344    |\n",
      "|                decoder.c_decoder.dur_emb.bias               |     99     |\n",
      "+-------------------------------------------------------------+------------+\n",
      "Total Trainable Parameters: 42210909\n",
      "\n",
      "Starting training...\n",
      "————————————————————————————————————————\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 116\u001b[0m\n\u001b[0;32m     87\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath/to/save/output\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     88\u001b[0m training_config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m256\u001b[39m,\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m     }\n\u001b[0;32m    115\u001b[0m }\n\u001b[1;32m--> 116\u001b[0m \u001b[43mtrain_polyphemus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 83\u001b[0m, in \u001b[0;36mtrain_polyphemus\u001b[1;34m(dataset_dir, output_dir, training_config, model_name, save_every, print_every, eval, eval_every, use_gpu, gpu_id, num_workers, tr_split, vl_split, max_epochs, seed)\u001b[0m\n\u001b[0;32m     78\u001b[0m print_divider()\n\u001b[0;32m     80\u001b[0m trainer \u001b[38;5;241m=\u001b[39m PolyphemusTrainer(model_dir, vae, optimizer, lr_scheduler\u001b[38;5;241m=\u001b[39mlr_scheduler,\n\u001b[0;32m     81\u001b[0m                             beta_scheduler\u001b[38;5;241m=\u001b[39mbeta_scheduler, save_every\u001b[38;5;241m=\u001b[39msave_every,\n\u001b[0;32m     82\u001b[0m                             print_every\u001b[38;5;241m=\u001b[39mprint_every, eval_every\u001b[38;5;241m=\u001b[39meval_every, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m---> 83\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 141\u001b[0m, in \u001b[0;36mPolyphemusTrainer.train\u001b[1;34m(self, trainloader, validloader, epochs, early_exit)\u001b[0m\n\u001b[0;32m    135\u001b[0m s_tensor, c_tensor \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39ms_tensor, graph\u001b[38;5;241m.\u001b[39mc_tensor\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m# Forward pass to obtain mu, log(sigma^2), computed by the\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# encoder, and structure and content logits, computed by the\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# decoder\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m     (s_logits, c_logits), mu, log_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# Compute losses\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     tot_loss, losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_losses(\n\u001b[0;32m    145\u001b[0m         s_tensor, s_logits,\n\u001b[0;32m    146\u001b[0m         c_tensor, c_logits,\n\u001b[0;32m    147\u001b[0m         mu, log_var\n\u001b[0;32m    148\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 666\u001b[0m, in \u001b[0;36mVAE.forward\u001b[1;34m(self, graph)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph):\n\u001b[0;32m    664\u001b[0m \n\u001b[0;32m    665\u001b[0m     \u001b[38;5;66;03m# Encoder pass\u001b[39;00m\n\u001b[1;32m--> 666\u001b[0m     mu, log_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;66;03m# Reparameterization trick\u001b[39;00m\n\u001b[0;32m    669\u001b[0m     z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m log_var)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 467\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, graph)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph):\n\u001b[0;32m    466\u001b[0m     z_s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms_encoder(graph)\n\u001b[1;32m--> 467\u001b[0m     z_c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;66;03m# Merge content and structure representations\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     z_g \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((z_c, z_s), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 401\u001b[0m, in \u001b[0;36mContentEncoder.forward\u001b[1;34m(self, graph)\u001b[0m\n\u001b[0;32m    399\u001b[0m graph\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m out\n\u001b[0;32m    400\u001b[0m graph\u001b[38;5;241m.\u001b[39mdistinct_bars \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mbars \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_bars\u001b[38;5;241m*\u001b[39mgraph\u001b[38;5;241m.\u001b[39mbatch\n\u001b[1;32m--> 401\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;66;03m# n_nodes x d\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \n\u001b[0;32m    404\u001b[0m \u001b[38;5;66;03m# Aggregate final node states into bar encodings with soft attention\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 189\u001b[0m, in \u001b[0;36mGCN.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m    188\u001b[0m     x, edge_index, edge_attrs \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39medge_attrs\n\u001b[1;32m--> 189\u001b[0m     edge_type \u001b[38;5;241m=\u001b[39m \u001b[43medge_attrs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    190\u001b[0m     edge_attr \u001b[38;5;241m=\u001b[39m edge_attrs[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)):\n",
      "\u001b[1;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_polyphemus(dataset_dir, output_dir, training_config, model_name=None, \n",
    "                     save_every=10, print_every=1, eval=True, eval_every=None, \n",
    "                     use_gpu=False, gpu_id=0, num_workers=10, tr_split=0.7, \n",
    "                     vl_split=0.1, max_epochs=100, seed=None):\n",
    "    \n",
    "    if seed is not None:\n",
    "        set_seed(seed)\n",
    "\n",
    "    # Explicitly set device to CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "        \n",
    "    # Use the provided training configuration directly\n",
    "    print(\"Using the provided training configuration.\")\n",
    "    \n",
    "    n_bars = training_config['model']['n_bars']\n",
    "    batch_size = training_config['batch_size']\n",
    "        \n",
    "    print(\"Preparing datasets and dataloaders...\")\n",
    "    \n",
    "    dataset = PolyphemusDataset(dataset_dir, n_bars=n_bars)\n",
    "    \n",
    "    tr_len = int(tr_split * len(dataset))\n",
    "    \n",
    "    if eval:\n",
    "        vl_len = int(vl_split * len(dataset))\n",
    "        ts_len = len(dataset) - tr_len - vl_len\n",
    "        lengths = (tr_len, vl_len, ts_len)\n",
    "    else:\n",
    "        ts_len = len(dataset) - tr_len\n",
    "        lengths = (tr_len, ts_len)\n",
    "        \n",
    "    split = random_split(dataset, lengths)\n",
    "    tr_set = split[0]\n",
    "    vl_set = split[1] if eval else None\n",
    "\n",
    "    trainloader = DataLoader(tr_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    if eval:\n",
    "        validloader = DataLoader(vl_set, batch_size=batch_size, shuffle=False,\n",
    "                                 num_workers=num_workers)\n",
    "        eval_every = len(trainloader) if eval_every is None else eval_every\n",
    "    else:\n",
    "        validloader = None\n",
    "        eval_every = None\n",
    "\n",
    "    model_name = model_name if model_name is not None else str(uuid.uuid1())\n",
    "    model_dir = os.path.join(output_dir, model_name)\n",
    "    \n",
    "    # Create output directory if it does not exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create model output directory (raise error if it already exists to avoid overwriting a trained model) \n",
    "    os.makedirs(model_dir, exist_ok=False)\n",
    "    \n",
    "    # Create the model\n",
    "    print(\"Creating the model and moving it on {} device...\".format(device))\n",
    "    vae = VAE(**training_config['model'], device=device).to(device)\n",
    "    print_params(vae)\n",
    "    print()\n",
    "    \n",
    "    # Creating optimizer and schedulers\n",
    "    optimizer = optim.Adam(vae.parameters(), **training_config['optimizer'])\n",
    "    lr_scheduler = ExpDecayLRScheduler(optimizer=optimizer, **training_config['lr_scheduler'])\n",
    "    beta_scheduler = StepBetaScheduler(**training_config['beta_scheduler'])\n",
    "    \n",
    "    # Save config\n",
    "    config_path = os.path.join(model_dir, 'configuration')\n",
    "    torch.save(training_config, config_path) \n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    print_divider()\n",
    "\n",
    "    trainer = PolyphemusTrainer(model_dir, vae, optimizer, lr_scheduler=lr_scheduler,\n",
    "                                beta_scheduler=beta_scheduler, save_every=save_every,\n",
    "                                print_every=print_every, eval_every=eval_every, device=device)\n",
    "    trainer.train(trainloader, validloader=validloader, epochs=max_epochs)\n",
    "\n",
    "# Example usage in a Jupyter Notebook\n",
    "dataset_dir = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed'\n",
    "output_dir = 'path/to/save/output'\n",
    "training_config = {\n",
    "    \"batch_size\": 256,\n",
    "    \"model\": {\n",
    "        \"dropout\": 0,\n",
    "        \"batch_norm\": True,\n",
    "        \"gnn_n_layers\": 8,\n",
    "        \"d\": 512,\n",
    "        \"n_bars\": 2,\n",
    "        \"resolution\": 8\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"betas\": (0.9, 0.999),  # Adjusted to be a tuple with two values\n",
    "        \"eps\": 1e-09,\n",
    "        \"lr\": 5e-06\n",
    "    },\n",
    "    \"lr_scheduler\": {\n",
    "        \"peak_lr\": 0.0001,\n",
    "        \"final_lr_scale\": 0.01,\n",
    "        \"warmup_steps\": 8000,\n",
    "        \"decay_steps\": 800000\n",
    "    },\n",
    "    \"beta_scheduler\": {\n",
    "        \"anneal_start\": 40000,\n",
    "        \"beta_max\": 0.01,\n",
    "        \"step_size\": 0.001,\n",
    "        \"anneal_end\": 500000\n",
    "    }\n",
    "}\n",
    "train_polyphemus(dataset_dir, output_dir, training_config, use_gpu=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
