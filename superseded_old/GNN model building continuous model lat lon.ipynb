{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Values Testing Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_geographical_data(samples=10, n_timesteps=100):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Latitude and Longitude ranges for New York City\n",
    "    lat_range = (40.7128, 40.7480)\n",
    "    lon_range = (-74.0060, -73.9352)\n",
    "\n",
    "    # Generate random lat-lon coordinates\n",
    "    latitudes = np.random.uniform(low=lat_range[0], high=lat_range[1], size=(samples, n_timesteps))\n",
    "    longitudes = np.random.uniform(low=lon_range[0], high=lon_range[1], size=(samples, n_timesteps))\n",
    "\n",
    "    # Simulate % greenage (0 to 100%)\n",
    "    greenage = np.random.uniform(low=0, high=100, size=(samples, n_timesteps))\n",
    "\n",
    "    # Normalize the features\n",
    "    latitudes_normalized = (latitudes - lat_range[0]) / (lat_range[1] - lat_range[0])\n",
    "    longitudes_normalized = (longitudes - lon_range[0]) / (lon_range[1] - lon_range[0])\n",
    "    greenage_normalized = greenage / 100  # Since greenage is already within 0 to 100%\n",
    "\n",
    "    return latitudes_normalized, longitudes_normalized, greenage_normalized\n",
    "\n",
    "# Example usage\n",
    "latitudes, longitudes, greenage = generate_geographical_data(samples=10, n_timesteps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6364104112637512, 0)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latitudes[1, 1], 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "csv_file = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\sample data\\diary.csv' \n",
    "df = pd.read_csv(csv_file)  \n",
    "\n",
    "# Assuming df is your original DataFrame\n",
    "lat_lon_df = df[['lat', 'lon']].copy()\n",
    "lat_lon_df.dropna(subset=['lat', 'lon'], inplace=True)\n",
    "\n",
    "# Rename 'lat' and 'lon' columns\n",
    "lat_lon_df.rename(columns={'lat': 'Latitude', 'lon': 'Longitude'}, inplace=True)\n",
    "\n",
    "# Save the modified DataFrame to a CSV file\n",
    "lat_lon_df.to_csv('lat_lon.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: preprocessed\\sample_0_segment_0.npz\n",
      "File saved: preprocessed\\sample_0_segment_1.npz\n",
      "File saved: preprocessed\\sample_1_segment_0.npz\n",
      "File saved: preprocessed\\sample_1_segment_1.npz\n",
      "File saved: preprocessed\\sample_2_segment_0.npz\n",
      "File saved: preprocessed\\sample_2_segment_1.npz\n",
      "File saved: preprocessed\\sample_3_segment_0.npz\n",
      "File saved: preprocessed\\sample_3_segment_1.npz\n",
      "File saved: preprocessed\\sample_4_segment_0.npz\n",
      "File saved: preprocessed\\sample_4_segment_1.npz\n",
      "File saved: preprocessed\\sample_5_segment_0.npz\n",
      "File saved: preprocessed\\sample_5_segment_1.npz\n",
      "File saved: preprocessed\\sample_6_segment_0.npz\n",
      "File saved: preprocessed\\sample_6_segment_1.npz\n",
      "File saved: preprocessed\\sample_7_segment_0.npz\n",
      "File saved: preprocessed\\sample_7_segment_1.npz\n",
      "File saved: preprocessed\\sample_8_segment_0.npz\n",
      "File saved: preprocessed\\sample_8_segment_1.npz\n",
      "File saved: preprocessed\\sample_9_segment_0.npz\n",
      "File saved: preprocessed\\sample_9_segment_1.npz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "from enum import Enum\n",
    "\n",
    "# Constants\n",
    "N_DISCRETE_VALUES = 4  \n",
    "N_SAMPLES = 10\n",
    "N_TIMESTEPS = 100  # For the initial data generation\n",
    "PAD_VALUE = 0  # Define a padding value for activities and modes\n",
    "MAX_SIMU_TOKENS = 16  \n",
    "N_TRACKS = 4  # Number of \"instrument\" tracks\n",
    "N_BARS = 2  # Assuming 2 bars for simplicity, bars = days\n",
    "RESOLUTION = 8  # Assuming 8 timesteps per beat, adjust as needed, hours ?\n",
    "\n",
    "# Assuming the location track is the second track (index 1)\n",
    "LOCATION_TRACK_IDX = 1  # Adjust this index based on which track you want to use for location data\n",
    "\n",
    "class EdgeTypes(Enum):\n",
    "    TRACK = 0 # This has to be interpreted as the starting index\n",
    "    ONSET = N_TRACKS\n",
    "    NEXT = N_TRACKS + 1\n",
    "\n",
    "# N_TRACKS track types + 1 onset edge type + 1 next edge type\n",
    "N_EDGE_TYPES = N_TRACKS + 2\n",
    "\n",
    "def generate_sample_data():\n",
    "    # Generate emotions and locations with random values\n",
    "    emotions = np.random.randint(0, N_DISCRETE_VALUES, (N_SAMPLES, N_TIMESTEPS))\n",
    "    locations = np.random.randint(0, N_DISCRETE_VALUES, (N_SAMPLES, N_TIMESTEPS))\n",
    "    return emotions, locations\n",
    "\n",
    "# Generate or load real data for emotions, locations, activities, and modes\n",
    "# This is a placeholder. Replace it with actual data loading or generation\n",
    "emotions, locations = generate_sample_data()  # You need to define this function\n",
    "latitudes, longitudes, greenage = generate_geographical_data()  \n",
    "\n",
    "# Calculate window size and stride for sliding windows\n",
    "window_size = N_BARS * 4 * RESOLUTION\n",
    "stride = window_size // 2\n",
    "\n",
    "def preprocess_sample_data(csv_file, dest_dir):\n",
    "    # Load CSV and preprocess activities and modes\n",
    "    df = pd.read_csv(csv_file)\n",
    "    activities_map = {activity: i for i, activity in enumerate(df['mainActivity'].unique(), start=1)}\n",
    "    modes_map = {mode: i for i, mode in enumerate(df['howTravelled'].unique(), start=1)}\n",
    "    df['activity_int'] = df['mainActivity'].fillna('PAD').map(lambda x: activities_map.get(x, PAD_VALUE))\n",
    "    df['mode_int'] = df['howTravelled'].fillna('PAD').map(lambda x: modes_map.get(x, PAD_VALUE))\n",
    "        \n",
    "    activities = np.full((N_SAMPLES, N_TIMESTEPS), PAD_VALUE, dtype=int)\n",
    "    modes = np.full((N_SAMPLES, N_TIMESTEPS), PAD_VALUE, dtype=int)\n",
    "    activities[:min(N_SAMPLES, len(df)), :] = df['activity_int'].head(N_TIMESTEPS)\n",
    "    modes[:min(N_SAMPLES, len(df)), :] = df['mode_int'].head(N_TIMESTEPS)\n",
    "        \n",
    "    for sample_idx in range(N_SAMPLES):\n",
    "        # Initialize tensors for this sample with the full length first\n",
    "        full_c_tensor = np.zeros((N_TRACKS, N_TIMESTEPS, MAX_SIMU_TOKENS, 2), dtype=np.float32)\n",
    "\n",
    "        # Populate the full tensors\n",
    "        for t in range(N_TIMESTEPS):\n",
    "            for track_idx, data in enumerate([emotions, locations, activities, modes]):\n",
    "                value = data[sample_idx, t]\n",
    "                if track_idx == LOCATION_TRACK_IDX:\n",
    "                    # Populate latitude, longitude, and greenage in the location track\n",
    "                    full_c_tensor[track_idx, t, 0, :] = [value, 1]  # Emotion/Location value and duration\n",
    "                    full_c_tensor[track_idx, t, 1, :] = [latitudes[sample_idx, t], 1]  # Latitude\n",
    "                    # print(full_c_tensor[track_idx, t, 1, :])\n",
    "                    full_c_tensor[track_idx, t, 2, :] = [longitudes[sample_idx, t], 1]  # Longitude\n",
    "                    full_c_tensor[track_idx, t, 3, :] = [greenage[sample_idx, t], 1]  # Greenage\n",
    "                else:\n",
    "                    # Populate other tracks with their respective data\n",
    "                    full_c_tensor[track_idx, t, 0, :] = [value, 1]  # Example of populating non-location data\n",
    "\n",
    "        # Calculate full_s_tensor based on non-zero values in full_c_tensor for the current sample\n",
    "        full_s_tensor = np.any(full_c_tensor != 0, axis=(2, 3)).astype(int)\n",
    "\n",
    "        # Windowing over time\n",
    "        for start_idx in range(0, N_TIMESTEPS - window_size + 1, stride):\n",
    "            c_tensor_segment = full_c_tensor[:, start_idx:start_idx + window_size, :, :]\n",
    "            s_tensor_segment = full_s_tensor[:, start_idx:start_idx + window_size]\n",
    "\n",
    "            # Save the tensors for this segment to an .npz file\n",
    "            sample_filepath = os.path.join(dest_dir, f\"sample_{sample_idx}_segment_{start_idx//stride}.npz\")\n",
    "            try:\n",
    "                np.savez(sample_filepath, c_tensor=c_tensor_segment, s_tensor=s_tensor_segment)\n",
    "                print(f\"File saved: {sample_filepath}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to save {sample_filepath}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "csv_file = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\sample data\\diary.csv'   # Update this path to your CSV file\n",
    "dest_dir = 'preprocessed'  # Destination directory for preprocessed data\n",
    "preprocess_sample_data(csv_file, dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of c_tensor: (4, 64, 16, 2)\n",
      "Shape of s_tensor: (4, 64)\n"
     ]
    }
   ],
   "source": [
    "file_path = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed\\sample_1_segment_0.npz'\n",
    "\n",
    "# Load the .npz file\n",
    "data = np.load(file_path)\n",
    "\n",
    "# Access the tensors\n",
    "c_tensor = data['c_tensor']\n",
    "s_tensor = data['s_tensor']\n",
    "\n",
    "# Print their shapes\n",
    "print(f'Shape of c_tensor: {c_tensor.shape}')\n",
    "print(f'Shape of s_tensor: {s_tensor.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.        , 1.        ],\n",
       "       [0.5085707 , 1.        ],\n",
       "       [0.92775226, 1.        ],\n",
       "       [0.6958991 , 1.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_tensor[1, 3, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.collate import collate\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "\n",
    "def get_node_labels(s_tensor, ones_idxs):\n",
    "    # Build a tensor which has node labels in place of each activation in the\n",
    "    # stucture tensor\n",
    "    labels = torch.zeros_like(s_tensor, dtype=torch.long, \n",
    "                              device=s_tensor.device)\n",
    "    n_nodes = len(ones_idxs[0])\n",
    "    labels[ones_idxs] = torch.arange(n_nodes, device=s_tensor.device)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_track_edges(s_tensor, ones_idxs=None, node_labels=None):\n",
    "\n",
    "    track_edges = []\n",
    "\n",
    "    if ones_idxs is None:\n",
    "        # Indices where the binary structure tensor is active\n",
    "        ones_idxs = torch.nonzero(s_tensor, as_tuple=True)\n",
    "\n",
    "    if node_labels is None:\n",
    "        node_labels = get_node_labels(s_tensor, ones_idxs)\n",
    "\n",
    "    # For each track, add direct and inverse edges between consecutive nodes\n",
    "    for track in range(s_tensor.size(0)):\n",
    "        # List of active timesteps in the current track\n",
    "        tss = list(ones_idxs[1][ones_idxs[0] == track])\n",
    "        edge_type = EdgeTypes.TRACK.value + track\n",
    "        edges = [\n",
    "            # Edge tuple: (u, v, type, ts_distance). Zip is used to obtain\n",
    "            # consecutive active timesteps. Edges in different tracks have\n",
    "            # different types.\n",
    "            (node_labels[track, t1],\n",
    "             node_labels[track, t2], edge_type, t2 - t1)\n",
    "            for t1, t2 in zip(tss[:-1], tss[1:])\n",
    "        ]\n",
    "        inverse_edges = [(u, v, t, d) for (v, u, t, d) in edges]\n",
    "        track_edges.extend(edges + inverse_edges)\n",
    "\n",
    "    return torch.tensor(track_edges, dtype=torch.long)\n",
    "\n",
    "\n",
    "def get_onset_edges(s_tensor, ones_idxs=None, node_labels=None):\n",
    "\n",
    "    onset_edges = []\n",
    "    edge_type = EdgeTypes.ONSET.value\n",
    "\n",
    "    if ones_idxs is None:\n",
    "        # Indices where the binary structure tensor is active\n",
    "        ones_idxs = torch.nonzero(s_tensor, as_tuple=True)\n",
    "\n",
    "    if node_labels is None:\n",
    "        node_labels = get_node_labels(s_tensor, ones_idxs)\n",
    "\n",
    "    # Add direct and inverse edges between nodes played in the same timestep\n",
    "    for ts in range(s_tensor.size(1)):\n",
    "        # List of active tracks in the current timestep\n",
    "        tracks = list(ones_idxs[0][ones_idxs[1] == ts])\n",
    "        # Obtain all possible pairwise combinations of active tracks\n",
    "        combinations = list(itertools.combinations(tracks, 2))\n",
    "        edges = [\n",
    "            # Edge tuple: (u, v, type, ts_distance(=0)).\n",
    "            (node_labels[track1, ts], node_labels[track2, ts], edge_type, 0)\n",
    "            for track1, track2 in combinations\n",
    "        ]\n",
    "        inverse_edges = [(u, v, t, d) for (v, u, t, d) in edges]\n",
    "        onset_edges.extend(edges + inverse_edges)\n",
    "\n",
    "    return torch.tensor(onset_edges, dtype=torch.long)\n",
    "\n",
    "\n",
    "def get_next_edges(s_tensor, ones_idxs=None, node_labels=None):\n",
    "\n",
    "    next_edges = []\n",
    "    edge_type = EdgeTypes.NEXT.value\n",
    "\n",
    "    if ones_idxs is None:\n",
    "        # Indices where the binary structure tensor is active\n",
    "        ones_idxs = torch.nonzero(s_tensor, as_tuple=True)\n",
    "\n",
    "    if node_labels is None:\n",
    "        node_labels = get_node_labels(s_tensor, ones_idxs)\n",
    "\n",
    "    # List of active timesteps\n",
    "    tss = torch.nonzero(torch.any(s_tensor.bool(), dim=0)).squeeze()\n",
    "    if tss.dim() == 0:\n",
    "        return torch.tensor([], dtype=torch.long)\n",
    "\n",
    "    for i in range(tss.size(0)-1):\n",
    "        # Get consecutive active timesteps\n",
    "        t1, t2 = tss[i], tss[i+1]\n",
    "        # Get all the active tracks in the two timesteps\n",
    "        t1_tracks = ones_idxs[0][ones_idxs[1] == t1]\n",
    "        t2_tracks = ones_idxs[0][ones_idxs[1] == t2]\n",
    "\n",
    "        # Combine the source and destination tracks, removing combinations with\n",
    "        # the same source and destination track (since these represent track\n",
    "        # edges).\n",
    "        tracks_product = list(itertools.product(t1_tracks, t2_tracks))\n",
    "        tracks_product = [(track1, track2)\n",
    "                          for (track1, track2) in tracks_product\n",
    "                          if track1 != track2]\n",
    "        # Edge tuple: (u, v, type, ts_distance).\n",
    "        edges = [(node_labels[track1, t1], node_labels[track2, t2],\n",
    "                  edge_type, t2 - t1)\n",
    "                 for track1, track2 in tracks_product]\n",
    "\n",
    "        next_edges.extend(edges)\n",
    "\n",
    "    return torch.tensor(next_edges, dtype=torch.long)\n",
    "\n",
    "\n",
    "def get_track_features(s_tensor):\n",
    "\n",
    "    # Indices where the binary structure tensor is active\n",
    "    ones_idxs = torch.nonzero(s_tensor)\n",
    "\n",
    "    n_nodes = len(ones_idxs)\n",
    "    tracks = ones_idxs[:, 0]\n",
    "    n_tracks = s_tensor.size(0)\n",
    "\n",
    "    # The feature n_nodes x n_tracks tensor contains one-hot tracks\n",
    "    # representations for each node\n",
    "    features = torch.zeros((n_nodes, n_tracks))\n",
    "    features[torch.arange(n_nodes), tracks] = 1\n",
    "\n",
    "    return features\n",
    "\n",
    "def custom_collate(data_list):\n",
    "    batch = Batch.from_data_list(data_list)\n",
    "    if hasattr(batch, 'batch'):\n",
    "        batch.bars = batch.batch.clone()  # Optionally clone to ensure no accidental modification\n",
    "    return batch\n",
    "\n",
    "\n",
    "def graph_from_tensor(s_tensor):\n",
    "\n",
    "    bars = []\n",
    "\n",
    "    # Iterate over bars and construct a graph for each bar\n",
    "    for i in range(s_tensor.size(0)):\n",
    "\n",
    "        bar = s_tensor[i]\n",
    "\n",
    "        # If the bar contains no activations, add a fake one to avoid having \n",
    "        # to deal with empty graphs\n",
    "        if not torch.any(bar):\n",
    "            bar[0, 0] = 1\n",
    "\n",
    "        # Get edges from boolean activations\n",
    "        track_edges = get_track_edges(bar)\n",
    "        onset_edges = get_onset_edges(bar)\n",
    "        next_edges = get_next_edges(bar)\n",
    "        edges = [track_edges, onset_edges, next_edges]\n",
    "\n",
    "        # Concatenate edge tensors (N x 4) (if any)\n",
    "        is_edgeless = (len(track_edges) == 0 and\n",
    "                       len(onset_edges) == 0 and\n",
    "                       len(next_edges) == 0)\n",
    "        if not is_edgeless:\n",
    "            edge_list = torch.cat([x for x in edges\n",
    "                                   if torch.numel(x) > 0])\n",
    "\n",
    "        # Adapt tensor to torch_geometric's Data\n",
    "        # If no edges, add fake self-edge\n",
    "        # edge_list[:, :2] contains source and destination node labels\n",
    "        # edge_list[:, 2:] contains edge types and timestep distances\n",
    "        edge_index = (edge_list[:, :2].t().contiguous() if not is_edgeless else\n",
    "                      torch.LongTensor([[0], [0]]))\n",
    "        attrs = (edge_list[:, 2:] if not is_edgeless else\n",
    "                 torch.Tensor([[0, 0]]))\n",
    "\n",
    "        # Add one hot timestep distance to edge attributes\n",
    "        edge_attr = torch.zeros(attrs.size(0), s_tensor.shape[-1] + 1)\n",
    "        edge_attr[:, 0] = attrs[:, 0]\n",
    "        edge_attr[torch.arange(edge_attr.size(0)),\n",
    "                   attrs.long()[:, 1] + 1] = 1\n",
    "        # print(type(edge_attr))\n",
    "\n",
    "        node_features = get_track_features(bar)\n",
    "        is_drum = node_features[:, 0].bool()\n",
    "        num_nodes = torch.sum(bar, dtype=torch.long)\n",
    "        bars.append(Data(edge_index=edge_index, edge_attr=edge_attr,\n",
    "                         num_nodes=num_nodes, node_features=node_features,\n",
    "                         is_drum=is_drum).to(s_tensor.device))\n",
    "\n",
    "    # Use custom collate function to merge all Data objects into a single Batch\n",
    "    #graph = custom_collate(bars)\n",
    "\n",
    "    # Manage the batch attribute as bars, if it exists\n",
    "    #if hasattr(graph, 'batch'):\n",
    "    #    graph.bars = graph.batch.clone()  # Cloning to ensure that original batch numbers are preserved\n",
    "    #    # print(\"Bars attribute added to graph\")\n",
    "\n",
    "    #return graph\n",
    "\n",
    "    # Merge the graphs corresponding to different bars into a single big graph\n",
    "    graph, _, _ = collate(\n",
    "        Data,\n",
    "        data_list=bars,\n",
    "        increment=True,\n",
    "        add_batch=True\n",
    "    )\n",
    "\n",
    "    # Change bars assignment vector name (otherwise, Dataloader's collate\n",
    "    # would overwrite graphs.batch)\n",
    "    graph.bars = graph.batch\n",
    "\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Dataset\n",
    "\n",
    "N_PITCH_TOKENS = 131\n",
    "N_DUR_TOKENS = 99\n",
    "D_TOKEN_PAIR = N_PITCH_TOKENS + N_DUR_TOKENS\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "N_PITCH_TOKENS = 131\n",
    "N_DUR_TOKENS = 99\n",
    "D_TOKEN_PAIR = N_PITCH_TOKENS + N_DUR_TOKENS\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "class PolyphemusDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dir, n_bars=2):\n",
    "        self.dir = dir\n",
    "        self.files = [entry.path for entry in os.scandir(self.dir) if entry.is_file()]\n",
    "        self.len = len(self.files)\n",
    "        self.n_bars = n_bars\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load tensors\n",
    "        sample_path = self.files[idx]\n",
    "        data = np.load(sample_path, allow_pickle=True)\n",
    "        c_tensor = torch.tensor(data[\"c_tensor\"], dtype=torch.float32)\n",
    "        s_tensor = torch.tensor(data[\"s_tensor\"], dtype=torch.bool)\n",
    "\n",
    "        # Split continuous and categorical data\n",
    "        continuous_data = c_tensor[:, :, 1:4]  # Assuming continuous data indices\n",
    "        categorical_data = c_tensor[:, :, :1]  # Assuming categorical data indices (only pitches for simplicity)\n",
    "\n",
    "        # Reshape for `n_bars` alignment\n",
    "        # Ensure that `n_timesteps` is divisible evenly by `n_bars`\n",
    "        n_timesteps_per_bar = c_tensor.shape[1] // self.n_bars\n",
    "\n",
    "        continuous_data = continuous_data.reshape(-1, self.n_bars, n_timesteps_per_bar, continuous_data.shape[2], continuous_data.shape[3])\n",
    "        continuous_data = continuous_data.permute(1, 0, 2, 3, 4)  # Reorder to (n_bars, batch, timesteps, features, feature_dims)\n",
    "\n",
    "        categorical_data = categorical_data.reshape(-1, self.n_bars, n_timesteps_per_bar, categorical_data.shape[2], categorical_data.shape[3])\n",
    "        categorical_data = categorical_data.permute(1, 0, 2, 3, 4)  # Reorder similarly\n",
    "\n",
    "        s_tensor = s_tensor.reshape(-1, self.n_bars, n_timesteps_per_bar)\n",
    "        s_tensor = s_tensor.permute(1, 0, 2)  # Reorder for structure tensor\n",
    "\n",
    "        s_tensor = s_tensor.reshape(-1, self.n_bars, s_tensor.shape[1] // self.n_bars)\n",
    "        s_tensor = s_tensor.permute(1, 0, 2)\n",
    "\n",
    "        # Process pitches as one-hot encoding\n",
    "        pitches = categorical_data[..., 0]\n",
    "        onehot_p = torch.zeros((pitches.numel(), N_PITCH_TOKENS), dtype=torch.float32)\n",
    "        onehot_p[torch.arange(pitches.numel()), pitches.long().flatten()] = 1.\n",
    "        onehot_p = onehot_p.view(*pitches.shape, N_PITCH_TOKENS)\n",
    "\n",
    "        # Assuming durations follow pitches in the second channel of the original categorical data\n",
    "        durs = categorical_data[..., 1]\n",
    "        onehot_d = torch.zeros((durs.numel(), N_DUR_TOKENS), dtype=torch.float32)\n",
    "        onehot_d[torch.arange(durs.numel()), durs.long().flatten()] = 1.\n",
    "        onehot_d = onehot_d.view(*durs.shape, N_DUR_TOKENS)\n",
    "    \n",
    "        # Match dimensions for concatenation\n",
    "        pad_size_p = (0, 0, continuous_data.shape[3] - 1, 0)  # Add padding to match the second last dimension\n",
    "        pad_size_d = (0, 0, continuous_data.shape[3] - 1, 0)\n",
    "        onehot_p = pad(onehot_p, pad_size_p, \"constant\", 0)\n",
    "        onehot_d = pad(onehot_d, pad_size_d, \"constant\", 0)\n",
    "\n",
    "        # Concatenate tensors\n",
    "        c_tensor = torch.cat((onehot_p, onehot_d, continuous_data), dim=-1)\n",
    "\n",
    "        # Build graph structure from structure tensor\n",
    "        graph = graph_from_tensor(s_tensor)\n",
    "        #print(type(graph.edge_attr))  # This should print <class 'torch.Tensor'>\n",
    "\n",
    "        # Filter silences in order to get a sparse representation\n",
    "        c_tensor = c_tensor.reshape(-1, c_tensor.shape[-2], c_tensor.shape[-1])\n",
    "        c_tensor = c_tensor[s_tensor.reshape(-1).bool()]\n",
    "\n",
    "        graph.c_tensor = c_tensor\n",
    "        graph.s_tensor = s_tensor.float()\n",
    "\n",
    "\n",
    "        return graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Dataset\n",
    "\n",
    "# Assuming constants are defined somewhere in your codebase\n",
    "N_PITCH_TOKENS = 131\n",
    "N_DUR_TOKENS = 99\n",
    "\n",
    "# Import the PolyphemusDataset class definition here\n",
    "\n",
    "# Define the directory where your dataset is located\n",
    "data_dir = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed'\n",
    "\n",
    "# Create an instance of the PolyphemusDataset\n",
    "dataset = PolyphemusDataset(dir=data_dir, n_bars=2)\n",
    "\n",
    "# Access the first item in the dataset to check edge_attr\n",
    "graph = dataset[0]  # This will call the __getitem__ method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph: Data(edge_index=[2, 24448], edge_attr=[24448, 3], num_nodes=256, node_features=[256, 64], is_drum=[256], batch=[256], ptr=[3], bars=[256], c_tensor=[256, 3, 232], s_tensor=[2, 64, 2])\n",
      "Graph.c_tensor shape: torch.Size([256, 3, 232])\n",
      "Graph.s_tensor shape: torch.Size([2, 64, 2])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Dataset\n",
    "\n",
    "# Constants for one-hot encoding\n",
    "N_PITCH_TOKENS = 131\n",
    "N_DUR_TOKENS = 99\n",
    "D_TOKEN_PAIR = N_PITCH_TOKENS + N_DUR_TOKENS\n",
    "\n",
    "# PolyphemusDataset class definition here (as provided in your question)\n",
    "\n",
    "# Create an instance of the PolyphemusDataset\n",
    "dir = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed'\n",
    "dataset = PolyphemusDataset(dir, n_bars=2)\n",
    "\n",
    "# Load a sample from the dataset\n",
    "sample_index = 0  # For example, load the first sample\n",
    "graph = dataset[sample_index]\n",
    "\n",
    "# Inspect the graph and its tensors\n",
    "print(\"Graph:\", graph)\n",
    "print(\"Graph.c_tensor shape:\", graph.c_tensor.shape)\n",
    "print(\"Graph.s_tensor shape:\", graph.s_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [05:42<?, ?it/s]\n",
      "  0%|          | 0/7 [03:47<?, ?it/s]\n",
      "  0%|          | 0/7 [02:09<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from typing import Union, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "from torch_sparse import SparseTensor, masked_select_nnz\n",
    "from torch_geometric.typing import OptTensor, Adj\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "from torch_geometric.nn.glob import GlobalAttention\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.nn.conv import RGCNConv\n",
    "\n",
    "@torch.jit._overload\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    # type: (Tensor, Tensor) -> Tensor\n",
    "    pass\n",
    "\n",
    "\n",
    "@torch.jit._overload\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    # type: (SparseTensor, Tensor) -> SparseTensor\n",
    "    pass\n",
    "\n",
    "\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    if isinstance(edge_index, Tensor):\n",
    "        return edge_index[:, edge_mask]\n",
    "    else:\n",
    "        return masked_select_nnz(edge_index, edge_mask, layout='coo')\n",
    "\n",
    "\n",
    "def masked_edge_attr(edge_attr, edge_mask):\n",
    "    return edge_attr[edge_mask, :]\n",
    "\n",
    "\n",
    "class GCL(RGCNConv):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, num_relations, nn,\n",
    "                 dropout=0.1, **kwargs):\n",
    "        super().__init__(in_channels=in_channels, out_channels=out_channels,\n",
    "                         num_relations=num_relations, **kwargs)\n",
    "        self.nn = nn\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_edge_nn()\n",
    "\n",
    "    def reset_edge_nn(self):\n",
    "        reset(self.nn)\n",
    "\n",
    "    def forward(self, x: Union[OptTensor, Tuple[OptTensor, Tensor]],\n",
    "                edge_index: Adj, edge_type: OptTensor = None,\n",
    "                edge_attr: OptTensor = None):\n",
    "        # print(f\"Before processing: x shape: {x.shape}, edge_index shape: {edge_index.shape}, edge_type shape: {edge_type.shape}, edge_attr shape: {edge_attr.shape}\")\n",
    "        # Convert input features to a pair of node features or node indices.\n",
    "        x_l: OptTensor = None\n",
    "        if isinstance(x, tuple):\n",
    "            x_l = x[0]\n",
    "        else:\n",
    "            x_l = x\n",
    "        if x_l is None:\n",
    "            x_l = torch.arange(self.in_channels_l, device=self.weight.device)\n",
    "\n",
    "        x_r: Tensor = x_l\n",
    "        if isinstance(x, tuple):\n",
    "            x_r = x[1]\n",
    "\n",
    "        size = (x_l.size(0), x_r.size(0))\n",
    "\n",
    "        if isinstance(edge_index, SparseTensor):\n",
    "            edge_type = edge_index.storage.value()\n",
    "        assert edge_type is not None\n",
    "\n",
    "        # propagate_type: (x: Tensor)\n",
    "        out = torch.zeros(x_r.size(0), self.out_channels, device=x_r.device)\n",
    "        weight = self.weight\n",
    "\n",
    "        # Basis-decomposition\n",
    "        if self.num_bases is not None:\n",
    "            weight = (self.comp @ weight.view(self.num_bases, -1)).view(\n",
    "                self.num_relations, self.in_channels_l, self.out_channels)\n",
    "\n",
    "        # Block-diagonal-decomposition\n",
    "        if self.num_blocks is not None:\n",
    "\n",
    "            if x_l.dtype == torch.long and self.num_blocks is not None:\n",
    "                raise ValueError('Block-diagonal decomposition not supported '\n",
    "                                 'for non-continuous input features.')\n",
    "\n",
    "            for i in range(self.num_relations):\n",
    "                tmp = masked_edge_index(edge_index, edge_type == i)\n",
    "                h = self.propagate(tmp, x=x_l, size=size)\n",
    "                h = h.view(-1, weight.size(1), weight.size(2))\n",
    "                h = torch.einsum('abc,bcd->abd', h, weight[i])\n",
    "                out += h.contiguous().view(-1, self.out_channels)\n",
    "\n",
    "        else:\n",
    "            # No regularization/Basis-decomposition\n",
    "            for i in range(self.num_relations):\n",
    "                tmp = masked_edge_index(edge_index, edge_type == i)\n",
    "                attr = masked_edge_attr(edge_attr, edge_type == i)\n",
    "\n",
    "                if x_l.dtype == torch.long:\n",
    "                    out += self.propagate(tmp, x=weight[i, x_l], size=size)\n",
    "                else:\n",
    "                    h = self.propagate(tmp, x=x_l, size=size,\n",
    "                                       edge_attr=attr)\n",
    "                    out = out + (h @ weight[i])\n",
    "\n",
    "        root = self.root\n",
    "        if root is not None:\n",
    "            out += root[x_r] if x_r.dtype == torch.long else x_r @ root\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "        # print(f\"After processing: Output shape: {out.shape}\")\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "\n",
    "        # Use edge nn to compute weight tensor from edge attributes\n",
    "        # (=onehot timestep distances between nodes)\n",
    "        # print(f\"Message: x_j shape: {x_j.shape}, edge_attr shape: {edge_attr.shape}\")\n",
    "        weights = self.nn(edge_attr)\n",
    "        weights = weights[..., :self.in_channels_l]\n",
    "        weights = weights.view(-1, self.in_channels_l)\n",
    "\n",
    "        out = x_j * weights\n",
    "        # print(f\"Message output shape: {out.shape}\")\n",
    "        out = F.relu(out)\n",
    "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=256, hidden_dim=256, output_dim=256,\n",
    "                 num_layers=2, activation=True, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        if num_layers == 1:\n",
    "            self.layers.append(nn.Linear(input_dim, output_dim))\n",
    "        else:\n",
    "            # Input layer (1) + Intermediate layers (n-2) + Output layer (1)\n",
    "            self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            for _ in range(num_layers - 2):\n",
    "                self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        self.activation = activation\n",
    "        self.p = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.dropout(x, p=self.p, training=self.training)\n",
    "            x = layer(x)\n",
    "            if self.activation:\n",
    "                x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=256, hidden_dim=256, n_layers=3,\n",
    "                 num_relations=3, num_dists=32, batch_norm=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.norm_layers = nn.ModuleList()\n",
    "        edge_nn = nn.Linear(num_dists, input_dim)\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.layers.append(GCL(input_dim, hidden_dim, num_relations, edge_nn))\n",
    "        if self.batch_norm:\n",
    "            self.norm_layers.append(BatchNorm(hidden_dim))\n",
    "\n",
    "        for i in range(n_layers-1):\n",
    "            self.layers.append(GCL(hidden_dim, hidden_dim,\n",
    "                                   num_relations, edge_nn))\n",
    "            if self.batch_norm:\n",
    "                self.norm_layers.append(BatchNorm(hidden_dim))\n",
    "\n",
    "        self.p = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        # print(type(edge_attr))\n",
    "        edge_type = edge_attr[:, 0]\n",
    "        edge_attr = edge_attr[:, 1:]\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            # print(f\"Layer {i}: Input shape: {x.shape}\")\n",
    "            residual = x\n",
    "            x = F.dropout(x, p=self.p, training=self.training)\n",
    "            x = self.layers[i](x, edge_index, edge_type, edge_attr)\n",
    "\n",
    "            if self.batch_norm:\n",
    "                x = self.norm_layers[i](x)\n",
    "\n",
    "            x = F.relu(x)\n",
    "            x = residual + x\n",
    "            # print(f\"Layer {i}: Output shape: {x.shape}\")\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, output_dim=64, dense_dim=64, batch_norm=False,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        if batch_norm:\n",
    "            self.conv = nn.Sequential(\n",
    "                # From (4 x 32) to (8 x 4 x 32)\n",
    "                nn.Conv2d(1, 8, 3, padding=1),\n",
    "                nn.BatchNorm2d(8),\n",
    "                nn.ReLU(True),\n",
    "                # From (8 x 4 x 32) to (8 x 4 x 8)\n",
    "                nn.MaxPool2d((1, 4), stride=(1, 4)),\n",
    "                # From (8 x 4 x 8) to (16 x 4 x 8)\n",
    "                nn.Conv2d(8, 16, 3, padding=1),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(True)\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(1, 8, 3, padding=1),\n",
    "                nn.ReLU(True),\n",
    "                nn.MaxPool2d((1, 4), stride=(1, 4)),\n",
    "                nn.Conv2d(8, 16, 3, padding=1),\n",
    "                nn.ReLU(True)\n",
    "            )\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "        # Linear layers\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(16 * 4 * 8, dense_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dense_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=64, dense_dim=64, batch_norm=False,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear decompressors\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(input_dim, dense_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dense_dim, 16 * 4 * 8),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(16, 4, 8))\n",
    "\n",
    "        # Upsample and convolutional layers\n",
    "        if batch_norm:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=(1, 4), mode='nearest'),\n",
    "                nn.Conv2d(16, 8, 3, padding=1),\n",
    "                nn.BatchNorm2d(8),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(8, 1, 3, padding=1)\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=(1, 4), mode='nearest'),\n",
    "                nn.Conv2d(16, 8, 3, padding=1),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(8, 1, 3, padding=1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.conv(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ContentEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        #self.device = device  # Store the device as an instance variable\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        # Pitch and duration embedding layers (separate layers for drums\n",
    "        # and non drums)\n",
    "        self.non_drums_pitch_emb = nn.Linear(N_PITCH_TOKENS, \n",
    "                                             self.d//2)\n",
    "        self.drums_pitch_emb = nn.Linear(N_PITCH_TOKENS, self.d//2)\n",
    "        self.dur_emb = nn.Linear(N_DUR_TOKENS, self.d//2)\n",
    "\n",
    "        # Batch norm layers\n",
    "        self.bn_non_drums = nn.BatchNorm1d(num_features=self.d//2)\n",
    "        self.bn_drums = nn.BatchNorm1d(num_features=self.d//2)\n",
    "        self.bn_dur = nn.BatchNorm1d(num_features=self.d//2)\n",
    "\n",
    "        self.chord_encoder = nn.Linear(\n",
    "            self.d * (MAX_SIMU_TOKENS-1), self.d)\n",
    "\n",
    "        self.graph_encoder = GCN(\n",
    "            dropout=self.dropout,\n",
    "            input_dim=self.d,\n",
    "            hidden_dim=self.d,\n",
    "            n_layers=self.gnn_n_layers,\n",
    "            num_relations=N_EDGE_TYPES,\n",
    "            batch_norm=self.batch_norm\n",
    "        )\n",
    "\n",
    "        # Soft attention node-aggregation layer\n",
    "        gate_nn = nn.Sequential(\n",
    "            MLP(input_dim=self.d, output_dim=1, num_layers=1,\n",
    "                activation=False, dropout=self.dropout),\n",
    "            nn.BatchNorm1d(1)\n",
    "        )\n",
    "        self.graph_attention = GlobalAttention(gate_nn)\n",
    "\n",
    "        self.bars_encoder = nn.Linear(self.n_bars * self.d, self.d)\n",
    "    \n",
    "    def forward(self, graph):\n",
    "        c_tensor = graph.c_tensor\n",
    "        print(c_tensor.shape)\n",
    "        # Discard SOS token\n",
    "        c_tensor = c_tensor[:, 1:, :]\n",
    "\n",
    "        # Get drums and non drums tensors\n",
    "        drums = c_tensor[graph.is_drum]\n",
    "        non_drums = c_tensor[torch.logical_not(graph.is_drum)]\n",
    "\n",
    "        # Compute drums embeddings\n",
    "        sz = drums.size()\n",
    "        drums_pitch = self.drums_pitch_emb(\n",
    "            drums[..., :N_PITCH_TOKENS])\n",
    "        drums_pitch = self.bn_drums(drums_pitch.view(-1, self.d//2))\n",
    "        drums_pitch = drums_pitch.view(sz[0], sz[1], self.d//2)\n",
    "        drums_dur = self.dur_emb(drums[..., N_PITCH_TOKENS:])\n",
    "        drums_dur = self.bn_dur(drums_dur.view(-1, self.d//2))\n",
    "        drums_dur = drums_dur.view(sz[0], sz[1], self.d//2)\n",
    "        drums = torch.cat((drums_pitch, drums_dur), dim=-1)\n",
    "        # n_nodes x MAX_SIMU_TOKENS x d\n",
    "\n",
    "        # Compute non drums embeddings\n",
    "        sz = non_drums.size()\n",
    "        non_drums_pitch = self.non_drums_pitch_emb(\n",
    "            non_drums[..., :N_PITCH_TOKENS]\n",
    "        )\n",
    "        non_drums_pitch = self.bn_non_drums(non_drums_pitch.view(-1, self.d//2))\n",
    "        non_drums_pitch = non_drums_pitch.view(sz[0], sz[1], self.d//2)\n",
    "        non_drums_dur = self.dur_emb(non_drums[..., N_PITCH_TOKENS:])\n",
    "        non_drums_dur = self.bn_dur(non_drums_dur.view(-1, self.d//2))\n",
    "        non_drums_dur = non_drums_dur.view(sz[0], sz[1], self.d//2)\n",
    "        non_drums = torch.cat((non_drums_pitch, non_drums_dur), dim=-1)\n",
    "        # n_nodes x MAX_SIMU_TOKENS x d\n",
    "\n",
    "        # Compute chord embeddings (drums and non drums)\n",
    "        drums = self.chord_encoder(\n",
    "            drums.view(-1, self.d * (MAX_SIMU_TOKENS-1))\n",
    "        )\n",
    "        non_drums = self.chord_encoder(\n",
    "            non_drums.view(-1, self.d * (MAX_SIMU_TOKENS-1))\n",
    "        )\n",
    "        drums = F.relu(drums)\n",
    "        non_drums = F.relu(non_drums)\n",
    "        drums = self.dropout_layer(drums)\n",
    "        non_drums = self.dropout_layer(non_drums)\n",
    "        # n_nodes x d\n",
    "\n",
    "        # Merge drums and non drums\n",
    "        out = torch.zeros((c_tensor.size(0), self.d), device=self.device,\n",
    "                          dtype=drums.dtype)\n",
    "        out[graph.is_drum] = drums\n",
    "        out[torch.logical_not(graph.is_drum)] = non_drums\n",
    "        # n_nodes x d\n",
    "\n",
    "        # Set initial graph node states to intermediate chord representations \n",
    "        # and pass through GCN\n",
    "        graph.x = out\n",
    "        graph.distinct_bars = graph.bars + self.n_bars*graph.batch\n",
    "        out = self.graph_encoder(graph)\n",
    "        # n_nodes x d\n",
    "\n",
    "        # Aggregate final node states into bar encodings with soft attention\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            out = self.graph_attention(out, batch=graph.distinct_bars)\n",
    "        # bs x n_bars x d\n",
    "\n",
    "        out = out.view(-1, self.n_bars * self.d)\n",
    "        # bs x (n_bars*d)\n",
    "        z_c = self.bars_encoder(out)\n",
    "        # bs x d\n",
    "        \n",
    "        return z_c\n",
    "\n",
    "\n",
    "class StructureEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.cnn_encoder = CNNEncoder(\n",
    "            dense_dim=self.d,\n",
    "            output_dim=self.d,\n",
    "            dropout=self.dropout,\n",
    "            batch_norm=self.batch_norm\n",
    "        )\n",
    "        self.bars_encoder = nn.Linear(self.n_bars * self.d, self.d)\n",
    "        # self.bars_encoder = nn.Linear(self.d, self.d)\n",
    "    \n",
    "    def forward(self, graph):\n",
    "        s_tensor = graph.s_tensor\n",
    "        # print(f\"Initial s_tensor shape: {s_tensor.shape}\")  # Initial shape: [bs, N_TRACKS, feature_dim]\n",
    "\n",
    "        # Adjust the reshaping for CNN input\n",
    "        # We assume feature_dim is the total dimension for each track, needing to be reshaped\n",
    "        # to fit CNN expectations. If your CNN expects [batch_size, channels, height, width],\n",
    "        # and here you interpret N_TRACKS as channels, your reshaping needs to reflect this.\n",
    "        s_tensor_reshaped = s_tensor.view(-1, N_TRACKS , self.resolution* 4 )  # Reshape to include a channel dimension\n",
    "\n",
    "        # Feed into CNN, the expected shape might need to adjust based on CNN's expected input dimensions\n",
    "        out = self.cnn_encoder(s_tensor_reshaped)\n",
    "        # print(f\"After CNN encoder, out shape: {out.shape}\")  # Check CNN output shape\n",
    "\n",
    "        # Correct the reshaping to re-establish batch dimension\n",
    "        out = out.view(-1, self.n_bars * self.d)  # Reshape to [bs, n_bars * d]\n",
    "        # print(f\"After view, out shape: {out.shape}\")\n",
    "        # print(f\"Shape before linear layer: {out.shape}\")\n",
    "        z_s = self.bars_encoder(out)\n",
    "        # print(f\"Final z_s shape: {z_s.shape}\")  # Expect [bs, d]\n",
    "\n",
    "        return z_s\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.s_encoder = StructureEncoder(**kwargs)\n",
    "        self.c_encoder = ContentEncoder(**kwargs)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        # Linear layer that merges content and structure representations\n",
    "        self.linear_merge = nn.Linear(2*self.d, self.d)\n",
    "        self.bn_linear_merge = nn.BatchNorm1d(num_features=self.d)\n",
    "\n",
    "        self.linear_mu = nn.Linear(self.d, self.d)\n",
    "        self.linear_log_var = nn.Linear(self.d, self.d)\n",
    "\n",
    "    def forward(self, graph):\n",
    "        \n",
    "        z_s = self.s_encoder(graph)\n",
    "        z_c = self.c_encoder(graph)\n",
    "        \n",
    "        # Merge content and structure representations\n",
    "        # print(f\"z_c shape: {z_c.shape}\")\n",
    "        # print(f\"z_s shape: {z_s.shape}\")\n",
    "        z_g = torch.cat((z_c, z_s), dim=1)\n",
    "        z_g = self.dropout_layer(z_g)\n",
    "        z_g = self.linear_merge(z_g)\n",
    "        z_g = self.bn_linear_merge(z_g)\n",
    "        z_g = F.relu(z_g)\n",
    "\n",
    "        # Compute mu and log(std^2)\n",
    "        z_g = self.dropout_layer(z_g)\n",
    "        mu = self.linear_mu(z_g)\n",
    "        log_var = self.linear_log_var(z_g)\n",
    "\n",
    "        return mu, log_var\n",
    "\n",
    "\n",
    "class StructureDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.bars_decoder = nn.Linear(self.d, self.d * self.n_bars)\n",
    "        #self.bars_decoder = nn.Linear(self.d, self.d)\n",
    "        self.cnn_decoder = CNNDecoder(\n",
    "            input_dim=self.d,\n",
    "            dense_dim=self.d,\n",
    "            dropout=self.dropout,\n",
    "            batch_norm=self.batch_norm\n",
    "        )\n",
    "\n",
    "    def forward(self, z_s):\n",
    "        # z_s: bs x d\n",
    "        out = self.bars_decoder(z_s)  # bs x (n_bars*d)\n",
    "        # out = self.cnn_decoder(out)\n",
    "        out = self.cnn_decoder(out.reshape(-1, self.d))\n",
    "        out = out.view(z_s.size(0), self.n_bars, N_TRACKS, -1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ContentDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.bars_decoder = nn.Linear(self.d, self.d * self.n_bars)\n",
    "\n",
    "        self.graph_decoder = GCN(\n",
    "            dropout=self.dropout,\n",
    "            input_dim=self.d,\n",
    "            hidden_dim=self.d,\n",
    "            n_layers=self.gnn_n_layers,\n",
    "            num_relations=N_EDGE_TYPES,\n",
    "            batch_norm=self.batch_norm\n",
    "        )\n",
    "\n",
    "        self.chord_decoder = nn.Linear(\n",
    "            self.d, self.d*(MAX_SIMU_TOKENS-1))\n",
    "\n",
    "        # Pitch and duration (un)embedding linear layers\n",
    "        self.drums_pitch_emb = nn.Linear(self.d//2, N_PITCH_TOKENS)\n",
    "        self.non_drums_pitch_emb = nn.Linear(\n",
    "            self.d//2, N_PITCH_TOKENS)\n",
    "        self.dur_emb = nn.Linear(self.d//2, N_DUR_TOKENS)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "    def forward(self, z_c, s):\n",
    "\n",
    "        out = self.bars_decoder(z_c)  # bs x (n_bars*d)\n",
    "\n",
    "        # Initialize node features with corresponding z_bar\n",
    "        # and propagate with GNN\n",
    "        s.distinct_bars = s.bars + self.n_bars*s.batch\n",
    "        _, counts = torch.unique(s.distinct_bars, return_counts=True)\n",
    "        out = out.view(-1, self.d)\n",
    "        out = torch.repeat_interleave(out, counts, axis=0)  # n_nodes x d\n",
    "        s.x = out\n",
    "        out = self.graph_decoder(s)  # n_nodes x d\n",
    "\n",
    "        out = self.chord_decoder(out)  # n_nodes x (MAX_SIMU_TOKENS*d)\n",
    "        out = out.view(-1, MAX_SIMU_TOKENS-1, self.d)\n",
    "\n",
    "        drums = out[s.is_drum]  # n_nodes_drums x MAX_SIMU_TOKENS x d\n",
    "        non_drums = out[torch.logical_not(s.is_drum)]\n",
    "        # n_nodes_non_drums x MAX_SIMU_TOKENS x d\n",
    "\n",
    "        # Obtain final pitch and dur logits (softmax will be applied\n",
    "        # outside forward)\n",
    "        non_drums = self.dropout_layer(non_drums)\n",
    "        drums = self.dropout_layer(drums)\n",
    "\n",
    "        drums_pitch = self.drums_pitch_emb(drums[..., :self.d//2])\n",
    "        drums_dur = self.dur_emb(drums[..., self.d//2:])\n",
    "        drums = torch.cat((drums_pitch, drums_dur), dim=-1)\n",
    "        # n_nodes_drums x MAX_SIMU_TOKENS x d_token\n",
    "\n",
    "        non_drums_pitch = self.non_drums_pitch_emb(non_drums[..., :self.d//2])\n",
    "        non_drums_dur = self.dur_emb(non_drums[..., self.d//2:])\n",
    "        non_drums = torch.cat((non_drums_pitch, non_drums_dur), dim=-1)\n",
    "        # n_nodes_non_drums x MAX_SIMU_TOKENS x d_token\n",
    "\n",
    "        # Merge drums and non-drums in the final output tensor\n",
    "        d_token = D_TOKEN_PAIR\n",
    "        out = torch.zeros((s.num_nodes, MAX_SIMU_TOKENS-1, d_token),\n",
    "                          device=self.device, dtype=drums.dtype)\n",
    "        out[s.is_drum] = drums\n",
    "        out[torch.logical_not(s.is_drum)] = non_drums\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.lin_decoder = nn.Linear(self.d, 2 * self.d)\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features=2*self.d)\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        self.s_decoder = StructureDecoder(**kwargs)\n",
    "        self.c_decoder = ContentDecoder(**kwargs)\n",
    "\n",
    "        self.sigmoid_thresh = 0.5\n",
    "\n",
    "    def _structure_from_binary(self, s_tensor):\n",
    "\n",
    "        # Create graph structures for each batch\n",
    "        s = []\n",
    "        for i in range(s_tensor.size(0)):\n",
    "            s.append(graph_from_tensor(s_tensor[i]))\n",
    "\n",
    "        # Create batch of graphs from single graphs\n",
    "        s = Batch.from_data_list(s, exclude_keys=['batch'])\n",
    "        s = s.to(next(self.parameters()).device)\n",
    "\n",
    "        return s\n",
    "\n",
    "    def _binary_from_logits(self, s_logits):\n",
    "\n",
    "        # Hard threshold instead of sampling gives more pleasant results\n",
    "        s_tensor = torch.sigmoid(s_logits)\n",
    "        s_tensor[s_tensor >= self.sigmoid_thresh] = 1\n",
    "        s_tensor[s_tensor < self.sigmoid_thresh] = 0\n",
    "        s_tensor = s_tensor.bool()\n",
    "        \n",
    "        # Avoid empty bars by creating a fake activation for each empty\n",
    "        # (n_tracks x n_timesteps) bar matrix in position [0, 0]\n",
    "        empty_mask = ~s_tensor.any(dim=-1).any(dim=-1)\n",
    "        idxs = torch.nonzero(empty_mask, as_tuple=True)\n",
    "        s_tensor[idxs + (0, 0)] = True\n",
    "\n",
    "        return s_tensor\n",
    "\n",
    "    def _structure_from_logits(self, s_logits):\n",
    "\n",
    "        # Compute binary structure tensor from logits and build torch geometric\n",
    "        # structure from binary tensor\n",
    "        s_tensor = self._binary_from_logits(s_logits)\n",
    "        s = self._structure_from_binary(s_tensor)\n",
    "\n",
    "        return s\n",
    "\n",
    "    def forward(self, z, s=None):\n",
    "\n",
    "        # Obtain z_s and z_c from z\n",
    "        z = self.lin_decoder(z)\n",
    "        z = self.batch_norm(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.dropout(z)  # bs x (2*d)\n",
    "        z_s, z_c = z[:, :self.d], z[:, self.d:]\n",
    "\n",
    "        # Obtain the tensor containing structure logits\n",
    "        s_logits = self.s_decoder(z_s)\n",
    "\n",
    "        if s is None:\n",
    "            # Build torch geometric graph structure from structure logits.\n",
    "            # This step involves non differentiable operations.\n",
    "            # No gradients pass through here.\n",
    "            s = self._structure_from_logits(s_logits.detach())\n",
    "\n",
    "        # Obtain the tensor containing content logits\n",
    "        c_logits = self.c_decoder(z_c, s)\n",
    "\n",
    "        return s_logits, c_logits\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(**kwargs)\n",
    "        self.decoder = Decoder(**kwargs)\n",
    "\n",
    "    def forward(self, graph):\n",
    "\n",
    "        # Encoder pass\n",
    "        mu, log_var = self.encoder(graph)\n",
    "\n",
    "        # Reparameterization trick\n",
    "        z = torch.exp(0.5 * log_var)\n",
    "        z = z * torch.randn_like(z)\n",
    "        z = z + mu\n",
    "\n",
    "        # Decoder pass\n",
    "        out = self.decoder(z, graph)\n",
    "\n",
    "        return out, mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PitchToken(Enum):\n",
    "    SOS = 128\n",
    "    EOS = 129\n",
    "    PAD = 130\n",
    "\n",
    "\n",
    "N_PITCH_TOKENS = 131\n",
    "MAX_PITCH_TOKEN = 127\n",
    "\n",
    "\n",
    "# Duration tokens have values in the range [0, 98]. Tokens from 0 to 95 have to\n",
    "# be interpreted as durations from 1 to 96 timesteps.\n",
    "class DurationToken(Enum):\n",
    "    SOS = 96\n",
    "    EOS = 97\n",
    "    PAD = 98\n",
    "    \n",
    "def append_dict(dest_d, source_d):\n",
    "\n",
    "    for k, v in source_d.items():\n",
    "        dest_d[k].append(v)\n",
    "\n",
    "def print_divider():\n",
    "    print('' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from statistics import mean\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "import pprint\n",
    "import math\n",
    "\n",
    "#import constants\n",
    "#from constants import PitchToken, DurationToken\n",
    "#from utils import append_dict, print_divider\n",
    "\n",
    "\n",
    "class StepBetaScheduler():\n",
    "    def __init__(self, anneal_start, beta_max, step_size, anneal_end):\n",
    "        self.anneal_start = anneal_start\n",
    "        self.beta_max = beta_max\n",
    "        self.step_size = step_size\n",
    "        self.anneal_end = anneal_end\n",
    "\n",
    "        self.update_steps = 0\n",
    "        self.beta = 0\n",
    "        n_steps = self.beta_max // self.step_size\n",
    "        self.inc_every = (self.anneal_end-self.anneal_start) // n_steps\n",
    "\n",
    "    def step(self):\n",
    "        self.update_steps += 1\n",
    "\n",
    "        if (self.update_steps >= self.anneal_start or\n",
    "                self.update_steps < self.anneal_end):\n",
    "            # If we are annealing, update beta according to current step\n",
    "            curr_step = (self.update_steps-self.anneal_start) // self.inc_every\n",
    "            self.beta = self.step_size * (curr_step+1)\n",
    "            \n",
    "        return self.beta\n",
    "\n",
    "\n",
    "class ExpDecayLRScheduler():\n",
    "    def __init__(self, optimizer, peak_lr, warmup_steps, final_lr_scale,\n",
    "                 decay_steps):\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.peak_lr = peak_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.decay_steps = decay_steps\n",
    "\n",
    "        # Find the decay factor needed to reach the specified\n",
    "        # learning rate scale after decay_steps steps\n",
    "        self.decay_factor = -math.log(final_lr_scale) / self.decay_steps\n",
    "\n",
    "        self.update_steps = 0\n",
    "\n",
    "    def set_lr(self, optimizer, lr):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def step(self):\n",
    "        self.update_steps += 1\n",
    "\n",
    "        if self.update_steps <= self.warmup_steps:\n",
    "            self.lr = self.peak_lr\n",
    "        else:\n",
    "            # Decay lr exponentially\n",
    "            steps_after_warmup = self.update_steps - self. warmup_steps\n",
    "            self.lr = \\\n",
    "                self.peak_lr * math.exp(-self.decay_factor*steps_after_warmup)\n",
    "\n",
    "        self.set_lr(self.optimizer, self.lr)\n",
    "\n",
    "        return self.lr\n",
    "\n",
    "\n",
    "class PolyphemusTrainer():\n",
    "\n",
    "    def __init__(self, model_dir, model, optimizer, init_lr=1e-4,\n",
    "                 lr_scheduler=None, beta_scheduler=None, device=None, \n",
    "                 print_every=1, save_every=1, eval_every=100, \n",
    "                 iters_to_accumulate=1, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.model_dir = model_dir\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.init_lr = init_lr\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.beta_scheduler = beta_scheduler\n",
    "        self.device = device if device is not None else torch.device(\"cpu\")\n",
    "        self.cuda = True if self.device.type == 'cuda' else False\n",
    "        self.print_every = print_every\n",
    "        self.save_every = save_every\n",
    "        self.eval_every = eval_every\n",
    "        self.iters_to_accumulate = iters_to_accumulate\n",
    "\n",
    "        # Losses (ignoring PAD tokens)\n",
    "        self.bce_unreduced = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        self.ce_p = nn.CrossEntropyLoss(ignore_index=PitchToken.PAD.value)\n",
    "        self.ce_d = nn.CrossEntropyLoss(ignore_index=DurationToken.PAD.value)\n",
    "\n",
    "        # Training stats\n",
    "        self.tr_losses = defaultdict(list)\n",
    "        self.tr_accuracies = defaultdict(list)\n",
    "        self.val_losses = defaultdict(list)\n",
    "        self.val_accuracies = defaultdict(list)\n",
    "        self.lrs = []\n",
    "        self.betas = []\n",
    "        self.times = []\n",
    "\n",
    "    def train(self, trainloader, validloader=None, epochs=100, early_exit=None):\n",
    "\n",
    "        self.tot_batches = 0\n",
    "        self.beta = 0\n",
    "        self.min_val_loss = np.inf\n",
    "\n",
    "        start = time.time()\n",
    "        self.times.append(start)\n",
    "\n",
    "        self.model.train()\n",
    "        scaler = torch.cuda.amp.GradScaler() if self.cuda else None\n",
    "        self.optimizer.zero_grad()\n",
    "        progress_bar = tqdm(range(len(trainloader)))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.cur_epoch = epoch\n",
    "            for batch_idx, graph in enumerate(trainloader):\n",
    "                self.cur_batch_idx = batch_idx\n",
    "\n",
    "                # Move batch of graphs to device. Note: a single graph here\n",
    "                # represents a bar in the original sequence.\n",
    "                graph = graph.to(self.device)\n",
    "                s_tensor, c_tensor = graph.s_tensor, graph.c_tensor\n",
    "\n",
    "                with torch.cuda.amp.autocast(enabled=self.cuda):\n",
    "                    # Forward pass to obtain mu, log(sigma^2), computed by the\n",
    "                    # encoder, and structure and content logits, computed by the\n",
    "                    # decoder\n",
    "                    (s_logits, c_logits), mu, log_var = self.model(graph)\n",
    "\n",
    "                    # Compute losses\n",
    "                    tot_loss, losses = self._losses(\n",
    "                        s_tensor, s_logits,\n",
    "                        c_tensor, c_logits,\n",
    "                        mu, log_var\n",
    "                    )\n",
    "                    tot_loss = tot_loss / self.iters_to_accumulate\n",
    "\n",
    "                # Backpropagation\n",
    "                if self.cuda:\n",
    "                    scaler.scale(tot_loss).backward()\n",
    "                else:\n",
    "                    tot_loss.backward()\n",
    "\n",
    "                # Update weights with accumulated gradients\n",
    "                if (self.tot_batches + 1) % self.iters_to_accumulate == 0:\n",
    "\n",
    "                    if self.cuda:\n",
    "                        scaler.step(self.optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        self.optimizer.step()\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                    # Update lr and beta\n",
    "                    if self.lr_scheduler is not None:\n",
    "                        self.lr_scheduler.step()\n",
    "                    if self.beta_scheduler is not None:\n",
    "                        self.beta_scheduler.step()\n",
    "\n",
    "                # Compute accuracies\n",
    "                accs = self._accuracies(\n",
    "                    s_tensor, s_logits,\n",
    "                    c_tensor, c_logits,\n",
    "                    graph.is_drum\n",
    "                )\n",
    "\n",
    "                # Update the stats\n",
    "                append_dict(self.tr_losses, losses)\n",
    "                append_dict(self.tr_accuracies, accs)\n",
    "                last_lr = (self.lr_scheduler.lr\n",
    "                           if self.lr_scheduler is not None else self.init_lr)\n",
    "                self.lrs.append(last_lr)\n",
    "                self.betas.append(self.beta)\n",
    "                now = time.time()\n",
    "                self.times.append(now)\n",
    "\n",
    "                # Print stats\n",
    "                if (self.tot_batches + 1) % self.print_every == 0:\n",
    "                    print(\"Training on batch {}/{} of epoch {}/{} complete.\"\n",
    "                          .format(batch_idx+1,\n",
    "                                  len(trainloader),\n",
    "                                  epoch+1,\n",
    "                                  epochs))\n",
    "                    self._print_stats()\n",
    "                    print_divider()\n",
    "\n",
    "                # Eval on VL every `eval_every` gradient updates\n",
    "                if (validloader is not None and\n",
    "                        (self.tot_batches + 1) % self.eval_every == 0):\n",
    "\n",
    "                    # Evaluate on VL\n",
    "                    print(\"\\nEvaluating on validation set...\\n\")\n",
    "                    val_losses, val_accuracies = self.evaluate(validloader)\n",
    "\n",
    "                    # Update stats\n",
    "                    append_dict(self.val_losses, val_losses)\n",
    "                    append_dict(self.val_accuracies, val_accuracies)\n",
    "\n",
    "                    print(\"Val losses:\")\n",
    "                    print(val_losses)\n",
    "                    print(\"Val accuracies:\")\n",
    "                    print(val_accuracies)\n",
    "\n",
    "                    # Save model if VL loss (tot) reached a new minimum\n",
    "                    # Example check before accessing 'tot'\n",
    "                    if 'tot' in val_losses:\n",
    "                        tot_loss = val_losses['tot']\n",
    "                    else:\n",
    "                        # Handle the missing key, e.g., set a default value or raise a more informative error\n",
    "                        tot_loss = 0  # or some default value\n",
    "                        print(\"Key 'tot' not found in val_losses. Setting tot_loss to 0.\")\n",
    "\n",
    "                    if tot_loss < self.min_val_loss:\n",
    "                        print(\"\\nValidation loss improved.\")\n",
    "                        print(\"Saving new best model to disk...\\n\")\n",
    "                        self._save_model('best_model')\n",
    "                        self.min_val_loss = tot_loss\n",
    "\n",
    "                    self.model.train()\n",
    "\n",
    "                progress_bar.update(1)\n",
    "\n",
    "                # Save model and stats on disk\n",
    "                if (self.save_every > 0 and\n",
    "                        (self.tot_batches + 1) % self.save_every == 0):\n",
    "                    self._save_model('checkpoint')\n",
    "\n",
    "                # Stop prematurely if early_exit is set and reached\n",
    "                if (early_exit is not None and\n",
    "                        (self.tot_batches + 1) > early_exit):\n",
    "                    break\n",
    "\n",
    "                self.tot_batches += 1\n",
    "\n",
    "        end = time.time()\n",
    "        hours, rem = divmod(end-start, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        print(\"Training completed in (h:m:s): {:0>2}:{:0>2}:{:05.2f}\"\n",
    "              .format(int(hours), int(minutes), seconds))\n",
    "\n",
    "        self._save_model('checkpoint')\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        print(f\"Starting evaluation with {len(loader)} batches...\")\n",
    "        losses = defaultdict(list)\n",
    "        accs = defaultdict(list)\n",
    "\n",
    "        self.model.eval()\n",
    "        progress_bar = tqdm(range(len(loader)))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _, graph in enumerate(loader):\n",
    "\n",
    "                # Get the inputs and move them to device\n",
    "                graph = graph.to(self.device)\n",
    "                s_tensor, c_tensor = graph.s_tensor, graph.c_tensor\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    # Forward pass, get the reconstructions\n",
    "                    (s_logits, c_logits), mu, log_var = self.model(graph)\n",
    "\n",
    "                    _, losses_b = self._losses(\n",
    "                        s_tensor, s_logits,\n",
    "                        c_tensor, c_logits,\n",
    "                        mu, log_var\n",
    "                    )\n",
    "\n",
    "                accs_b = self._accuracies(\n",
    "                    s_tensor, s_logits,\n",
    "                    c_tensor, c_logits,\n",
    "                    graph.is_drum\n",
    "                )\n",
    "\n",
    "                # Save losses and accuracies\n",
    "                append_dict(losses, losses_b)\n",
    "                append_dict(accs, accs_b)\n",
    "\n",
    "                progress_bar.update(1)\n",
    "\n",
    "        # Compute avg losses and accuracies\n",
    "        avg_losses = {}\n",
    "        for k, l in losses.items():\n",
    "            avg_losses[k] = mean(l)\n",
    "\n",
    "        avg_accs = {}\n",
    "        for k, l in accs.items():\n",
    "            avg_accs[k] = mean(l)\n",
    "\n",
    "        return avg_losses, avg_accs\n",
    "\n",
    "    def _losses(self, s_tensor, s_logits, c_tensor, c_logits, mu, log_var):\n",
    "\n",
    "        # Do not consider SOS token\n",
    "        c_tensor = c_tensor[..., 1:, :]\n",
    "        c_logits = c_logits.reshape(-1, c_logits.size(-1))\n",
    "        c_tensor = c_tensor.reshape(-1, c_tensor.size(-1))\n",
    "\n",
    "        # Reshape logits to match s_tensor dimensions:\n",
    "        # n_graphs (in batch) x n_tracks x n_timesteps\n",
    "        s_logits = s_tensor.reshape(-1, *s_logits.shape[2:])\n",
    "\n",
    "        # Binary structure tensor loss (binary cross entropy)\n",
    "        s_loss = self.bce_unreduced(\n",
    "            s_logits.view(-1), s_tensor.view(-1).float())\n",
    "        s_loss = torch.mean(s_loss)\n",
    "\n",
    "        # Content tensor loss (pitches)\n",
    "        # argmax is used to obtain token ids from onehot rep\n",
    "        pitch_logits = c_logits[:, :N_PITCH_TOKENS]\n",
    "        pitch_true = c_tensor[:, :N_PITCH_TOKENS].argmax(dim=1)\n",
    "        pitch_loss = self.ce_p(pitch_logits, pitch_true)\n",
    "\n",
    "        # Content tensor loss (durations)\n",
    "        dur_logits = c_logits[:, N_PITCH_TOKENS:]\n",
    "        dur_true = c_tensor[:, N_PITCH_TOKENS:].argmax(dim=1)\n",
    "        dur_loss = self.ce_d(dur_logits, dur_true)\n",
    "\n",
    "        # Kullback-Leibler divergence loss\n",
    "        # Derivation in Kingma, Diederik P., and Max Welling. \"Auto-encoding\n",
    "        # variational bayes.\" (2013), Appendix B.\n",
    "        # (https://arxiv.org/pdf/1312.6114.pdf)\n",
    "        kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(),\n",
    "                                    dim=1)\n",
    "        kld_loss = torch.mean(kld_loss)\n",
    "\n",
    "        # Reconstruction loss and total loss\n",
    "        rec_loss = pitch_loss + dur_loss + s_loss\n",
    "        tot_loss = rec_loss + self.beta*kld_loss\n",
    "\n",
    "        losses = {\n",
    "            'tot': tot_loss.item(),\n",
    "            'pitch': pitch_loss.item(),\n",
    "            'dur': dur_loss.item(),\n",
    "            'structure': s_loss.item(),\n",
    "            'reconstruction': rec_loss.item(),\n",
    "            'kld': kld_loss.item(),\n",
    "            'beta*kld': self.beta*kld_loss.item()\n",
    "        }\n",
    "\n",
    "        return tot_loss, losses\n",
    "\n",
    "    def _accuracies(self, s_tensor, s_logits, c_tensor, c_logits, is_drum):\n",
    "\n",
    "        # Do not consider SOS token\n",
    "        c_tensor = c_tensor[..., 1:, :]\n",
    "\n",
    "        # Reshape logits to match s_tensor dimensions:\n",
    "        # n_graphs (in batch) x n_tracks x n_timesteps\n",
    "        s_logits = s_tensor.reshape(-1, *s_logits.shape[2:])\n",
    "\n",
    "        # Note accuracy considers both pitches and durations\n",
    "        note_acc = self._note_accuracy(c_logits, c_tensor)\n",
    "\n",
    "        pitch_acc = self._pitch_accuracy(c_logits, c_tensor)\n",
    "\n",
    "        # Compute pitch accuracies for drums and non drums separately\n",
    "        pitch_acc_drums = self._pitch_accuracy(\n",
    "            c_logits, c_tensor, drums=True, is_drum=is_drum\n",
    "        )\n",
    "        pitch_acc_non_drums = self._pitch_accuracy(\n",
    "            c_logits, c_tensor, drums=False, is_drum=is_drum\n",
    "        )\n",
    "\n",
    "        dur_acc = self._duration_accuracy(c_logits, c_tensor)\n",
    "\n",
    "        s_acc = self._structure_accuracy(s_logits, s_tensor)\n",
    "        s_precision = self._structure_precision(s_logits, s_tensor)\n",
    "        s_recall = self._structure_recall(s_logits, s_tensor)\n",
    "        s_f1 = (2*s_recall*s_precision / (s_recall+s_precision))\n",
    "\n",
    "        accs = {\n",
    "            'note': note_acc.item(),\n",
    "            'pitch': pitch_acc.item(),\n",
    "            'pitch_drums': pitch_acc_drums.item(),\n",
    "            'pitch_non_drums': pitch_acc_non_drums.item(),\n",
    "            'dur': dur_acc.item(),\n",
    "            's_acc': s_acc.item(),\n",
    "            's_precision': s_precision.item(),\n",
    "            's_recall': s_recall.item(),\n",
    "            's_f1': s_f1.item()\n",
    "        }\n",
    "\n",
    "        return accs\n",
    "\n",
    "    def _pitch_accuracy(self, c_logits, c_tensor, drums=None, is_drum=None):\n",
    "\n",
    "        # When drums is None, just compute the global pitch accuracy without\n",
    "        # distinguishing between drum and non drum pitches\n",
    "        if drums is not None:\n",
    "            if drums:\n",
    "                c_logits = c_logits[is_drum]\n",
    "                c_tensor = c_tensor[is_drum]\n",
    "            else:\n",
    "                c_logits = c_logits[torch.logical_not(is_drum)]\n",
    "                c_tensor = c_tensor[torch.logical_not(is_drum)]\n",
    "\n",
    "        # Apply softmax to obtain pitch reconstructions\n",
    "        pitch_rec = c_logits[..., :N_PITCH_TOKENS]\n",
    "        pitch_rec = F.softmax(pitch_rec, dim=-1)\n",
    "        pitch_rec = torch.argmax(pitch_rec, dim=-1)\n",
    "\n",
    "        pitch_true = c_tensor[..., :N_PITCH_TOKENS]\n",
    "        pitch_true = torch.argmax(pitch_true, dim=-1)\n",
    "\n",
    "        # Do not consider PAD tokens when computing accuracies\n",
    "        not_pad = (pitch_true != PitchToken.PAD.value)\n",
    "\n",
    "        correct = (pitch_rec == pitch_true)\n",
    "        correct = torch.logical_and(correct, not_pad)\n",
    "\n",
    "        return torch.sum(correct) / torch.sum(not_pad)\n",
    "\n",
    "    def _duration_accuracy(self, c_logits, c_tensor):\n",
    "\n",
    "        # Apply softmax to obtain reconstructed durations\n",
    "        dur_rec = c_logits[..., N_PITCH_TOKENS:]\n",
    "        dur_rec = F.softmax(dur_rec, dim=-1)\n",
    "        dur_rec = torch.argmax(dur_rec, dim=-1)\n",
    "\n",
    "        dur_true = c_tensor[..., N_PITCH_TOKENS:]\n",
    "        dur_true = torch.argmax(dur_true, dim=-1)\n",
    "\n",
    "        # Do not consider PAD tokens when computing accuracies\n",
    "        not_pad = (dur_true != DurationToken.PAD.value)\n",
    "\n",
    "        correct = (dur_rec == dur_true)\n",
    "        correct = torch.logical_and(correct, not_pad)\n",
    "\n",
    "        return torch.sum(correct) / torch.sum(not_pad)\n",
    "\n",
    "    def _note_accuracy(self, c_logits, c_tensor):\n",
    "\n",
    "        # Apply softmax to obtain pitch reconstructions\n",
    "        pitch_rec = c_logits[..., :N_PITCH_TOKENS]\n",
    "        pitch_rec = F.softmax(pitch_rec, dim=-1)\n",
    "        pitch_rec = torch.argmax(pitch_rec, dim=-1)\n",
    "\n",
    "        pitch_true = c_tensor[..., :N_PITCH_TOKENS]\n",
    "        pitch_true = torch.argmax(pitch_true, dim=-1)\n",
    "\n",
    "        not_pad_p = (pitch_true != PitchToken.PAD.value)\n",
    "\n",
    "        correct_p = (pitch_rec == pitch_true)\n",
    "        correct_p = torch.logical_and(correct_p, not_pad_p)\n",
    "\n",
    "        dur_rec = c_logits[..., N_PITCH_TOKENS:]\n",
    "        dur_rec = F.softmax(dur_rec, dim=-1)\n",
    "        dur_rec = torch.argmax(dur_rec, dim=-1)\n",
    "\n",
    "        dur_true = c_tensor[..., N_PITCH_TOKENS:]\n",
    "        dur_true = torch.argmax(dur_true, dim=-1)\n",
    "\n",
    "        not_pad_d = (dur_true != DurationToken.PAD.value)\n",
    "\n",
    "        correct_d = (dur_rec == dur_true)\n",
    "        correct_d = torch.logical_and(correct_d, not_pad_d)\n",
    "\n",
    "        note_accuracy = torch.sum(\n",
    "            torch.logical_and(correct_p, correct_d)) / torch.sum(not_pad_p)\n",
    "\n",
    "        return note_accuracy\n",
    "\n",
    "    def _structure_accuracy(self, s_logits, s_tensor):\n",
    "\n",
    "        s_logits = torch.sigmoid(s_logits)\n",
    "        s_logits[s_logits < 0.5] = 0\n",
    "        s_logits[s_logits >= 0.5] = 1\n",
    "\n",
    "        return torch.sum(s_logits == s_tensor) / s_tensor.numel()\n",
    "\n",
    "    def _structure_precision(self, s_logits, s_tensor):\n",
    "\n",
    "        s_logits = torch.sigmoid(s_logits)\n",
    "        s_logits[s_logits < 0.5] = 0\n",
    "        s_logits[s_logits >= 0.5] = 1\n",
    "\n",
    "        tp = torch.sum(s_tensor[s_logits == 1])\n",
    "\n",
    "        return tp / torch.sum(s_logits)\n",
    "\n",
    "    def _structure_recall(self, s_logits, s_tensor):\n",
    "\n",
    "        s_logits = torch.sigmoid(s_logits)\n",
    "        s_logits[s_logits < 0.5] = 0\n",
    "        s_logits[s_logits >= 0.5] = 1\n",
    "\n",
    "        tp = torch.sum(s_tensor[s_logits == 1])\n",
    "\n",
    "        return tp / torch.sum(s_tensor)\n",
    "\n",
    "    def _save_model(self, filename):\n",
    "\n",
    "        path = os.path.join(self.model_dir, filename)\n",
    "        print(\"Saving model to disk...\")\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': self.cur_epoch,\n",
    "            'batch': self.cur_batch_idx,\n",
    "            'tot_batches': self.tot_batches,\n",
    "            'betas': self.betas,\n",
    "            'min_val_loss': self.min_val_loss,\n",
    "            'print_every': self.print_every,\n",
    "            'save_every': self.save_every,\n",
    "            'eval_every': self.eval_every,\n",
    "            'lrs': self.lrs,\n",
    "            'tr_losses': self.tr_losses,\n",
    "            'tr_accuracies': self.tr_accuracies,\n",
    "            'val_losses': self.val_losses,\n",
    "            'val_accuracies': self.val_accuracies,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict()\n",
    "        }, path)\n",
    "\n",
    "        print(\"The model has been successfully saved.\")\n",
    "\n",
    "    def _print_stats(self):\n",
    "\n",
    "        hours, rem = divmod(self.times[-1]-self.times[0], 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        print(\"Elapsed time from start (h:m:s): {:0>2}:{:0>2}:{:05.2f}\"\n",
    "              .format(int(hours), int(minutes), seconds))\n",
    "\n",
    "        # Take mean of the last non-printed batches for each loss and accuracy\n",
    "        avg_losses = {}\n",
    "        for k, l in self.tr_losses.items():\n",
    "            v = mean(l[-self.print_every:])\n",
    "            avg_losses[k] = round(v, 2)\n",
    "\n",
    "        avg_accs = {}\n",
    "        for k, l in self.tr_accuracies.items():\n",
    "            v = mean(l[-self.print_every:])\n",
    "            avg_accs[k] = round(v, 2)\n",
    "\n",
    "        print(\"Losses:\")\n",
    "        pprint.pprint(avg_losses, indent=2)\n",
    "\n",
    "        print(\"Accuracies:\")\n",
    "        pprint.pprint(avg_accs, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "import random\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "\n",
    "\n",
    "def print_params(model):\n",
    "\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "\n",
    "    for name, parameter in model.named_parameters():\n",
    "\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params += param\n",
    "\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Parameters: {total_params}\")\n",
    "\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the configuration file C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\training.json...\n",
      "Preparing datasets and dataloaders...\n",
      "Creating the model and moving it to cpu device...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s222445\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------+------------+\n",
      "|                           Modules                           | Parameters |\n",
      "+-------------------------------------------------------------+------------+\n",
      "|         encoder.s_encoder.cnn_encoder.conv.0.weight         |     72     |\n",
      "|          encoder.s_encoder.cnn_encoder.conv.0.bias          |     8      |\n",
      "|         encoder.s_encoder.cnn_encoder.conv.1.weight         |     8      |\n",
      "|          encoder.s_encoder.cnn_encoder.conv.1.bias          |     8      |\n",
      "|         encoder.s_encoder.cnn_encoder.conv.4.weight         |    1152    |\n",
      "|          encoder.s_encoder.cnn_encoder.conv.4.bias          |     16     |\n",
      "|         encoder.s_encoder.cnn_encoder.conv.5.weight         |     16     |\n",
      "|          encoder.s_encoder.cnn_encoder.conv.5.bias          |     16     |\n",
      "|          encoder.s_encoder.cnn_encoder.lin.1.weight         |   262144   |\n",
      "|           encoder.s_encoder.cnn_encoder.lin.1.bias          |    512     |\n",
      "|          encoder.s_encoder.cnn_encoder.lin.4.weight         |   262144   |\n",
      "|           encoder.s_encoder.cnn_encoder.lin.4.bias          |    512     |\n",
      "|            encoder.s_encoder.bars_encoder.weight            |   524288   |\n",
      "|             encoder.s_encoder.bars_encoder.bias             |    512     |\n",
      "|       encoder.c_encoder.continuous_feature_norm.weight      |     3      |\n",
      "|        encoder.c_encoder.continuous_feature_norm.bias       |     3      |\n",
      "|       encoder.c_encoder.continuous_to_embedding.weight      |    1536    |\n",
      "|        encoder.c_encoder.continuous_to_embedding.bias       |    512     |\n",
      "|         encoder.c_encoder.non_drums_pitch_emb.weight        |   33536    |\n",
      "|          encoder.c_encoder.non_drums_pitch_emb.bias         |    256     |\n",
      "|           encoder.c_encoder.drums_pitch_emb.weight          |   33536    |\n",
      "|            encoder.c_encoder.drums_pitch_emb.bias           |    256     |\n",
      "|               encoder.c_encoder.dur_emb.weight              |   25344    |\n",
      "|                encoder.c_encoder.dur_emb.bias               |    256     |\n",
      "|            encoder.c_encoder.bn_non_drums.weight            |    256     |\n",
      "|             encoder.c_encoder.bn_non_drums.bias             |    256     |\n",
      "|              encoder.c_encoder.bn_drums.weight              |    256     |\n",
      "|               encoder.c_encoder.bn_drums.bias               |    256     |\n",
      "|               encoder.c_encoder.bn_dur.weight               |    256     |\n",
      "|                encoder.c_encoder.bn_dur.bias                |    256     |\n",
      "|            encoder.c_encoder.chord_encoder.weight           |  3932160   |\n",
      "|             encoder.c_encoder.chord_encoder.bias            |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.0.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.0.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.0.bias        |    512     |\n",
      "|      encoder.c_encoder.graph_encoder.layers.0.nn.weight     |   16384    |\n",
      "|       encoder.c_encoder.graph_encoder.layers.0.nn.bias      |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.1.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.1.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.1.bias        |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.2.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.2.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.2.bias        |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.3.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.3.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.3.bias        |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.4.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.4.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.4.bias        |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.5.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.5.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.5.bias        |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.6.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.6.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.6.bias        |    512     |\n",
      "|       encoder.c_encoder.graph_encoder.layers.7.weight       |  1572864   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.7.root        |   262144   |\n",
      "|        encoder.c_encoder.graph_encoder.layers.7.bias        |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.0.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.0.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.1.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.1.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.2.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.2.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.3.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.3.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.4.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.4.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.5.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.5.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.6.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.6.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_encoder.norm_layers.7.module.weight |    512     |\n",
      "|  encoder.c_encoder.graph_encoder.norm_layers.7.module.bias  |    512     |\n",
      "| encoder.c_encoder.graph_attention.gate_nn.0.layers.0.weight |    512     |\n",
      "|  encoder.c_encoder.graph_attention.gate_nn.0.layers.0.bias  |     1      |\n",
      "|      encoder.c_encoder.graph_attention.gate_nn.1.weight     |     1      |\n",
      "|       encoder.c_encoder.graph_attention.gate_nn.1.bias      |     1      |\n",
      "|            encoder.c_encoder.bars_encoder.weight            |   524288   |\n",
      "|             encoder.c_encoder.bars_encoder.bias             |    512     |\n",
      "|                 encoder.linear_merge.weight                 |   524288   |\n",
      "|                  encoder.linear_merge.bias                  |    512     |\n",
      "|                encoder.bn_linear_merge.weight               |    512     |\n",
      "|                 encoder.bn_linear_merge.bias                |    512     |\n",
      "|                   encoder.linear_mu.weight                  |   262144   |\n",
      "|                    encoder.linear_mu.bias                   |    512     |\n",
      "|                encoder.linear_log_var.weight                |   262144   |\n",
      "|                 encoder.linear_log_var.bias                 |    512     |\n",
      "|                  decoder.lin_decoder.weight                 |   524288   |\n",
      "|                   decoder.lin_decoder.bias                  |    1024    |\n",
      "|                  decoder.batch_norm.weight                  |    1024    |\n",
      "|                   decoder.batch_norm.bias                   |    1024    |\n",
      "|            decoder.s_decoder.bars_decoder.weight            |   524288   |\n",
      "|             decoder.s_decoder.bars_decoder.bias             |    1024    |\n",
      "|          decoder.s_decoder.cnn_decoder.lin.1.weight         |   262144   |\n",
      "|           decoder.s_decoder.cnn_decoder.lin.1.bias          |    512     |\n",
      "|          decoder.s_decoder.cnn_decoder.lin.4.weight         |   262144   |\n",
      "|           decoder.s_decoder.cnn_decoder.lin.4.bias          |    512     |\n",
      "|         decoder.s_decoder.cnn_decoder.conv.1.weight         |    1152    |\n",
      "|          decoder.s_decoder.cnn_decoder.conv.1.bias          |     8      |\n",
      "|         decoder.s_decoder.cnn_decoder.conv.2.weight         |     8      |\n",
      "|          decoder.s_decoder.cnn_decoder.conv.2.bias          |     8      |\n",
      "|         decoder.s_decoder.cnn_decoder.conv.4.weight         |     72     |\n",
      "|          decoder.s_decoder.cnn_decoder.conv.4.bias          |     1      |\n",
      "|            decoder.c_decoder.bars_decoder.weight            |   524288   |\n",
      "|             decoder.c_decoder.bars_decoder.bias             |    1024    |\n",
      "|       decoder.c_decoder.graph_decoder.layers.0.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.0.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.0.bias        |    512     |\n",
      "|      decoder.c_decoder.graph_decoder.layers.0.nn.weight     |   16384    |\n",
      "|       decoder.c_decoder.graph_decoder.layers.0.nn.bias      |    512     |\n",
      "|       decoder.c_decoder.graph_decoder.layers.1.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.1.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.1.bias        |    512     |\n",
      "|       decoder.c_decoder.graph_decoder.layers.2.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.2.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.2.bias        |    512     |\n",
      "|       decoder.c_decoder.graph_decoder.layers.3.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.3.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.3.bias        |    512     |\n",
      "|       decoder.c_decoder.graph_decoder.layers.4.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.4.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.4.bias        |    512     |\n",
      "|       decoder.c_decoder.graph_decoder.layers.5.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.5.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.5.bias        |    512     |\n",
      "|       decoder.c_decoder.graph_decoder.layers.6.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.6.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.6.bias        |    512     |\n",
      "|       decoder.c_decoder.graph_decoder.layers.7.weight       |  1572864   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.7.root        |   262144   |\n",
      "|        decoder.c_decoder.graph_decoder.layers.7.bias        |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.0.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.0.module.bias  |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.1.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.1.module.bias  |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.2.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.2.module.bias  |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.3.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.3.module.bias  |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.4.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.4.module.bias  |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.5.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.5.module.bias  |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.6.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.6.module.bias  |    512     |\n",
      "| decoder.c_decoder.graph_decoder.norm_layers.7.module.weight |    512     |\n",
      "|  decoder.c_decoder.graph_decoder.norm_layers.7.module.bias  |    512     |\n",
      "|            decoder.c_decoder.chord_decoder.weight           |  3932160   |\n",
      "|             decoder.c_decoder.chord_decoder.bias            |    7680    |\n",
      "|           decoder.c_decoder.drums_pitch_emb.weight          |   33536    |\n",
      "|            decoder.c_decoder.drums_pitch_emb.bias           |    131     |\n",
      "|         decoder.c_decoder.non_drums_pitch_emb.weight        |   33536    |\n",
      "|          decoder.c_decoder.non_drums_pitch_emb.bias         |    131     |\n",
      "|               decoder.c_decoder.dur_emb.weight              |   25344    |\n",
      "|                decoder.c_decoder.dur_emb.bias               |     99     |\n",
      "+-------------------------------------------------------------+------------+\n",
      "Total Trainable Parameters: 42212963\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 16, 2])\n",
      "torch.Size([4, 2, 32, 16, 2])\n",
      "torch.Size([2, 4, 32, 16, 2])\n",
      "torch.Size([4, 64, 16, 2])\n",
      "torch.Size([4, 2, 32, 16, 2])\n",
      "torch.Size([2, 4, 32, 16, 2])\n",
      "Initial continuous features shape: torch.Size([8, 64, 3])\n",
      "Shape after batch norm: torch.Size([8, 3, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (24x64 and 3x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[151], line 91\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     80\u001b[0m trainer \u001b[38;5;241m=\u001b[39m PolyphemusTrainer(\n\u001b[0;32m     81\u001b[0m     model_dir\u001b[38;5;241m=\u001b[39mmodel_dir,\n\u001b[0;32m     82\u001b[0m     model\u001b[38;5;241m=\u001b[39mvae,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     89\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m     90\u001b[0m )\n\u001b[1;32m---> 91\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[149], line 141\u001b[0m, in \u001b[0;36mPolyphemusTrainer.train\u001b[1;34m(self, trainloader, validloader, epochs, early_exit)\u001b[0m\n\u001b[0;32m    135\u001b[0m s_tensor, c_tensor \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39ms_tensor, graph\u001b[38;5;241m.\u001b[39mc_tensor\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m# Forward pass to obtain mu, log(sigma^2), computed by the\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# encoder, and structure and content logits, computed by the\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# decoder\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m     (s_logits, c_logits), mu, log_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# Compute losses\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     tot_loss, losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_losses(\n\u001b[0;32m    145\u001b[0m         s_tensor, s_logits,\n\u001b[0;32m    146\u001b[0m         c_tensor, c_logits,\n\u001b[0;32m    147\u001b[0m         mu, log_var\n\u001b[0;32m    148\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[147], line 697\u001b[0m, in \u001b[0;36mVAE.forward\u001b[1;34m(self, graph)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph):\n\u001b[0;32m    695\u001b[0m \n\u001b[0;32m    696\u001b[0m     \u001b[38;5;66;03m# Encoder pass\u001b[39;00m\n\u001b[1;32m--> 697\u001b[0m     mu, log_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# Reparameterization trick\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m log_var)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[147], line 494\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, graph)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph):\n\u001b[0;32m    493\u001b[0m     z_s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms_encoder(graph)\n\u001b[1;32m--> 494\u001b[0m     z_c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;66;03m# Merge content and structure representations\u001b[39;00m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;66;03m# print(f\"z_c shape: {z_c.shape}\")\u001b[39;00m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;66;03m# print(f\"z_s shape: {z_s.shape}\")\u001b[39;00m\n\u001b[0;32m    499\u001b[0m     z_g \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((z_c, z_s), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[147], line 359\u001b[0m, in \u001b[0;36mContentEncoder.forward\u001b[1;34m(self, graph)\u001b[0m\n\u001b[0;32m    357\u001b[0m continuous_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontinuous_feature_norm(continuous_features)\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape after batch norm:\u001b[39m\u001b[38;5;124m\"\u001b[39m, continuous_features\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 359\u001b[0m continuous_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontinuous_to_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontinuous_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContinuous embeddings shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, continuous_embeddings\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# Combine continuous embeddings back into c_tensor\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (24x64 and 3x512)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "# Ensure you have imported PolyphemusDataset, VAE, set_seed, print_params, print_divider, PolyphemusTrainer, ExpDecayLRScheduler, StepBetaScheduler correctly\n",
    "\n",
    "# Configuration and setup\n",
    "dataset_dir = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\preprocessed'\n",
    "output_dir = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\output'\n",
    "config_file = r'C:\\Users\\s222445\\Spatiotemporal GNN Thesis\\training.json'\n",
    "model_name = 'testing_continuous lat lon'  # or None to use a UUID\n",
    "save_every = 10\n",
    "print_every = 1\n",
    "eval_flag = True  # Set to False if you don't want to perform evaluation\n",
    "eval_every = 10  \n",
    "use_gpu = False\n",
    "gpu_id = 0\n",
    "num_workers = 0\n",
    "tr_split = 0.7\n",
    "vl_split = 0.1\n",
    "max_epochs = 5\n",
    "seed = 42  # or None\n",
    "\n",
    "# Set seed for reproducibility\n",
    "if seed is not None:\n",
    "    set_seed(seed)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "#if use_gpu:\n",
    "#    torch.cuda.set_device(gpu_id)\n",
    "\n",
    "# Load training configuration from JSON file\n",
    "print(f\"Loading the configuration file {config_file}...\")\n",
    "with open(config_file, 'r') as f:\n",
    "    training_config = json.load(f)\n",
    "\n",
    "n_bars = training_config['model']['n_bars']\n",
    "batch_size = training_config['batch_size']\n",
    "\n",
    "# Prepare datasets and dataloaders\n",
    "print(\"Preparing datasets and dataloaders...\")\n",
    "dataset = PolyphemusDataset(dataset_dir, n_bars)\n",
    "\n",
    "tr_len = int(tr_split * len(dataset))\n",
    "vl_len = int(vl_split * len(dataset)) if eval_flag else 0\n",
    "ts_len = len(dataset) - tr_len - vl_len\n",
    "lengths = (tr_len, vl_len, ts_len) if eval_flag else (tr_len, len(dataset) - tr_len)\n",
    "\n",
    "split = random_split(dataset, lengths)\n",
    "tr_set, vl_set = split[0], (split[1] if eval_flag else None)\n",
    "\n",
    "trainloader = DataLoader(tr_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "validloader = DataLoader(vl_set, batch_size=batch_size, shuffle=False, num_workers=num_workers) if eval_flag else None\n",
    "\n",
    "# Model setup\n",
    "model_dir = os.path.join(output_dir, model_name or str(uuid.uuid1()))\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=False)\n",
    "\n",
    "print(f\"Creating the model and moving it to {device} device...\")\n",
    "vae = VAE(**training_config['model'], device=device).to(device)\n",
    "print_params(vae)\n",
    "\n",
    "# Optimizer and schedulers setup\n",
    "optimizer = optim.Adam(vae.parameters(), **training_config['optimizer'])\n",
    "lr_scheduler = ExpDecayLRScheduler(optimizer=optimizer, **training_config['lr_scheduler'])\n",
    "beta_scheduler = StepBetaScheduler(**training_config['beta_scheduler'])\n",
    "\n",
    "# Save configuration\n",
    "config_path = os.path.join(model_dir, 'configuration.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(training_config, f)\n",
    "\n",
    "# Training\n",
    "print(\"Starting training...\")\n",
    "trainer = PolyphemusTrainer(\n",
    "    model_dir=model_dir,\n",
    "    model=vae,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    beta_scheduler=beta_scheduler,\n",
    "    save_every=save_every,\n",
    "    print_every=print_every,\n",
    "    eval_every=eval_every,\n",
    "    device=device\n",
    ")\n",
    "trainer.train(trainloader, validloader=validloader, epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import torch\n",
    "\n",
    "def plot_structure(s_tensor, save_dir=None, name='structure'):\n",
    "\n",
    "    lines_linewidth = 1\n",
    "    axes_linewidth = 1\n",
    "    font_size = 14\n",
    "    fformat = 'svg'\n",
    "    dpi = 200\n",
    "\n",
    "    n_bars = s_tensor.shape[0]\n",
    "    figsize = (3 * n_bars, 3)\n",
    "\n",
    "    n_timesteps = s_tensor.size(2)\n",
    "    resolution = n_timesteps // 4\n",
    "    s_tensor = s_tensor.permute(1, 0, 2)\n",
    "    s_tensor = s_tensor.reshape(s_tensor.shape[0], -1)\n",
    "\n",
    "    with mpl.rc_context({'lines.linewidth': lines_linewidth,\n",
    "                         'axes.linewidth': axes_linewidth,\n",
    "                         'font.size': font_size}):\n",
    "\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.pcolormesh(s_tensor, edgecolors='k', linewidth=1)\n",
    "        ax = plt.gca()\n",
    "\n",
    "        plt.xticks(range(0, s_tensor.shape[1], resolution),\n",
    "                   range(1, 4*n_bars + 1))\n",
    "        plt.yticks(range(0, s_tensor.shape[0]), TRACKS)\n",
    "\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, name + \".\" + fformat),\n",
    "                        format=fformat, dpi=dpi)\n",
    "    \n",
    "    \n",
    "def plot_stats(stat_names, stats_tr, stats_val=None, eval_every=None, \n",
    "               labels=None, rx=None, ry=None):\n",
    "\n",
    "    for i, stat in enumerate(stat_names):\n",
    "\n",
    "        label = stat if not labels else labels[i]\n",
    "\n",
    "        plt.plot(range(1, len(stats_tr[stat])+1), stats_tr[stat],\n",
    "                label=label+' (TR)')\n",
    "\n",
    "        if stats_val:\n",
    "            plt.plot(range(eval_every, len(stats_tr[stat])+1, eval_every),\n",
    "                    stats_val[stat], '.', label=label+' (VL)')\n",
    "\n",
    "    plt.grid()\n",
    "\n",
    "    plt.ylim(ry) if ry else plt.ylim(0)\n",
    "    plt.xlim(rx) if rx else plt.xlim(0)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "# Dictionary that maps loss statistic name to plot label \n",
    "loss_labels = {\n",
    "    'tot': 'Total Loss',\n",
    "    'structure': 'Structure',\n",
    "    'pitch': 'Pitches',\n",
    "    'dur': 'Duration',\n",
    "    'reconstruction': 'Reconstruction Term',\n",
    "    'kld': 'KLD',\n",
    "    'beta*kld': 'beta * KLD'\n",
    "}\n",
    "\n",
    "\n",
    "def plot_losses(model_dir, losses, plot_val=False):\n",
    "    \n",
    "    checkpoint_path = os.path.join(model_dir, 'checkpoint')\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    labels = [loss_labels[loss] for loss in losses]\n",
    "    \n",
    "    tr_losses = checkpoint['tr_losses']\n",
    "    val_losses = checkpoint['val_losses'] if plot_val == True else None\n",
    "    eval_every = checkpoint['eval_every'] if plot_val == True else None\n",
    "\n",
    "    plot_stats(losses, tr_losses, stats_val=val_losses,\n",
    "               eval_every=eval_every, labels=labels, rx=(0))\n",
    "    \n",
    "\n",
    "# Dictionary that maps accuracy statistic name to plot label \n",
    "accuracy_labels = {\n",
    "    's_acc': 'Struct. Accuracy',\n",
    "    's_precision': 'Struct. Precision',\n",
    "    's_recall': 'Struct. Recall',\n",
    "    's_f1': 'Struct. F1',\n",
    "    'pitch': 'Pitch Accuracy',\n",
    "    'pitch_drums': 'Pitch Accuracy (Drums)',\n",
    "    'pitch_non_drums': 'Pitch Accuracy (Non Drums)',\n",
    "    'dur': 'Duration Accuracy',\n",
    "    'note': 'Note Accuracy'\n",
    "}\n",
    "\n",
    "\n",
    "def plot_accuracies(model_dir, accuracies, plot_val=False):\n",
    "    \n",
    "    checkpoint_path = os.path.join(model_dir, 'checkpoint')\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    labels = [accuracy_labels[accuracy] for accuracy in accuracies]\n",
    "    \n",
    "    tr_accuracies = checkpoint['tr_accuracies']\n",
    "    val_accuracies = checkpoint['val_accuracies'] if plot_val == True else None\n",
    "    eval_every = checkpoint['eval_every'] if plot_val == True else None\n",
    "\n",
    "    plot_stats(accuracies, tr_accuracies, stats_val=val_accuracies,\n",
    "               eval_every=eval_every, labels=labels, ry=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming your model's directory is 'model_dir'\n",
    "plot_losses(model_dir='output/testing_continuous', losses=['tot', 'structure', 'pitch'], plot_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming your model's directory is 'model_dir'\n",
    "plot_accuracies(model_dir='output/testing_continuous', accuracies=['s_acc', 'pitch', 'dur'], plot_val=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_original_data(data, title='Original Data', samples=5):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(samples):\n",
    "        plt.subplot(samples, 1, i+1) \n",
    "        plt.plot(data[i], marker='o', linestyle='-')\n",
    "        plt.title(f\"{title} for Sample {i+1}\")\n",
    "        plt.ylabel('Value')\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_original_data(emotions, title='Original Emotions')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
